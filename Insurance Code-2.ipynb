{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bf26a520",
   "metadata": {},
   "outputs": [],
   "source": [
    "#hp = hyperparameters[dataset]\n",
    "\n",
    "#classes = hp['classes']\n",
    "\n",
    "import pandas as pd\n",
    "#import xlrd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import numpy as np\n",
    "from sklearn.impute import KNNImputer\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "pd.DataFrame.iteritems=pd.DataFrame.items\n",
    "# change the strings to ints]\n",
    "#input= pd.DataFrame(pd.read_csv(\"US Insurance Claims Data.xlsx\", encoding='utf-16-le', encoding_errors='ignore'))\n",
    "input=pd.read_excel(\"US_Insurance_Claims_Data.xlsx\")\n",
    "input=input.fillna(method='bfill').fillna(method='ffill')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3a776d9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "input.incident_location=input.incident_location.astype('category').cat.codes\n",
    "input.policy_state=input.policy_state.astype('category').cat.codes\n",
    "input.insured_education_level=input.insured_education_level.astype('category').cat.codes\n",
    "input.insured_sex=input.insured_sex.astype('category').cat.codes\n",
    "input.insured_hobbies=input.insured_hobbies.astype('category').cat.codes\n",
    "input.insured_relationship=input.insured_relationship.astype('category').cat.codes\n",
    "input.incident_type=input.incident_type.astype('category').cat.codes\n",
    "input.collision_type=input.collision_type.astype('category').cat.codes\n",
    "input.incident_severity=input.incident_severity.astype('category').cat.codes\n",
    "input.authorities_contacted=input.authorities_contacted.astype('category').cat.codes\n",
    "input.incident_state=input.incident_state.astype('category').cat.codes\n",
    "input.incident_city=input.incident_city.astype('category').cat.codes\n",
    "input.insured_occupation=input.insured_occupation.astype('category').cat.codes\n",
    "input.property_damage=input.property_damage.astype('category').cat.codes\n",
    "input.police_report_available=input.police_report_available.astype('category').cat.codes\n",
    "input.auto_make=input.auto_make.astype('category').cat.codes\n",
    "input.auto_model=input.auto_model.astype('category').cat.codes\n",
    "#input['fraud_reported']=input['fraud_reported'].fillna(method='ffill').fillna(method='bfill')\n",
    "input.fraud_reported=input.fraud_reported.astype('category').cat.codes\n",
    "input.incident_date=input.incident_date.astype('category').cat.codes\n",
    "input.policy_bind_date=input.incident_date.astype('category').cat.codes\n",
    "input['policy_csl'] = input['policy_csl'].apply(lambda x: eval(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "76bd1ae5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(0.25455432859496063, shape=(), dtype=float64)\n",
      "tf.Tensor(0.22527721005429807, shape=(), dtype=float64)\n",
      "tf.Tensor(0.22981096220373876, shape=(), dtype=float64)\n",
      "tf.Tensor(0.2456511322270351, shape=(), dtype=float64)\n",
      "tf.Tensor(0.2630404693632685, shape=(), dtype=float64)\n",
      "tf.Tensor(0.28240764748947594, shape=(), dtype=float64)\n",
      "tf.Tensor(0.2838972627669822, shape=(), dtype=float64)\n",
      "tf.Tensor(0.296305685093954, shape=(), dtype=float64)\n",
      "tf.Tensor(0.30167030587145965, shape=(), dtype=float64)\n",
      "tf.Tensor(0.30485903341264514, shape=(), dtype=float64)\n",
      "tf.Tensor(0.3125306284961528, shape=(), dtype=float64)\n",
      "tf.Tensor(0.30535246377867703, shape=(), dtype=float64)\n",
      "tf.Tensor(0.3137295603001454, shape=(), dtype=float64)\n",
      "tf.Tensor(0.31763062585781066, shape=(), dtype=float64)\n",
      "tf.Tensor(0.31813805905228026, shape=(), dtype=float64)\n",
      "tf.Tensor(0.32051290453475484, shape=(), dtype=float64)\n",
      "tf.Tensor(0.31768172387904936, shape=(), dtype=float64)\n",
      "tf.Tensor(0.32168594007852847, shape=(), dtype=float64)\n",
      "tf.Tensor(0.32305270024938243, shape=(), dtype=float64)\n"
     ]
    }
   ],
   "source": [
    "#img_rows, img_cols = 32, 32\n",
    "#train_size = trainX.shape[0]\n",
    "\n",
    "def KNN_impute(input, k):\n",
    "    imputer = KNNImputer(n_neighbors=k)\n",
    "    df_filled = imputer.fit_transform(input)\n",
    "    matrix=pd.DataFrame(df_filled, columns=['months_as_customer', 'age', 'policy_number', 'policy_bind_date' , 'policy_state', 'policy_csl', 'policy_deductable', 'policy_annual_premium', 'umbrella_limit', 'insured_zip', 'insured_sex', 'insured_education_level', 'insured_occupation', 'insured_hobbies', 'insured_relationship', 'capital-gains', 'capital-loss', 'incident_date', 'incident_type', 'collision_type', 'incident_severity', 'authorities_contacted', 'incident_state', 'incident_city', 'incident_location', 'incident_hour_of_the_day', 'number_of_vehicles_involved', 'property_damage', 'bodily_injuries', 'witnesses', 'police_report_available', 'total_claim_amount', 'injury_claim', 'property_claim', 'vehicle_claim', 'auto_make', 'auto_model', 'auto_year', 'fraud_reported'])\n",
    "    #matrix = pd.DataFrame(df_filled, columns=['months_as_customer', 'age', 'policy_number', 'policy_bind_date', 'policy_state', 'policy_csl', 'policy_deductable', 'policy_annual_premium', 'insured_zip', 'insured_sex', 'insured_education_level', 'insured_occupation', 'insured_hobbies', 'insured_relationship', 'capital-gains', 'capital-loss', 'incident_date', ])\n",
    "    matrix.to_csv(\"DataKNN\" +  str(k) + \".csv\" )\n",
    "    return matrix\n",
    "\n",
    "for k in range(1,20,1):\n",
    "    matrix=KNN_impute(input, k)\n",
    "    aws=KNeighborsRegressor(k)\n",
    "    X = matrix.drop('fraud_reported',axis=1)\n",
    "    y =  matrix['fraud_reported']\n",
    "    from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "    ros = RandomUnderSampler()\n",
    "    X_res, y_res = ros.fit_resample(X, y)\n",
    "    aws.fit(X_res, y_res)\n",
    "    cv_scores = cross_val_score(aws, X_res, y_res,  cv=10, scoring='neg_mean_squared_error')\n",
    "    print(tf.math.abs(cv_scores.mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f40a469c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a1fc9ea4",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# trainX,testX,trainY,testY=train_test_split(X_res,y_res,test_size=0.2)\n",
    "matrix=KNN_impute(input, 2)   \n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "#trainX_padded = pad_sequences(trainX, padding='post', truncating='post')\n",
    "#testX_padded = pad_sequences(testX, padding='post', truncating='post')\n",
    "#trainY_padded = pad_sequences(trainY, padding='post', truncating='post')\n",
    "#testY_padded = pad_sequences(testY, padding='post', truncating='post')\n",
    "\n",
    "\n",
    "\n",
    "grouped = input.groupby('months_as_customer')\n",
    "\n",
    "# Extract sequences and labels\n",
    "sequences = grouped.apply(lambda X: X[['months_as_customer', 'age', 'policy_number', 'policy_bind_date' , 'policy_state', 'policy_csl', 'policy_deductable', 'policy_annual_premium', 'umbrella_limit', 'insured_zip', 'insured_sex', 'insured_education_level', 'insured_occupation', 'insured_hobbies', 'insured_relationship', 'capital-gains', 'capital-loss', 'incident_date', 'incident_type', 'collision_type', 'incident_severity', 'authorities_contacted', 'incident_state', 'incident_city', 'incident_location', 'incident_hour_of_the_day', 'number_of_vehicles_involved', 'property_damage', 'bodily_injuries', 'witnesses', 'police_report_available', 'total_claim_amount', 'injury_claim', 'property_claim', 'vehicle_claim', 'auto_make', 'auto_model', 'auto_year']].values.tolist())\n",
    "labels = grouped['fraud_reported'].first().values\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Pad to same length (post-padding with zeros)\n",
    "X = pad_sequences(sequences.tolist(), padding='post', dtype='float32')\n",
    "y = np.array(labels)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# Flatten for oversampling\n",
    "\n",
    "#X_resampled, y_resampled = ros.fit_resample(X_flat, y)\n",
    "#\n",
    "#X_resampled, y_resampled = sm.fit_resample(X_flat, y)\n",
    "\n",
    "\n",
    "# Reshape back to 3D\n",
    "\n",
    "X_flat = X.reshape((X.shape[0], -1))\n",
    "#ros = RandomOverSampler(random_state=1)\n",
    "ros=RandomUnderSampler(random_state=1000)\n",
    "sm=SMOTE(random_state=1000)\n",
    "\n",
    "X_resampled, y_resampled = sm.fit_resample(X_flat, y)\n",
    "X_resampled, y_resampled = ros.fit_resample(X_resampled,  y_resampled)\n",
    "\n",
    "X_resampled = X_resampled.reshape((-1, X.shape[1], X.shape[2]))\n",
    "\n",
    "trainX,testX,trainY,testY=train_test_split((X_resampled), (y_resampled), test_size=0.2, random_state=1000)\n",
    "\n",
    "\n",
    "#trainY = utils.to_categorical(trainY)\n",
    "#testY = utils.to_categorical(testY)\n",
    "#model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy', 'top_k_categorical_accuracy'])\n",
    "#csv_logger = CSVLogger(logfile, append=True, separator=';')\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "a35bf608",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout, Masking\n",
    "from sklearn.utils import class_weight\n",
    "from tensorflow.keras.initializers import RandomNormal\n",
    "\n",
    "optim_adam = tf.compat.v1.train.AdamOptimizer(0.1)\n",
    "\n",
    "model = Sequential([\n",
    "    Masking(mask_value=0.0, input_shape=(X.shape[1], X.shape[2])),\n",
    "    #LSTM(2),\n",
    "    LSTM(128), # Bad init,\n",
    "   # Dense(1, activation='relu'),\n",
    "    # Binary classification\n",
    "    #kernel_initializer='random_normal'),\n",
    "    Dense(1, activation='sigmoid')  # Binary classification\n",
    "    \n",
    "])\n",
    "\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout, Masking\n",
    "from sklearn.utils import class_weight\n",
    "\n",
    "optim_adam = tf.compat.v1.train.AdamOptimizer(0.1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "convex_loss_fn = tf.keras.losses.BinaryFocalCrossentropy(\n",
    "    apply_class_balancing=False, alpha=0.25, gamma=0.0, from_logits=False,\n",
    "    label_smoothing=0.0, axis=-1, reduction='sum_over_batch_size'\n",
    ")\n",
    "\n",
    "\n",
    "hybrid_convex_loss_fn = tf.keras.losses.BinaryFocalCrossentropy(\n",
    "    apply_class_balancing=False, alpha=0.25, gamma=2.0, from_logits=False,\n",
    "    label_smoothing=0.0, axis=-1, reduction='sum_over_batch_size'\n",
    ")\n",
    "\n",
    "\n",
    "nonconvex_loss_fn = tf.keras.losses.BinaryFocalCrossentropy(\n",
    "    apply_class_balancing=False, alpha=0.25, gamma=8.0, from_logits=False,\n",
    "    label_smoothing=0.0, axis=-1, reduction='sum_over_batch_size'\n",
    ")\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.backend as K\n",
    "\n",
    "class NoisyLossWrapper(tf.keras.losses.Loss):\n",
    "    def __init__(self, base_loss_fn, noise_scale=0.5, name='noisy_loss'):\n",
    "        super(NoisyLossWrapper, self).__init__(name=name)\n",
    "        self.base_loss_fn = base_loss_fn\n",
    "        self.noise_scale = noise_scale\n",
    "\n",
    "    def call(self, y_true, y_pred):\n",
    "        base_loss = self.base_loss_fn(y_true, y_pred)\n",
    "        noise = tf.random.normal(shape=tf.shape(base_loss), mean=0.0, stddev=self.noise_scale)\n",
    "        return base_loss + noise\n",
    "\n",
    "\n",
    "nonconvex_focal_loss = NoisyLossWrapper(nonconvex_loss_fn, noise_scale=0.5)\n",
    "convex_focal_loss = NoisyLossWrapper(convex_loss_fn, noise_scale=0.5)\n",
    "\n",
    "#nonconvex_focal_loss = nonconvex_loss_fn\n",
    "#convex_focal_loss = convex_loss_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "ba6874b0",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔁 Training Stage 1 | Epochs: 5 | Loss: noisy_loss\n",
      "Epoch 1/5\n",
      "633/633 [==============================] - 6s 8ms/step - loss: 0.5979 - accuracy: 0.6856 - val_loss: 0.6141 - val_accuracy: 0.7296\n",
      "Epoch 2/5\n",
      "633/633 [==============================] - 5s 7ms/step - loss: 0.5952 - accuracy: 0.6888 - val_loss: 0.6564 - val_accuracy: 0.6478\n",
      "Epoch 3/5\n",
      "633/633 [==============================] - 4s 7ms/step - loss: 0.6216 - accuracy: 0.6840 - val_loss: 0.5979 - val_accuracy: 0.6792\n",
      "Epoch 4/5\n",
      "633/633 [==============================] - 4s 7ms/step - loss: 0.6185 - accuracy: 0.6983 - val_loss: 0.6343 - val_accuracy: 0.6730\n",
      "Epoch 5/5\n",
      "633/633 [==============================] - 4s 7ms/step - loss: 0.5957 - accuracy: 0.6872 - val_loss: 0.5566 - val_accuracy: 0.6855\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔁 Training Stage 2 | Epochs: 95 | Loss: noisy_loss\n",
      "Epoch 6/100\n",
      "633/633 [==============================] - 6s 8ms/step - loss: -0.0073 - accuracy: 0.6935 - val_loss: -0.0692 - val_accuracy: 0.7044\n",
      "Epoch 7/100\n",
      "633/633 [==============================] - 5s 7ms/step - loss: 0.0200 - accuracy: 0.7046 - val_loss: 0.0127 - val_accuracy: 0.7233\n",
      "Epoch 8/100\n",
      "633/633 [==============================] - 5s 7ms/step - loss: 0.0079 - accuracy: 0.6951 - val_loss: -0.0389 - val_accuracy: 0.6981\n",
      "Epoch 9/100\n",
      "633/633 [==============================] - 5s 7ms/step - loss: 0.0189 - accuracy: 0.6983 - val_loss: 0.0050 - val_accuracy: 0.7107\n",
      "Epoch 10/100\n",
      "633/633 [==============================] - 5s 7ms/step - loss: 0.0023 - accuracy: 0.7030 - val_loss: -0.0700 - val_accuracy: 0.7610\n",
      "Epoch 11/100\n",
      "633/633 [==============================] - 5s 7ms/step - loss: 0.0166 - accuracy: 0.7014 - val_loss: -0.0129 - val_accuracy: 0.7673\n",
      "Epoch 12/100\n",
      "633/633 [==============================] - 5s 7ms/step - loss: -0.0022 - accuracy: 0.6967 - val_loss: -0.0024 - val_accuracy: 0.7736\n",
      "Epoch 13/100\n",
      "633/633 [==============================] - 5s 7ms/step - loss: -0.0061 - accuracy: 0.7204 - val_loss: -0.0220 - val_accuracy: 0.7610\n",
      "Epoch 14/100\n",
      "633/633 [==============================] - 5s 7ms/step - loss: 0.0148 - accuracy: 0.7188 - val_loss: 0.0169 - val_accuracy: 0.7484\n",
      "Epoch 15/100\n",
      "633/633 [==============================] - 5s 7ms/step - loss: -0.0150 - accuracy: 0.7046 - val_loss: 0.0041 - val_accuracy: 0.7421\n",
      "Epoch 16/100\n",
      "633/633 [==============================] - 5s 7ms/step - loss: 0.0499 - accuracy: 0.6872 - val_loss: -0.0453 - val_accuracy: 0.7233\n",
      "Epoch 17/100\n",
      "633/633 [==============================] - 5s 7ms/step - loss: 0.0246 - accuracy: 0.6998 - val_loss: -0.0865 - val_accuracy: 0.7044\n",
      "Epoch 18/100\n",
      "633/633 [==============================] - 5s 7ms/step - loss: -0.0286 - accuracy: 0.6983 - val_loss: -0.0398 - val_accuracy: 0.7107\n",
      "Epoch 19/100\n",
      "633/633 [==============================] - 5s 7ms/step - loss: 0.0027 - accuracy: 0.7046 - val_loss: -0.0163 - val_accuracy: 0.7233\n",
      "Epoch 20/100\n",
      "633/633 [==============================] - 5s 7ms/step - loss: -0.0157 - accuracy: 0.7172 - val_loss: 0.0704 - val_accuracy: 0.7296\n",
      "Epoch 21/100\n",
      "633/633 [==============================] - 5s 7ms/step - loss: 0.0128 - accuracy: 0.7093 - val_loss: -0.0316 - val_accuracy: 0.7421\n",
      "Epoch 22/100\n",
      "633/633 [==============================] - 5s 7ms/step - loss: 0.0130 - accuracy: 0.7093 - val_loss: 0.0555 - val_accuracy: 0.7296\n",
      "Epoch 23/100\n",
      "633/633 [==============================] - 5s 7ms/step - loss: 0.0225 - accuracy: 0.7251 - val_loss: -0.0253 - val_accuracy: 0.7358\n",
      "Epoch 24/100\n",
      "633/633 [==============================] - 5s 7ms/step - loss: 0.0014 - accuracy: 0.7251 - val_loss: -0.0230 - val_accuracy: 0.7358\n",
      "Epoch 25/100\n",
      "633/633 [==============================] - 5s 7ms/step - loss: 0.0208 - accuracy: 0.7267 - val_loss: -0.0863 - val_accuracy: 0.7358\n",
      "Epoch 26/100\n",
      "633/633 [==============================] - 5s 7ms/step - loss: 0.0114 - accuracy: 0.7251 - val_loss: -0.0355 - val_accuracy: 0.7358\n",
      "Epoch 27/100\n",
      "633/633 [==============================] - 5s 7ms/step - loss: 0.0213 - accuracy: 0.7235 - val_loss: -0.0330 - val_accuracy: 0.7421\n",
      "Epoch 28/100\n",
      "633/633 [==============================] - 5s 7ms/step - loss: -0.0074 - accuracy: 0.7299 - val_loss: 0.0121 - val_accuracy: 0.7484\n",
      "Epoch 29/100\n",
      "633/633 [==============================] - 5s 7ms/step - loss: 4.3413e-04 - accuracy: 0.7235 - val_loss: -0.0181 - val_accuracy: 0.7421\n",
      "Epoch 30/100\n",
      "633/633 [==============================] - 5s 7ms/step - loss: -0.0072 - accuracy: 0.7299 - val_loss: 0.0388 - val_accuracy: 0.7358\n",
      "Epoch 31/100\n",
      "633/633 [==============================] - 5s 7ms/step - loss: 0.0056 - accuracy: 0.7141 - val_loss: -0.0086 - val_accuracy: 0.7421\n",
      "Epoch 32/100\n",
      "633/633 [==============================] - 5s 7ms/step - loss: 0.0057 - accuracy: 0.7220 - val_loss: 8.5571e-04 - val_accuracy: 0.7296\n",
      "Epoch 33/100\n",
      "633/633 [==============================] - 5s 7ms/step - loss: -0.0014 - accuracy: 0.7172 - val_loss: -0.0486 - val_accuracy: 0.7358\n",
      "Epoch 34/100\n",
      "633/633 [==============================] - 5s 7ms/step - loss: 0.0077 - accuracy: 0.7220 - val_loss: -0.0825 - val_accuracy: 0.7421\n",
      "Epoch 35/100\n",
      "633/633 [==============================] - 5s 7ms/step - loss: -0.0035 - accuracy: 0.7251 - val_loss: 0.0057 - val_accuracy: 0.7358\n",
      "Epoch 36/100\n",
      "633/633 [==============================] - 5s 7ms/step - loss: 0.0081 - accuracy: 0.7314 - val_loss: 0.0337 - val_accuracy: 0.7358\n",
      "Epoch 37/100\n",
      "633/633 [==============================] - 5s 7ms/step - loss: 0.0213 - accuracy: 0.7314 - val_loss: 0.0172 - val_accuracy: 0.7358\n",
      "Epoch 38/100\n",
      "633/633 [==============================] - 5s 7ms/step - loss: 0.0456 - accuracy: 0.7314 - val_loss: -0.0290 - val_accuracy: 0.7421\n",
      "Epoch 39/100\n",
      "633/633 [==============================] - 5s 7ms/step - loss: -0.0010 - accuracy: 0.7283 - val_loss: -0.0363 - val_accuracy: 0.7358\n",
      "Epoch 40/100\n",
      "633/633 [==============================] - 5s 7ms/step - loss: -0.0121 - accuracy: 0.7299 - val_loss: -0.0509 - val_accuracy: 0.7421\n",
      "Epoch 41/100\n",
      "633/633 [==============================] - 5s 7ms/step - loss: 0.0289 - accuracy: 0.7314 - val_loss: -0.0098 - val_accuracy: 0.7421\n",
      "Epoch 42/100\n",
      "633/633 [==============================] - 5s 7ms/step - loss: 0.0101 - accuracy: 0.7283 - val_loss: 0.0538 - val_accuracy: 0.7421\n",
      "Epoch 43/100\n",
      "633/633 [==============================] - 5s 7ms/step - loss: -0.0027 - accuracy: 0.7299 - val_loss: -0.0114 - val_accuracy: 0.7296\n",
      "Epoch 44/100\n",
      "633/633 [==============================] - 5s 7ms/step - loss: 0.0224 - accuracy: 0.7299 - val_loss: -0.0569 - val_accuracy: 0.7296\n",
      "Epoch 45/100\n",
      "633/633 [==============================] - 5s 7ms/step - loss: 0.0192 - accuracy: 0.7283 - val_loss: 0.0696 - val_accuracy: 0.7296\n",
      "Epoch 46/100\n",
      "633/633 [==============================] - 5s 7ms/step - loss: -0.0027 - accuracy: 0.7283 - val_loss: -0.0108 - val_accuracy: 0.7233\n",
      "Epoch 47/100\n",
      "633/633 [==============================] - 5s 7ms/step - loss: 7.3211e-04 - accuracy: 0.7299 - val_loss: 0.0228 - val_accuracy: 0.7233\n",
      "Epoch 48/100\n",
      "633/633 [==============================] - 5s 7ms/step - loss: 0.0064 - accuracy: 0.7283 - val_loss: -0.0159 - val_accuracy: 0.7358\n",
      "Epoch 49/100\n",
      "633/633 [==============================] - 5s 7ms/step - loss: 0.0188 - accuracy: 0.7299 - val_loss: 0.0118 - val_accuracy: 0.7358\n",
      "Epoch 50/100\n",
      "633/633 [==============================] - 5s 7ms/step - loss: -0.0149 - accuracy: 0.7283 - val_loss: 0.0457 - val_accuracy: 0.7421\n",
      "Epoch 51/100\n",
      "633/633 [==============================] - 5s 7ms/step - loss: -0.0027 - accuracy: 0.7235 - val_loss: -0.0056 - val_accuracy: 0.7421\n",
      "Epoch 52/100\n",
      "633/633 [==============================] - 5s 7ms/step - loss: 0.0161 - accuracy: 0.7299 - val_loss: 0.0249 - val_accuracy: 0.7421\n",
      "Epoch 53/100\n",
      "633/633 [==============================] - 5s 7ms/step - loss: 0.0078 - accuracy: 0.7362 - val_loss: 0.0504 - val_accuracy: 0.7484\n",
      "Epoch 54/100\n",
      "633/633 [==============================] - 5s 7ms/step - loss: 0.0101 - accuracy: 0.7267 - val_loss: -0.0131 - val_accuracy: 0.7484\n",
      "Epoch 55/100\n",
      "633/633 [==============================] - 5s 7ms/step - loss: 0.0174 - accuracy: 0.7346 - val_loss: -0.0378 - val_accuracy: 0.7421\n",
      "Epoch 56/100\n",
      "633/633 [==============================] - 5s 7ms/step - loss: 0.0059 - accuracy: 0.7330 - val_loss: 0.0362 - val_accuracy: 0.7484\n",
      "Epoch 57/100\n",
      "633/633 [==============================] - 5s 7ms/step - loss: -0.0416 - accuracy: 0.7346 - val_loss: -0.0365 - val_accuracy: 0.7484\n",
      "Epoch 58/100\n",
      "633/633 [==============================] - 5s 7ms/step - loss: -0.0092 - accuracy: 0.7378 - val_loss: 0.0343 - val_accuracy: 0.7484\n",
      "Epoch 59/100\n",
      "633/633 [==============================] - 5s 7ms/step - loss: -0.0264 - accuracy: 0.7362 - val_loss: 0.0407 - val_accuracy: 0.7484\n",
      "Epoch 60/100\n",
      "633/633 [==============================] - 5s 7ms/step - loss: 0.0135 - accuracy: 0.7409 - val_loss: -0.0514 - val_accuracy: 0.7484\n",
      "Epoch 61/100\n",
      "633/633 [==============================] - 5s 7ms/step - loss: 0.0052 - accuracy: 0.7378 - val_loss: 0.0314 - val_accuracy: 0.7484\n",
      "Epoch 62/100\n",
      "633/633 [==============================] - 5s 7ms/step - loss: -0.0041 - accuracy: 0.7378 - val_loss: -0.0490 - val_accuracy: 0.7484\n",
      "Epoch 63/100\n",
      "633/633 [==============================] - 5s 7ms/step - loss: -0.0015 - accuracy: 0.7393 - val_loss: 0.0324 - val_accuracy: 0.7484\n",
      "Epoch 64/100\n",
      "633/633 [==============================] - 5s 7ms/step - loss: 0.0182 - accuracy: 0.7378 - val_loss: 0.0767 - val_accuracy: 0.7484\n",
      "Epoch 65/100\n",
      "633/633 [==============================] - 5s 7ms/step - loss: 0.0260 - accuracy: 0.7378 - val_loss: 0.0492 - val_accuracy: 0.7484\n",
      "Epoch 66/100\n",
      "633/633 [==============================] - 5s 7ms/step - loss: 0.0316 - accuracy: 0.7409 - val_loss: 0.0243 - val_accuracy: 0.7484\n",
      "Epoch 67/100\n",
      "633/633 [==============================] - 5s 7ms/step - loss: -0.0174 - accuracy: 0.7378 - val_loss: -0.0180 - val_accuracy: 0.7484\n",
      "Epoch 68/100\n",
      "633/633 [==============================] - 5s 7ms/step - loss: 0.0235 - accuracy: 0.7393 - val_loss: 0.0316 - val_accuracy: 0.7484\n",
      "Epoch 69/100\n",
      "633/633 [==============================] - 5s 7ms/step - loss: 0.0068 - accuracy: 0.7409 - val_loss: 0.0347 - val_accuracy: 0.7484\n",
      "Epoch 70/100\n",
      "633/633 [==============================] - 5s 7ms/step - loss: 0.0138 - accuracy: 0.7393 - val_loss: 0.0021 - val_accuracy: 0.7484\n",
      "Epoch 71/100\n",
      "633/633 [==============================] - 5s 7ms/step - loss: 0.0261 - accuracy: 0.7409 - val_loss: 0.0184 - val_accuracy: 0.7421\n",
      "Epoch 72/100\n",
      "633/633 [==============================] - 5s 7ms/step - loss: -7.6615e-04 - accuracy: 0.7425 - val_loss: 0.0833 - val_accuracy: 0.7421\n",
      "Epoch 73/100\n",
      "633/633 [==============================] - 5s 7ms/step - loss: -0.0089 - accuracy: 0.7425 - val_loss: 0.0290 - val_accuracy: 0.7421\n",
      "Epoch 74/100\n",
      "633/633 [==============================] - 5s 7ms/step - loss: -0.0244 - accuracy: 0.7425 - val_loss: -0.0399 - val_accuracy: 0.7421\n",
      "Epoch 75/100\n",
      "633/633 [==============================] - 5s 7ms/step - loss: 0.0032 - accuracy: 0.7425 - val_loss: 0.0112 - val_accuracy: 0.7421\n",
      "Epoch 76/100\n",
      "633/633 [==============================] - 5s 7ms/step - loss: 0.0041 - accuracy: 0.7425 - val_loss: -0.0270 - val_accuracy: 0.7421\n",
      "Epoch 77/100\n",
      "633/633 [==============================] - 5s 7ms/step - loss: 0.0019 - accuracy: 0.7425 - val_loss: 0.0134 - val_accuracy: 0.7421\n",
      "Epoch 78/100\n",
      "633/633 [==============================] - 5s 7ms/step - loss: -0.0183 - accuracy: 0.7425 - val_loss: 0.0135 - val_accuracy: 0.7421\n",
      "Epoch 79/100\n",
      "633/633 [==============================] - 5s 7ms/step - loss: -0.0050 - accuracy: 0.7425 - val_loss: 0.0061 - val_accuracy: 0.7421\n",
      "Epoch 80/100\n",
      "633/633 [==============================] - 5s 7ms/step - loss: 0.0086 - accuracy: 0.7409 - val_loss: 0.0497 - val_accuracy: 0.7421\n",
      "Epoch 81/100\n",
      "633/633 [==============================] - 5s 7ms/step - loss: 0.0272 - accuracy: 0.7425 - val_loss: 0.0349 - val_accuracy: 0.7421\n",
      "Epoch 82/100\n",
      "633/633 [==============================] - 5s 7ms/step - loss: 0.0199 - accuracy: 0.7409 - val_loss: -0.0221 - val_accuracy: 0.7421\n",
      "Epoch 83/100\n",
      "633/633 [==============================] - 5s 7ms/step - loss: 0.0101 - accuracy: 0.7425 - val_loss: -0.0068 - val_accuracy: 0.7421\n",
      "Epoch 84/100\n",
      "633/633 [==============================] - 5s 7ms/step - loss: -0.0418 - accuracy: 0.7409 - val_loss: -0.0376 - val_accuracy: 0.7421\n",
      "Epoch 85/100\n",
      "633/633 [==============================] - 5s 7ms/step - loss: 0.0140 - accuracy: 0.7425 - val_loss: -0.0220 - val_accuracy: 0.7421\n",
      "Epoch 86/100\n",
      "633/633 [==============================] - 5s 7ms/step - loss: -0.0118 - accuracy: 0.7441 - val_loss: 0.0089 - val_accuracy: 0.7421\n",
      "Epoch 87/100\n",
      "633/633 [==============================] - 5s 7ms/step - loss: -0.0225 - accuracy: 0.7441 - val_loss: -0.0784 - val_accuracy: 0.7421\n",
      "Epoch 88/100\n",
      "633/633 [==============================] - 5s 7ms/step - loss: 0.0122 - accuracy: 0.7441 - val_loss: -0.0198 - val_accuracy: 0.7421\n",
      "Epoch 89/100\n",
      "633/633 [==============================] - 5s 7ms/step - loss: -0.0078 - accuracy: 0.7441 - val_loss: -0.0033 - val_accuracy: 0.7421\n",
      "Epoch 90/100\n",
      "633/633 [==============================] - 5s 7ms/step - loss: 0.0116 - accuracy: 0.7441 - val_loss: 0.0060 - val_accuracy: 0.7421\n",
      "Epoch 91/100\n",
      "633/633 [==============================] - 5s 7ms/step - loss: -0.0239 - accuracy: 0.7425 - val_loss: -0.0306 - val_accuracy: 0.7421\n",
      "Epoch 92/100\n",
      "633/633 [==============================] - 5s 7ms/step - loss: 0.0293 - accuracy: 0.7441 - val_loss: -0.0218 - val_accuracy: 0.7421\n",
      "Epoch 93/100\n",
      "633/633 [==============================] - 5s 7ms/step - loss: 0.0210 - accuracy: 0.7425 - val_loss: 0.0066 - val_accuracy: 0.7421\n",
      "Epoch 94/100\n",
      "633/633 [==============================] - 5s 7ms/step - loss: -0.0111 - accuracy: 0.7425 - val_loss: -0.0454 - val_accuracy: 0.7421\n",
      "Epoch 95/100\n",
      "633/633 [==============================] - 5s 7ms/step - loss: 0.0174 - accuracy: 0.7425 - val_loss: -0.0078 - val_accuracy: 0.7421\n",
      "Epoch 96/100\n",
      "633/633 [==============================] - 5s 7ms/step - loss: 0.0034 - accuracy: 0.7425 - val_loss: 0.0126 - val_accuracy: 0.7421\n",
      "Epoch 97/100\n",
      "633/633 [==============================] - 5s 7ms/step - loss: -0.0193 - accuracy: 0.7425 - val_loss: 0.0088 - val_accuracy: 0.7421\n",
      "Epoch 98/100\n",
      "633/633 [==============================] - 5s 7ms/step - loss: -0.0072 - accuracy: 0.7425 - val_loss: 0.0205 - val_accuracy: 0.7421\n",
      "Epoch 99/100\n",
      "633/633 [==============================] - 5s 7ms/step - loss: 0.0262 - accuracy: 0.7425 - val_loss: 0.0067 - val_accuracy: 0.7421\n",
      "Epoch 100/100\n",
      "633/633 [==============================] - 5s 7ms/step - loss: 0.0359 - accuracy: 0.7425 - val_loss: -0.0371 - val_accuracy: 0.7421\n"
     ]
    }
   ],
   "source": [
    "\n",
    "initial_learning_rate = 0.0001\n",
    "\n",
    "# You can adjust decay_steps and decay_rate based on your dataset size\n",
    "lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "    initial_learning_rate=initial_learning_rate,\n",
    "    decay_steps=1000,     # Number of steps before decay\n",
    "    decay_rate=0.9,       # Decay multiplier\n",
    "    staircase=True        # Decay in steps instead of smoothly\n",
    ")\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=lr_schedule)\n",
    "\n",
    "training_schedule = [\n",
    "    {\"loss_fn\": convex_focal_loss, \"epochs\": 5},      # Phase 1: convex loss recommended 5 epochs\n",
    "    {\"loss_fn\": nonconvex_focal_loss, \"epochs\": 95},  # Phase 2: nonconvex loss\n",
    "   # {\"loss_fn\": convex_focal_loss, \"epochs\": 15},     # Phase 3: convex again (optional)\n",
    "   # {\"loss_fn\": convex_focal_loss, \"epochs\": 50}   # Phase 4: final stage\n",
    "]\n",
    "\n",
    "# Compile the model before training\n",
    "current_epoch = 0\n",
    "\n",
    "for i, stage in enumerate(training_schedule):\n",
    "    print(f\"\\n🔁 Training Stage {i + 1} | Epochs: {stage['epochs']} | Loss: {stage['loss_fn'].name}\")\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss=stage['loss_fn'],\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    model.fit(\n",
    "        trainX, trainY,\n",
    "        initial_epoch=current_epoch,\n",
    "        epochs=current_epoch + stage[\"epochs\"],\n",
    "        batch_size=1,\n",
    "        validation_data=(testX, testY),\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    current_epoch += stage[\"epochs\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58262b07",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "pwd"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
