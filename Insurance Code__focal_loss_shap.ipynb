{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bf26a520",
   "metadata": {},
   "outputs": [],
   "source": [
    "#hp = hyperparameters[dataset]\n",
    "\n",
    "#classes = hp['classes']\n",
    "\n",
    "import pandas as pd\n",
    "#import xlrd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import numpy as np\n",
    "from sklearn.impute import KNNImputer\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "pd.DataFrame.iteritems=pd.DataFrame.items\n",
    "# change the strings to ints]\n",
    "#input= pd.DataFrame(pd.read_csv(\"US Insurance Claims Data.xlsx\", encoding='utf-16-le', encoding_errors='ignore'))\n",
    "input=pd.read_excel(\"US_Insurance_Claims_Data.xlsx\")\n",
    "input=input.fillna(method='bfill').fillna(method='ffill')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3a776d9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "input.incident_location=input.incident_location.astype('category').cat.codes\n",
    "input.policy_state=input.policy_state.astype('category').cat.codes\n",
    "input.insured_education_level=input.insured_education_level.astype('category').cat.codes\n",
    "input.insured_sex=input.insured_sex.astype('category').cat.codes\n",
    "input.insured_hobbies=input.insured_hobbies.astype('category').cat.codes\n",
    "input.insured_relationship=input.insured_relationship.astype('category').cat.codes\n",
    "input.incident_type=input.incident_type.astype('category').cat.codes\n",
    "input.collision_type=input.collision_type.astype('category').cat.codes\n",
    "input.incident_severity=input.incident_severity.astype('category').cat.codes\n",
    "input.authorities_contacted=input.authorities_contacted.astype('category').cat.codes\n",
    "input.incident_state=input.incident_state.astype('category').cat.codes\n",
    "input.incident_city=input.incident_city.astype('category').cat.codes\n",
    "input.insured_occupation=input.insured_occupation.astype('category').cat.codes\n",
    "input.property_damage=input.property_damage.astype('category').cat.codes\n",
    "input.police_report_available=input.police_report_available.astype('category').cat.codes\n",
    "input.auto_make=input.auto_make.astype('category').cat.codes\n",
    "input.auto_model=input.auto_model.astype('category').cat.codes\n",
    "#input['fraud_reported']=input['fraud_reported'].fillna(method='ffill').fillna(method='bfill')\n",
    "input.fraud_reported=input.fraud_reported.astype('category').cat.codes\n",
    "input.incident_date=input.incident_date.astype('category').cat.codes\n",
    "input.policy_bind_date=input.incident_date.astype('category').cat.codes\n",
    "input['policy_csl'] = input['policy_csl'].apply(lambda x: eval(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "76bd1ae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#img_rows, img_cols = 32, 32\n",
    "#train_size = trainX.shape[0]\n",
    "\n",
    "def KNN_impute(input, k):\n",
    "    imputer = KNNImputer(n_neighbors=k)\n",
    "    df_filled = imputer.fit_transform(input)\n",
    "    matrix=pd.DataFrame(df_filled, columns=['months_as_customer', 'age', 'policy_number', 'policy_bind_date' , 'policy_state', 'policy_csl', 'policy_deductable', 'policy_annual_premium', 'umbrella_limit', 'insured_zip', 'insured_sex', 'insured_education_level', 'insured_occupation', 'insured_hobbies', 'insured_relationship', 'capital-gains', 'capital-loss', 'incident_date', 'incident_type', 'collision_type', 'incident_severity', 'authorities_contacted', 'incident_state', 'incident_city', 'incident_location', 'incident_hour_of_the_day', 'number_of_vehicles_involved', 'property_damage', 'bodily_injuries', 'witnesses', 'police_report_available', 'total_claim_amount', 'injury_claim', 'property_claim', 'vehicle_claim', 'auto_make', 'auto_model', 'auto_year', 'fraud_reported'])\n",
    "    #matrix = pd.DataFrame(df_filled, columns=['months_as_customer', 'age', 'policy_number', 'policy_bind_date', 'policy_state', 'policy_csl', 'policy_deductable', 'policy_annual_premium', 'insured_zip', 'insured_sex', 'insured_education_level', 'insured_occupation', 'insured_hobbies', 'insured_relationship', 'capital-gains', 'capital-loss', 'incident_date', ])\n",
    "    matrix.to_csv(\"DataKNN\" +  str(k) + \".csv\" )\n",
    "    return matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f40a469c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a1fc9ea4",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# trainX,testX,trainY,testY=train_test_split(X_res,y_res,test_size=0.2)\n",
    "#matrix=KNN_impute(input, 2)   \n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "#trainX_padded = pad_sequences(trainX, padding='post', truncating='post')\n",
    "#testX_padded = pad_sequences(testX, padding='post', truncating='post')\n",
    "#trainY_padded = pad_sequences(trainY, padding='post', truncating='post')\n",
    "#testY_padded = pad_sequences(testY, padding='post', truncating='post')\n",
    "\n",
    "\n",
    "\n",
    "grouped = input.groupby('months_as_customer')\n",
    "\n",
    "# Extract sequences and labels\n",
    "sequences = grouped.apply(lambda X: X[['months_as_customer', 'age', 'policy_number', 'policy_bind_date' , 'policy_state', 'policy_csl', 'policy_deductable', 'policy_annual_premium', 'umbrella_limit', 'insured_zip', 'insured_sex', 'insured_education_level', 'insured_occupation', 'insured_hobbies', 'insured_relationship', 'capital-gains', 'capital-loss', 'incident_date', 'incident_type', 'collision_type', 'incident_severity', 'authorities_contacted', 'incident_state', 'incident_city', 'incident_location', 'incident_hour_of_the_day', 'number_of_vehicles_involved', 'property_damage', 'bodily_injuries', 'witnesses', 'police_report_available', 'total_claim_amount', 'injury_claim', 'property_claim', 'vehicle_claim', 'auto_make', 'auto_model', 'auto_year']].values.tolist())\n",
    "labels = grouped['fraud_reported'].first().values\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Pad to same length (post-padding with zeros)\n",
    "X = pad_sequences(sequences.tolist(), padding='post', dtype='float32')\n",
    "y = np.array(labels)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# Flatten for oversampling\n",
    "\n",
    "#X_resampled, y_resampled = ros.fit_resample(X_flat, y)\n",
    "#\n",
    "#X_resampled, y_resampled = sm.fit_resample(X_flat, y)\n",
    "\n",
    "\n",
    "# Reshape back to 3D\n",
    "\n",
    "X_flat = X.reshape((X.shape[0], -1))\n",
    "#ros = RandomOverSampler(random_state=1)\n",
    "ros=RandomUnderSampler(random_state=1000)\n",
    "sm=SMOTE(random_state=1000)\n",
    "\n",
    "X_resampled, y_resampled = sm.fit_resample(X_flat, y)\n",
    "X_resampled, y_resampled = ros.fit_resample(X_resampled,  y_resampled)\n",
    "\n",
    "X_resampled = X_resampled.reshape((-1, X.shape[1], X.shape[2]))\n",
    "\n",
    "trainX,testX,trainY,testY=train_test_split((X_resampled), (y_resampled), test_size=0.2, random_state=1000)\n",
    "\n",
    "\n",
    "#trainY = utils.to_categorical(trainY)\n",
    "#testY = utils.to_categorical(testY)\n",
    "#model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy', 'top_k_categorical_accuracy'])\n",
    "#csv_logger = CSVLogger(logfile, append=True, separator=';')\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb469a77",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a35bf608",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout, Masking\n",
    "from sklearn.utils import class_weight\n",
    "from tensorflow.keras.initializers import RandomNormal\n",
    "\n",
    "optim_adam = tf.compat.v1.train.AdamOptimizer(0.1)\n",
    "\n",
    "\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout, Masking\n",
    "from sklearn.utils import class_weight\n",
    "\n",
    "optim_adam = tf.compat.v1.train.AdamOptimizer(0.1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "convex_loss_fn = tf.keras.losses.BinaryFocalCrossentropy(\n",
    "    apply_class_balancing=False, alpha=0.25, gamma=0.0, from_logits=False,\n",
    "    label_smoothing=0.0, axis=-1, reduction='sum_over_batch_size'\n",
    ")\n",
    "\n",
    "\n",
    "hybrid_convex_loss_fn = tf.keras.losses.BinaryFocalCrossentropy(\n",
    "    apply_class_balancing=False, alpha=0.25, gamma=2.0, from_logits=False,\n",
    "    label_smoothing=0.0, axis=-1, reduction='sum_over_batch_size'\n",
    ")\n",
    "\n",
    "\n",
    "nonconvex_loss_fn = tf.keras.losses.BinaryFocalCrossentropy(\n",
    "    apply_class_balancing=False, alpha=0.25, gamma=8.0, from_logits=False,\n",
    "    label_smoothing=0.0, axis=-1, reduction='sum_over_batch_size'\n",
    ")\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.backend as K\n",
    "\n",
    "class NoisyLossWrapper(tf.keras.losses.Loss):\n",
    "    def __init__(self, base_loss_fn, noise_scale=0.5, name='noisy_loss'):\n",
    "        super(NoisyLossWrapper, self).__init__(name=name)\n",
    "        self.base_loss_fn = base_loss_fn\n",
    "        self.noise_scale = noise_scale\n",
    "\n",
    "    def call(self, y_true, y_pred):\n",
    "        base_loss = self.base_loss_fn(y_true, y_pred)\n",
    "        noise = tf.random.normal(shape=tf.shape(base_loss), mean=0.0, stddev=self.noise_scale)\n",
    "        return base_loss + noise\n",
    "\n",
    "\n",
    "nonconvex_focal_loss = NoisyLossWrapper(nonconvex_loss_fn, noise_scale=0.5)\n",
    "convex_focal_loss = NoisyLossWrapper(convex_loss_fn, noise_scale=0.5)\n",
    "\n",
    "#nonconvex_focal_loss = nonconvex_loss_fn\n",
    "#convex_focal_loss = convex_loss_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba6874b0",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "37437eda",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "class ConvexityRegularizer(tf.keras.regularizers.Regularizer):\n",
    "    def __init__(self, coeff=0.005):\n",
    "        self.coeff = coeff\n",
    "\n",
    "    def __call__(self, x):\n",
    "        diff = x[:, 1:] - x[:, :-1]\n",
    "        penalty = tf.nn.relu(-diff)\n",
    "        return self.coeff * tf.reduce_sum(penalty)\n",
    "\n",
    "    def get_config(self):\n",
    "        return {\"coeff\": self.coeff}\n",
    "\n",
    "\n",
    "\n",
    "def model_builder():\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Masking(mask_value=0.0, input_shape=(X.shape[1], X.shape[2])),\n",
    "        tf.keras.layers.LSTM(128),\n",
    "        tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "convex_focal_loss = tf.keras.losses.BinaryFocalCrossentropy(\n",
    "    apply_class_balancing=False, alpha=0.25, gamma=0.0, from_logits=False,\n",
    "    label_smoothing=0.0, axis=-1, reduction='sum_over_batch_size'\n",
    ")\n",
    "convex_focal_loss.__name__ = \"convex_focal_loss\"\n",
    "\n",
    "nonconvex_focal_loss = tf.keras.losses.BinaryFocalCrossentropy(\n",
    "    apply_class_balancing=False, alpha=0.25, gamma=8.0, from_logits=False,\n",
    "    label_smoothing=0.0, axis=-1, reduction='sum_over_batch_size'\n",
    ")\n",
    "nonconvex_focal_loss.__name__ = \"nonconvex_focal_loss\"\n",
    "\n",
    "training_schedule_convex = [\n",
    "    {\"loss_fn\": convex_focal_loss, \"epochs\": 100},\n",
    "]\n",
    "\n",
    "training_schedule_multistage = [\n",
    "    {\"loss_fn\": convex_focal_loss, \"epochs\": 15},\n",
    "    {\"loss_fn\": nonconvex_focal_loss, \"epochs\": 95},\n",
    "]\n",
    "\n",
    "training_schedule_nonconvex = [\n",
    "    {\"loss_fn\": nonconvex_focal_loss, \"epochs\": 100},\n",
    "]\n",
    "\n",
    "\n",
    "from tensorflow.keras.metrics import Precision, Recall\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "def train_with_schedule(schedule, schedule_name):\n",
    "    print(f\"\\n🚀 Training schedule: {schedule_name}\")\n",
    "    \n",
    "    model = model_builder()\n",
    "    \n",
    "    lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "        initial_learning_rate=0.0001,\n",
    "        decay_steps=1000,\n",
    "        decay_rate=0.9,\n",
    "        staircase=True\n",
    "    )\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=lr_schedule)\n",
    "    \n",
    "    current_epoch = 0\n",
    "    \n",
    "    for i, stage in enumerate(schedule):\n",
    "        loss_fn = stage[\"loss_fn\"]\n",
    "        epochs = stage[\"epochs\"]\n",
    "        print(f\"\\n➡️ Stage {i + 1} | Epochs: {epochs} | Loss: {loss_fn.__name__}\")\n",
    "\n",
    "        model.compile(\n",
    "            optimizer=optimizer,\n",
    "            loss=loss_fn,\n",
    "            metrics=['accuracy', Precision(name='precision'), Recall(name='recall')]\n",
    "        )\n",
    "\n",
    "        model.fit(\n",
    "            trainX, trainY,\n",
    "            initial_epoch=current_epoch,\n",
    "            epochs=current_epoch + epochs,\n",
    "            batch_size=1,\n",
    "            validation_data=(testX, testY),\n",
    "            verbose=1\n",
    "        )\n",
    "        current_epoch += epochs\n",
    "    \n",
    "    # Evaluate on test data (accuracy, precision, recall)\n",
    "    eval_results = model.evaluate(testX, testY, verbose=0)\n",
    "    loss = eval_results[0]\n",
    "    accuracy = eval_results[1]\n",
    "    precision = eval_results[2]\n",
    "    recall = eval_results[3]\n",
    "\n",
    "    # Predict and compute F1 score\n",
    "    y_pred_prob = model.predict(testX)\n",
    "    y_pred = (y_pred_prob > 0.5).astype(int)\n",
    "    f1 = f1_score(testY, y_pred)\n",
    "\n",
    "    print(f\"\\n🎯 Finished schedule '{schedule_name}' with test loss: {loss:.4f}, \"\n",
    "          f\"accuracy: {accuracy:.4f}, precision: {precision:.4f}, recall: {recall:.4f}, f1-score: {f1:.4f}\")\n",
    "\n",
    "    # ✅ SHAP explanation (computed after training)\n",
    "    try:\n",
    "        print(\"🔍 Computing SHAP values...\")\n",
    "        background = trainX[np.random.choice(trainX.shape[0], 100, replace=False)]  # Sampled for efficiency\n",
    "        explainer = shap.GradientExplainer(model, background)\n",
    "        shap_values = explainer.shap_values(testX[:100])  # Limit size for performance\n",
    "        print(\"✅ SHAP values computed.\")\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ SHAP computation failed: {e}\")\n",
    "        shap_values = None\n",
    "\n",
    "    return {\n",
    "        \"loss\": loss,\n",
    "        \"accuracy\": accuracy,\n",
    "        \"precision\": precision,\n",
    "        \"recall\": recall,\n",
    "        \"f1\": f1,\n",
    "        \"shap_values\": shap_values\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "1f9b23ed",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🚀 Training schedule: Convex Only\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "➡️ Stage 1 | Epochs: 100 | Loss: convex_focal_loss\n",
      "Epoch 1/100\n",
      "633/633 [==============================] - 7s 8ms/step - loss: 0.7020 - accuracy: 0.5371 - precision: 0.5286 - recall: 0.7003 - val_loss: 0.6662 - val_accuracy: 0.5786 - val_precision: 0.5714 - val_recall: 0.6076\n",
      "Epoch 2/100\n",
      "633/633 [==============================] - 5s 7ms/step - loss: 0.6833 - accuracy: 0.5845 - precision: 0.6071 - recall: 0.4826 - val_loss: 0.6981 - val_accuracy: 0.5597 - val_precision: 0.6452 - val_recall: 0.2532\n",
      "Epoch 3/100\n",
      "633/633 [==============================] - 5s 7ms/step - loss: 0.6745 - accuracy: 0.5845 - precision: 0.6421 - recall: 0.3849 - val_loss: 0.6728 - val_accuracy: 0.5849 - val_precision: 0.6512 - val_recall: 0.3544\n",
      "Epoch 4/100\n",
      "633/633 [==============================] - 5s 7ms/step - loss: 0.6714 - accuracy: 0.5956 - precision: 0.6805 - recall: 0.3628 - val_loss: 0.6790 - val_accuracy: 0.6226 - val_precision: 0.8065 - val_recall: 0.3165\n",
      "Epoch 5/100\n",
      "633/633 [==============================] - 5s 7ms/step - loss: 0.6695 - accuracy: 0.6082 - precision: 0.6906 - recall: 0.3943 - val_loss: 0.6692 - val_accuracy: 0.6101 - val_precision: 0.6604 - val_recall: 0.4430\n",
      "Epoch 6/100\n",
      "633/633 [==============================] - 5s 8ms/step - loss: 0.6592 - accuracy: 0.6193 - precision: 0.6681 - recall: 0.4763 - val_loss: 0.6621 - val_accuracy: 0.6478 - val_precision: 0.7255 - val_recall: 0.4684\n",
      "Epoch 7/100\n",
      "633/633 [==============================] - 5s 8ms/step - loss: 0.6511 - accuracy: 0.6240 - precision: 0.6681 - recall: 0.4953 - val_loss: 0.6577 - val_accuracy: 0.5912 - val_precision: 0.6167 - val_recall: 0.4684\n",
      "Epoch 8/100\n",
      "633/633 [==============================] - 5s 8ms/step - loss: 0.6491 - accuracy: 0.6367 - precision: 0.6706 - recall: 0.5394 - val_loss: 0.6661 - val_accuracy: 0.5975 - val_precision: 0.6531 - val_recall: 0.4051\n",
      "Epoch 9/100\n",
      "633/633 [==============================] - 5s 8ms/step - loss: 0.6481 - accuracy: 0.6524 - precision: 0.7100 - recall: 0.5174 - val_loss: 0.6676 - val_accuracy: 0.5975 - val_precision: 0.6471 - val_recall: 0.4177\n",
      "Epoch 10/100\n",
      "633/633 [==============================] - 5s 7ms/step - loss: 0.6456 - accuracy: 0.6588 - precision: 0.7095 - recall: 0.5394 - val_loss: 0.6605 - val_accuracy: 0.5912 - val_precision: 0.6296 - val_recall: 0.4304\n",
      "Epoch 11/100\n",
      "633/633 [==============================] - 5s 7ms/step - loss: 0.6457 - accuracy: 0.6556 - precision: 0.6911 - recall: 0.5647 - val_loss: 0.6655 - val_accuracy: 0.5849 - val_precision: 0.6182 - val_recall: 0.4304\n",
      "Epoch 12/100\n",
      "633/633 [==============================] - 5s 7ms/step - loss: 0.6428 - accuracy: 0.6572 - precision: 0.6838 - recall: 0.5868 - val_loss: 0.6676 - val_accuracy: 0.6101 - val_precision: 0.6441 - val_recall: 0.4810\n",
      "Epoch 13/100\n",
      "633/633 [==============================] - 5s 7ms/step - loss: 0.6417 - accuracy: 0.6556 - precision: 0.7004 - recall: 0.5457 - val_loss: 0.6671 - val_accuracy: 0.5912 - val_precision: 0.6167 - val_recall: 0.4684\n",
      "Epoch 14/100\n",
      "633/633 [==============================] - 5s 7ms/step - loss: 0.6412 - accuracy: 0.6619 - precision: 0.6929 - recall: 0.5836 - val_loss: 0.6665 - val_accuracy: 0.6038 - val_precision: 0.6290 - val_recall: 0.4937\n",
      "Epoch 15/100\n",
      "633/633 [==============================] - 5s 8ms/step - loss: 0.6438 - accuracy: 0.6682 - precision: 0.7034 - recall: 0.5836 - val_loss: 0.6655 - val_accuracy: 0.5849 - val_precision: 0.6066 - val_recall: 0.4684\n",
      "Epoch 16/100\n",
      "633/633 [==============================] - 5s 7ms/step - loss: 0.6418 - accuracy: 0.6682 - precision: 0.7034 - recall: 0.5836 - val_loss: 0.6661 - val_accuracy: 0.5912 - val_precision: 0.6129 - val_recall: 0.4810\n",
      "Epoch 17/100\n",
      "633/633 [==============================] - 5s 7ms/step - loss: 0.6426 - accuracy: 0.6635 - precision: 0.6898 - recall: 0.5962 - val_loss: 0.6646 - val_accuracy: 0.6038 - val_precision: 0.6379 - val_recall: 0.4684\n",
      "Epoch 18/100\n",
      "633/633 [==============================] - 5s 7ms/step - loss: 0.6421 - accuracy: 0.6556 - precision: 0.6840 - recall: 0.5804 - val_loss: 0.6685 - val_accuracy: 0.5912 - val_precision: 0.6061 - val_recall: 0.5063\n",
      "Epoch 19/100\n",
      "633/633 [==============================] - 5s 7ms/step - loss: 0.6395 - accuracy: 0.6682 - precision: 0.6974 - recall: 0.5962 - val_loss: 0.6645 - val_accuracy: 0.5975 - val_precision: 0.6190 - val_recall: 0.4937\n",
      "Epoch 20/100\n",
      "633/633 [==============================] - 5s 7ms/step - loss: 0.6373 - accuracy: 0.6730 - precision: 0.6993 - recall: 0.6088 - val_loss: 0.6601 - val_accuracy: 0.6101 - val_precision: 0.6308 - val_recall: 0.5190\n",
      "Epoch 21/100\n",
      "633/633 [==============================] - 5s 7ms/step - loss: 0.6365 - accuracy: 0.6698 - precision: 0.6888 - recall: 0.6215 - val_loss: 0.6591 - val_accuracy: 0.6226 - val_precision: 0.6462 - val_recall: 0.5316\n",
      "Epoch 22/100\n",
      "633/633 [==============================] - 5s 7ms/step - loss: 0.6346 - accuracy: 0.6730 - precision: 0.6858 - recall: 0.6404 - val_loss: 0.6602 - val_accuracy: 0.6289 - val_precision: 0.6562 - val_recall: 0.5316\n",
      "Epoch 23/100\n",
      "633/633 [==============================] - 5s 7ms/step - loss: 0.6387 - accuracy: 0.6682 - precision: 0.6877 - recall: 0.6183 - val_loss: 0.6623 - val_accuracy: 0.6352 - val_precision: 0.6615 - val_recall: 0.5443\n",
      "Epoch 24/100\n",
      "633/633 [==============================] - 5s 7ms/step - loss: 0.6375 - accuracy: 0.6730 - precision: 0.6871 - recall: 0.6372 - val_loss: 0.6622 - val_accuracy: 0.6038 - val_precision: 0.6212 - val_recall: 0.5190\n",
      "Epoch 25/100\n",
      "633/633 [==============================] - 5s 7ms/step - loss: 0.6361 - accuracy: 0.6698 - precision: 0.6824 - recall: 0.6372 - val_loss: 0.6613 - val_accuracy: 0.6038 - val_precision: 0.6212 - val_recall: 0.5190\n",
      "Epoch 26/100\n",
      "633/633 [==============================] - 5s 7ms/step - loss: 0.6355 - accuracy: 0.6651 - precision: 0.6756 - recall: 0.6372 - val_loss: 0.6635 - val_accuracy: 0.6226 - val_precision: 0.6338 - val_recall: 0.5696\n",
      "Epoch 27/100\n",
      "633/633 [==============================] - 5s 8ms/step - loss: 0.6339 - accuracy: 0.6667 - precision: 0.6732 - recall: 0.6498 - val_loss: 0.6575 - val_accuracy: 0.6415 - val_precision: 0.6486 - val_recall: 0.6076\n",
      "Epoch 28/100\n",
      "633/633 [==============================] - 5s 7ms/step - loss: 0.6321 - accuracy: 0.6667 - precision: 0.6755 - recall: 0.6435 - val_loss: 0.6610 - val_accuracy: 0.6101 - val_precision: 0.6133 - val_recall: 0.5823\n",
      "Epoch 29/100\n",
      "633/633 [==============================] - 5s 7ms/step - loss: 0.6332 - accuracy: 0.6682 - precision: 0.6766 - recall: 0.6467 - val_loss: 0.6619 - val_accuracy: 0.6038 - val_precision: 0.6111 - val_recall: 0.5570\n",
      "Epoch 30/100\n",
      "633/633 [==============================] - 5s 7ms/step - loss: 0.6316 - accuracy: 0.6714 - precision: 0.6787 - recall: 0.6530 - val_loss: 0.6600 - val_accuracy: 0.5975 - val_precision: 0.6027 - val_recall: 0.5570\n",
      "Epoch 31/100\n",
      "633/633 [==============================] - 5s 7ms/step - loss: 0.6304 - accuracy: 0.6777 - precision: 0.6852 - recall: 0.6593 - val_loss: 0.6606 - val_accuracy: 0.5975 - val_precision: 0.6027 - val_recall: 0.5570\n",
      "Epoch 32/100\n",
      "633/633 [==============================] - 5s 7ms/step - loss: 0.6305 - accuracy: 0.6777 - precision: 0.6840 - recall: 0.6625 - val_loss: 0.6594 - val_accuracy: 0.6038 - val_precision: 0.6111 - val_recall: 0.5570\n",
      "Epoch 33/100\n",
      "633/633 [==============================] - 5s 7ms/step - loss: 0.6299 - accuracy: 0.6793 - precision: 0.6863 - recall: 0.6625 - val_loss: 0.6586 - val_accuracy: 0.6101 - val_precision: 0.6197 - val_recall: 0.5570\n",
      "Epoch 34/100\n",
      "633/633 [==============================] - 5s 7ms/step - loss: 0.6284 - accuracy: 0.6840 - precision: 0.6931 - recall: 0.6625 - val_loss: 0.6589 - val_accuracy: 0.6101 - val_precision: 0.6197 - val_recall: 0.5570\n",
      "Epoch 35/100\n",
      "633/633 [==============================] - 5s 7ms/step - loss: 0.6291 - accuracy: 0.6825 - precision: 0.6895 - recall: 0.6656 - val_loss: 0.6561 - val_accuracy: 0.6226 - val_precision: 0.6301 - val_recall: 0.5823\n",
      "Epoch 36/100\n",
      "633/633 [==============================] - 5s 7ms/step - loss: 0.6270 - accuracy: 0.6856 - precision: 0.6928 - recall: 0.6688 - val_loss: 0.6537 - val_accuracy: 0.6352 - val_precision: 0.6438 - val_recall: 0.5949\n",
      "Epoch 37/100\n",
      "633/633 [==============================] - 5s 7ms/step - loss: 0.6273 - accuracy: 0.6777 - precision: 0.6852 - recall: 0.6593 - val_loss: 0.6546 - val_accuracy: 0.6289 - val_precision: 0.6351 - val_recall: 0.5949\n",
      "Epoch 38/100\n",
      "633/633 [==============================] - 5s 7ms/step - loss: 0.6271 - accuracy: 0.6809 - precision: 0.6885 - recall: 0.6625 - val_loss: 0.6559 - val_accuracy: 0.6226 - val_precision: 0.6267 - val_recall: 0.5949\n",
      "Epoch 39/100\n",
      "633/633 [==============================] - 5s 7ms/step - loss: 0.6258 - accuracy: 0.6777 - precision: 0.6852 - recall: 0.6593 - val_loss: 0.6571 - val_accuracy: 0.6164 - val_precision: 0.6216 - val_recall: 0.5823\n",
      "Epoch 40/100\n",
      "633/633 [==============================] - 5s 7ms/step - loss: 0.6272 - accuracy: 0.6761 - precision: 0.6806 - recall: 0.6656 - val_loss: 0.6570 - val_accuracy: 0.6289 - val_precision: 0.6351 - val_recall: 0.5949\n",
      "Epoch 41/100\n",
      "633/633 [==============================] - 5s 7ms/step - loss: 0.6270 - accuracy: 0.6809 - precision: 0.6849 - recall: 0.6719 - val_loss: 0.6580 - val_accuracy: 0.6226 - val_precision: 0.6267 - val_recall: 0.5949\n",
      "Epoch 42/100\n",
      "633/633 [==============================] - 5s 7ms/step - loss: 0.6259 - accuracy: 0.6856 - precision: 0.6928 - recall: 0.6688 - val_loss: 0.6584 - val_accuracy: 0.6164 - val_precision: 0.6184 - val_recall: 0.5949\n",
      "Epoch 43/100\n",
      "633/633 [==============================] - 5s 7ms/step - loss: 0.6255 - accuracy: 0.6793 - precision: 0.6839 - recall: 0.6688 - val_loss: 0.6566 - val_accuracy: 0.6226 - val_precision: 0.6267 - val_recall: 0.5949\n",
      "Epoch 44/100\n",
      "633/633 [==============================] - 5s 7ms/step - loss: 0.6252 - accuracy: 0.6793 - precision: 0.6839 - recall: 0.6688 - val_loss: 0.6570 - val_accuracy: 0.6226 - val_precision: 0.6267 - val_recall: 0.5949\n",
      "Epoch 45/100\n",
      "633/633 [==============================] - 5s 7ms/step - loss: 0.6247 - accuracy: 0.6777 - precision: 0.6840 - recall: 0.6625 - val_loss: 0.6574 - val_accuracy: 0.6164 - val_precision: 0.6184 - val_recall: 0.5949\n",
      "Epoch 46/100\n",
      "633/633 [==============================] - 5s 7ms/step - loss: 0.6238 - accuracy: 0.6793 - precision: 0.6851 - recall: 0.6656 - val_loss: 0.6574 - val_accuracy: 0.6164 - val_precision: 0.6184 - val_recall: 0.5949\n",
      "Epoch 47/100\n",
      "633/633 [==============================] - 5s 7ms/step - loss: 0.6231 - accuracy: 0.6777 - precision: 0.6828 - recall: 0.6656 - val_loss: 0.6556 - val_accuracy: 0.6164 - val_precision: 0.6216 - val_recall: 0.5823\n",
      "Epoch 48/100\n",
      "633/633 [==============================] - 5s 7ms/step - loss: 0.6231 - accuracy: 0.6793 - precision: 0.6851 - recall: 0.6656 - val_loss: 0.6556 - val_accuracy: 0.6164 - val_precision: 0.6216 - val_recall: 0.5823\n",
      "Epoch 49/100\n",
      "633/633 [==============================] - 5s 7ms/step - loss: 0.6229 - accuracy: 0.6793 - precision: 0.6839 - recall: 0.6688 - val_loss: 0.6534 - val_accuracy: 0.6164 - val_precision: 0.6216 - val_recall: 0.5823\n",
      "Epoch 50/100\n",
      "633/633 [==============================] - 5s 7ms/step - loss: 0.6218 - accuracy: 0.6809 - precision: 0.6849 - recall: 0.6719 - val_loss: 0.6539 - val_accuracy: 0.6164 - val_precision: 0.6216 - val_recall: 0.5823\n",
      "Epoch 51/100\n",
      "633/633 [==============================] - 5s 7ms/step - loss: 0.6221 - accuracy: 0.6825 - precision: 0.6859 - recall: 0.6751 - val_loss: 0.6547 - val_accuracy: 0.6164 - val_precision: 0.6216 - val_recall: 0.5823\n",
      "Epoch 52/100\n",
      "633/633 [==============================] - 5s 7ms/step - loss: 0.6209 - accuracy: 0.6825 - precision: 0.6859 - recall: 0.6751 - val_loss: 0.6544 - val_accuracy: 0.6101 - val_precision: 0.6133 - val_recall: 0.5823\n",
      "Epoch 53/100\n",
      "633/633 [==============================] - 5s 7ms/step - loss: 0.6210 - accuracy: 0.6840 - precision: 0.6881 - recall: 0.6751 - val_loss: 0.6556 - val_accuracy: 0.6101 - val_precision: 0.6133 - val_recall: 0.5823\n",
      "Epoch 54/100\n",
      "633/633 [==============================] - 5s 7ms/step - loss: 0.6225 - accuracy: 0.6856 - precision: 0.6903 - recall: 0.6751 - val_loss: 0.6559 - val_accuracy: 0.6101 - val_precision: 0.6133 - val_recall: 0.5823\n",
      "Epoch 55/100\n",
      "633/633 [==============================] - 5s 7ms/step - loss: 0.6212 - accuracy: 0.6856 - precision: 0.6903 - recall: 0.6751 - val_loss: 0.6565 - val_accuracy: 0.6038 - val_precision: 0.6081 - val_recall: 0.5696\n",
      "Epoch 56/100\n",
      "633/633 [==============================] - 5s 7ms/step - loss: 0.6208 - accuracy: 0.6825 - precision: 0.6859 - recall: 0.6751 - val_loss: 0.6558 - val_accuracy: 0.6101 - val_precision: 0.6133 - val_recall: 0.5823\n",
      "Epoch 57/100\n",
      "633/633 [==============================] - 5s 7ms/step - loss: 0.6201 - accuracy: 0.6793 - precision: 0.6851 - recall: 0.6656 - val_loss: 0.6565 - val_accuracy: 0.6101 - val_precision: 0.6133 - val_recall: 0.5823\n",
      "Epoch 58/100\n",
      "633/633 [==============================] - 5s 7ms/step - loss: 0.6202 - accuracy: 0.6825 - precision: 0.6871 - recall: 0.6719 - val_loss: 0.6557 - val_accuracy: 0.6038 - val_precision: 0.6081 - val_recall: 0.5696\n",
      "Epoch 59/100\n",
      "633/633 [==============================] - 5s 7ms/step - loss: 0.6186 - accuracy: 0.6809 - precision: 0.6861 - recall: 0.6688 - val_loss: 0.6552 - val_accuracy: 0.6101 - val_precision: 0.6164 - val_recall: 0.5696\n",
      "Epoch 60/100\n",
      "633/633 [==============================] - 5s 7ms/step - loss: 0.6182 - accuracy: 0.6825 - precision: 0.6871 - recall: 0.6719 - val_loss: 0.6553 - val_accuracy: 0.6101 - val_precision: 0.6164 - val_recall: 0.5696\n",
      "Epoch 61/100\n",
      "633/633 [==============================] - 5s 7ms/step - loss: 0.6180 - accuracy: 0.6825 - precision: 0.6871 - recall: 0.6719 - val_loss: 0.6553 - val_accuracy: 0.6101 - val_precision: 0.6164 - val_recall: 0.5696\n",
      "Epoch 62/100\n",
      "633/633 [==============================] - 5s 7ms/step - loss: 0.6178 - accuracy: 0.6825 - precision: 0.6871 - recall: 0.6719 - val_loss: 0.6552 - val_accuracy: 0.6101 - val_precision: 0.6164 - val_recall: 0.5696\n",
      "Epoch 63/100\n",
      "633/633 [==============================] - 5s 7ms/step - loss: 0.6178 - accuracy: 0.6825 - precision: 0.6871 - recall: 0.6719 - val_loss: 0.6552 - val_accuracy: 0.6101 - val_precision: 0.6164 - val_recall: 0.5696\n",
      "Epoch 64/100\n",
      "633/633 [==============================] - 5s 7ms/step - loss: 0.6182 - accuracy: 0.6840 - precision: 0.6893 - recall: 0.6719 - val_loss: 0.6545 - val_accuracy: 0.6101 - val_precision: 0.6164 - val_recall: 0.5696\n",
      "Epoch 65/100\n",
      "633/633 [==============================] - 5s 7ms/step - loss: 0.6180 - accuracy: 0.6856 - precision: 0.6916 - recall: 0.6719 - val_loss: 0.6538 - val_accuracy: 0.6101 - val_precision: 0.6164 - val_recall: 0.5696\n",
      "Epoch 66/100\n",
      "633/633 [==============================] - 5s 7ms/step - loss: 0.6172 - accuracy: 0.6856 - precision: 0.6916 - recall: 0.6719 - val_loss: 0.6539 - val_accuracy: 0.6101 - val_precision: 0.6164 - val_recall: 0.5696\n",
      "Epoch 67/100\n",
      "633/633 [==============================] - 5s 7ms/step - loss: 0.6173 - accuracy: 0.6856 - precision: 0.6916 - recall: 0.6719 - val_loss: 0.6537 - val_accuracy: 0.6101 - val_precision: 0.6164 - val_recall: 0.5696\n",
      "Epoch 68/100\n",
      "633/633 [==============================] - 5s 7ms/step - loss: 0.6172 - accuracy: 0.6840 - precision: 0.6906 - recall: 0.6688 - val_loss: 0.6526 - val_accuracy: 0.6164 - val_precision: 0.6216 - val_recall: 0.5823\n",
      "Epoch 69/100\n",
      "633/633 [==============================] - 5s 7ms/step - loss: 0.6171 - accuracy: 0.6840 - precision: 0.6906 - recall: 0.6688 - val_loss: 0.6535 - val_accuracy: 0.6164 - val_precision: 0.6216 - val_recall: 0.5823\n",
      "Epoch 70/100\n",
      "633/633 [==============================] - 5s 7ms/step - loss: 0.6169 - accuracy: 0.6840 - precision: 0.6906 - recall: 0.6688 - val_loss: 0.6530 - val_accuracy: 0.6164 - val_precision: 0.6216 - val_recall: 0.5823\n",
      "Epoch 71/100\n",
      "633/633 [==============================] - 5s 7ms/step - loss: 0.6166 - accuracy: 0.6840 - precision: 0.6906 - recall: 0.6688 - val_loss: 0.6520 - val_accuracy: 0.6164 - val_precision: 0.6216 - val_recall: 0.5823\n",
      "Epoch 72/100\n",
      "633/633 [==============================] - 5s 7ms/step - loss: 0.6164 - accuracy: 0.6856 - precision: 0.6928 - recall: 0.6688 - val_loss: 0.6520 - val_accuracy: 0.6164 - val_precision: 0.6216 - val_recall: 0.5823\n",
      "Epoch 73/100\n",
      "633/633 [==============================] - 5s 7ms/step - loss: 0.6163 - accuracy: 0.6856 - precision: 0.6928 - recall: 0.6688 - val_loss: 0.6527 - val_accuracy: 0.6164 - val_precision: 0.6216 - val_recall: 0.5823\n",
      "Epoch 74/100\n",
      "633/633 [==============================] - 5s 7ms/step - loss: 0.6163 - accuracy: 0.6856 - precision: 0.6928 - recall: 0.6688 - val_loss: 0.6512 - val_accuracy: 0.6164 - val_precision: 0.6216 - val_recall: 0.5823\n",
      "Epoch 75/100\n",
      "633/633 [==============================] - 5s 7ms/step - loss: 0.6162 - accuracy: 0.6840 - precision: 0.6906 - recall: 0.6688 - val_loss: 0.6520 - val_accuracy: 0.6164 - val_precision: 0.6216 - val_recall: 0.5823\n",
      "Epoch 76/100\n",
      "633/633 [==============================] - 5s 7ms/step - loss: 0.6163 - accuracy: 0.6840 - precision: 0.6906 - recall: 0.6688 - val_loss: 0.6512 - val_accuracy: 0.6164 - val_precision: 0.6216 - val_recall: 0.5823\n",
      "Epoch 77/100\n",
      "633/633 [==============================] - 5s 7ms/step - loss: 0.6161 - accuracy: 0.6856 - precision: 0.6928 - recall: 0.6688 - val_loss: 0.6512 - val_accuracy: 0.6164 - val_precision: 0.6216 - val_recall: 0.5823\n",
      "Epoch 78/100\n",
      "633/633 [==============================] - 5s 7ms/step - loss: 0.6160 - accuracy: 0.6856 - precision: 0.6928 - recall: 0.6688 - val_loss: 0.6511 - val_accuracy: 0.6164 - val_precision: 0.6216 - val_recall: 0.5823\n",
      "Epoch 79/100\n",
      "633/633 [==============================] - 5s 7ms/step - loss: 0.6160 - accuracy: 0.6856 - precision: 0.6928 - recall: 0.6688 - val_loss: 0.6511 - val_accuracy: 0.6164 - val_precision: 0.6216 - val_recall: 0.5823\n",
      "Epoch 80/100\n",
      "633/633 [==============================] - 5s 7ms/step - loss: 0.6158 - accuracy: 0.6856 - precision: 0.6928 - recall: 0.6688 - val_loss: 0.6510 - val_accuracy: 0.6164 - val_precision: 0.6216 - val_recall: 0.5823\n",
      "Epoch 81/100\n",
      "633/633 [==============================] - 5s 7ms/step - loss: 0.6158 - accuracy: 0.6840 - precision: 0.6906 - recall: 0.6688 - val_loss: 0.6512 - val_accuracy: 0.6164 - val_precision: 0.6216 - val_recall: 0.5823\n",
      "Epoch 82/100\n",
      "633/633 [==============================] - 5s 7ms/step - loss: 0.6156 - accuracy: 0.6856 - precision: 0.6928 - recall: 0.6688 - val_loss: 0.6512 - val_accuracy: 0.6164 - val_precision: 0.6216 - val_recall: 0.5823\n",
      "Epoch 83/100\n",
      "633/633 [==============================] - 5s 8ms/step - loss: 0.6156 - accuracy: 0.6856 - precision: 0.6928 - recall: 0.6688 - val_loss: 0.6513 - val_accuracy: 0.6164 - val_precision: 0.6216 - val_recall: 0.5823\n",
      "Epoch 84/100\n",
      "633/633 [==============================] - 5s 7ms/step - loss: 0.6154 - accuracy: 0.6872 - precision: 0.6951 - recall: 0.6688 - val_loss: 0.6512 - val_accuracy: 0.6164 - val_precision: 0.6216 - val_recall: 0.5823\n",
      "Epoch 85/100\n",
      "633/633 [==============================] - 5s 7ms/step - loss: 0.6154 - accuracy: 0.6872 - precision: 0.6951 - recall: 0.6688 - val_loss: 0.6518 - val_accuracy: 0.6164 - val_precision: 0.6216 - val_recall: 0.5823\n",
      "Epoch 86/100\n",
      "633/633 [==============================] - 5s 7ms/step - loss: 0.6154 - accuracy: 0.6872 - precision: 0.6951 - recall: 0.6688 - val_loss: 0.6518 - val_accuracy: 0.6164 - val_precision: 0.6216 - val_recall: 0.5823\n",
      "Epoch 87/100\n",
      "633/633 [==============================] - 5s 7ms/step - loss: 0.6154 - accuracy: 0.6856 - precision: 0.6928 - recall: 0.6688 - val_loss: 0.6519 - val_accuracy: 0.6164 - val_precision: 0.6216 - val_recall: 0.5823\n",
      "Epoch 88/100\n",
      "633/633 [==============================] - 5s 7ms/step - loss: 0.6154 - accuracy: 0.6856 - precision: 0.6928 - recall: 0.6688 - val_loss: 0.6519 - val_accuracy: 0.6164 - val_precision: 0.6216 - val_recall: 0.5823\n",
      "Epoch 89/100\n",
      "633/633 [==============================] - 5s 7ms/step - loss: 0.6154 - accuracy: 0.6856 - precision: 0.6928 - recall: 0.6688 - val_loss: 0.6519 - val_accuracy: 0.6164 - val_precision: 0.6216 - val_recall: 0.5823\n",
      "Epoch 90/100\n",
      "633/633 [==============================] - 5s 7ms/step - loss: 0.6152 - accuracy: 0.6856 - precision: 0.6928 - recall: 0.6688 - val_loss: 0.6519 - val_accuracy: 0.6164 - val_precision: 0.6216 - val_recall: 0.5823\n",
      "Epoch 91/100\n",
      "633/633 [==============================] - 5s 7ms/step - loss: 0.6152 - accuracy: 0.6856 - precision: 0.6928 - recall: 0.6688 - val_loss: 0.6519 - val_accuracy: 0.6164 - val_precision: 0.6216 - val_recall: 0.5823\n",
      "Epoch 92/100\n",
      "633/633 [==============================] - 5s 7ms/step - loss: 0.6152 - accuracy: 0.6856 - precision: 0.6928 - recall: 0.6688 - val_loss: 0.6519 - val_accuracy: 0.6164 - val_precision: 0.6216 - val_recall: 0.5823\n",
      "Epoch 93/100\n",
      "633/633 [==============================] - 5s 7ms/step - loss: 0.6152 - accuracy: 0.6856 - precision: 0.6928 - recall: 0.6688 - val_loss: 0.6519 - val_accuracy: 0.6164 - val_precision: 0.6216 - val_recall: 0.5823\n",
      "Epoch 94/100\n",
      "633/633 [==============================] - 5s 7ms/step - loss: 0.6152 - accuracy: 0.6856 - precision: 0.6928 - recall: 0.6688 - val_loss: 0.6519 - val_accuracy: 0.6164 - val_precision: 0.6216 - val_recall: 0.5823\n",
      "Epoch 95/100\n",
      "633/633 [==============================] - 5s 7ms/step - loss: 0.6152 - accuracy: 0.6856 - precision: 0.6928 - recall: 0.6688 - val_loss: 0.6519 - val_accuracy: 0.6164 - val_precision: 0.6216 - val_recall: 0.5823\n",
      "Epoch 96/100\n",
      "633/633 [==============================] - 5s 7ms/step - loss: 0.6151 - accuracy: 0.6856 - precision: 0.6928 - recall: 0.6688 - val_loss: 0.6519 - val_accuracy: 0.6164 - val_precision: 0.6216 - val_recall: 0.5823\n",
      "Epoch 97/100\n",
      "633/633 [==============================] - 5s 7ms/step - loss: 0.6151 - accuracy: 0.6856 - precision: 0.6928 - recall: 0.6688 - val_loss: 0.6518 - val_accuracy: 0.6164 - val_precision: 0.6216 - val_recall: 0.5823\n",
      "Epoch 98/100\n",
      "633/633 [==============================] - 5s 7ms/step - loss: 0.6151 - accuracy: 0.6856 - precision: 0.6928 - recall: 0.6688 - val_loss: 0.6515 - val_accuracy: 0.6164 - val_precision: 0.6216 - val_recall: 0.5823\n",
      "Epoch 99/100\n",
      "633/633 [==============================] - 5s 7ms/step - loss: 0.6151 - accuracy: 0.6856 - precision: 0.6928 - recall: 0.6688 - val_loss: 0.6515 - val_accuracy: 0.6164 - val_precision: 0.6216 - val_recall: 0.5823\n",
      "Epoch 100/100\n",
      "633/633 [==============================] - 5s 7ms/step - loss: 0.6151 - accuracy: 0.6856 - precision: 0.6928 - recall: 0.6688 - val_loss: 0.6515 - val_accuracy: 0.6164 - val_precision: 0.6216 - val_recall: 0.5823\n",
      "5/5 [==============================] - 1s 9ms/step\n",
      "\n",
      "🎯 Finished schedule 'Convex Only' with test loss: 0.6515, accuracy: 0.6164, precision: 0.6216, recall: 0.5823, f1-score: 0.6013\n",
      "🔍 Computing SHAP values...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/boabangfrancis/anaconda3/lib/python3.11/site-packages/keras/src/backend.py:452: UserWarning: `tf.keras.backend.set_learning_phase` is deprecated and will be removed after 2020-10-11. To update it, simply pass a True/False value to the `training` argument of the `__call__` method of your layer or model.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ SHAP values computed.\n",
      "\n",
      "🚀 Training schedule: Multistage\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "➡️ Stage 1 | Epochs: 15 | Loss: convex_focal_loss\n",
      "Epoch 1/15\n",
      "633/633 [==============================] - 7s 9ms/step - loss: 0.7077 - accuracy: 0.5276 - precision: 0.5429 - recall: 0.3596 - val_loss: 0.7073 - val_accuracy: 0.5157 - val_precision: 0.5152 - val_recall: 0.4304\n",
      "Epoch 2/15\n",
      "633/633 [==============================] - 5s 8ms/step - loss: 0.6865 - accuracy: 0.5434 - precision: 0.5574 - recall: 0.4290 - val_loss: 0.7134 - val_accuracy: 0.5157 - val_precision: 0.5128 - val_recall: 0.5063\n",
      "Epoch 3/15\n",
      "633/633 [==============================] - 5s 8ms/step - loss: 0.6820 - accuracy: 0.5545 - precision: 0.5593 - recall: 0.5205 - val_loss: 0.7042 - val_accuracy: 0.4906 - val_precision: 0.4828 - val_recall: 0.3544\n",
      "Epoch 4/15\n",
      "633/633 [==============================] - 5s 8ms/step - loss: 0.6752 - accuracy: 0.5750 - precision: 0.5828 - recall: 0.5331 - val_loss: 0.6988 - val_accuracy: 0.5157 - val_precision: 0.5132 - val_recall: 0.4937\n",
      "Epoch 5/15\n",
      "633/633 [==============================] - 5s 8ms/step - loss: 0.6804 - accuracy: 0.5561 - precision: 0.5608 - recall: 0.5237 - val_loss: 0.6921 - val_accuracy: 0.5220 - val_precision: 0.5190 - val_recall: 0.5190\n",
      "Epoch 6/15\n",
      "633/633 [==============================] - 5s 7ms/step - loss: 0.6739 - accuracy: 0.5908 - precision: 0.5973 - recall: 0.5615 - val_loss: 0.7046 - val_accuracy: 0.5346 - val_precision: 0.5342 - val_recall: 0.4937\n",
      "Epoch 7/15\n",
      "633/633 [==============================] - 5s 7ms/step - loss: 0.6706 - accuracy: 0.6098 - precision: 0.6151 - recall: 0.5899 - val_loss: 0.6955 - val_accuracy: 0.5472 - val_precision: 0.5467 - val_recall: 0.5190\n",
      "Epoch 8/15\n",
      "633/633 [==============================] - 5s 7ms/step - loss: 0.6708 - accuracy: 0.5924 - precision: 0.6175 - recall: 0.4890 - val_loss: 0.6927 - val_accuracy: 0.5157 - val_precision: 0.5161 - val_recall: 0.4051\n",
      "Epoch 9/15\n",
      "633/633 [==============================] - 5s 8ms/step - loss: 0.6649 - accuracy: 0.5829 - precision: 0.5993 - recall: 0.5047 - val_loss: 0.6993 - val_accuracy: 0.4591 - val_precision: 0.4478 - val_recall: 0.3797\n",
      "Epoch 10/15\n",
      "633/633 [==============================] - 5s 8ms/step - loss: 0.6713 - accuracy: 0.5845 - precision: 0.5931 - recall: 0.5426 - val_loss: 0.6944 - val_accuracy: 0.4906 - val_precision: 0.4868 - val_recall: 0.4684\n",
      "Epoch 11/15\n",
      "633/633 [==============================] - 5s 8ms/step - loss: 0.6686 - accuracy: 0.5972 - precision: 0.6020 - recall: 0.5773 - val_loss: 0.7002 - val_accuracy: 0.4780 - val_precision: 0.4737 - val_recall: 0.4557\n",
      "Epoch 12/15\n",
      "633/633 [==============================] - 5s 8ms/step - loss: 0.6647 - accuracy: 0.6051 - precision: 0.6128 - recall: 0.5741 - val_loss: 0.6993 - val_accuracy: 0.4969 - val_precision: 0.4933 - val_recall: 0.4684\n",
      "Epoch 13/15\n",
      "633/633 [==============================] - 5s 8ms/step - loss: 0.6616 - accuracy: 0.6114 - precision: 0.6134 - recall: 0.6057 - val_loss: 0.6850 - val_accuracy: 0.5535 - val_precision: 0.5444 - val_recall: 0.6203\n",
      "Epoch 14/15\n",
      "633/633 [==============================] - 5s 8ms/step - loss: 0.6642 - accuracy: 0.6145 - precision: 0.6205 - recall: 0.5931 - val_loss: 0.7002 - val_accuracy: 0.5346 - val_precision: 0.5362 - val_recall: 0.4684\n",
      "Epoch 15/15\n",
      "633/633 [==============================] - 5s 8ms/step - loss: 0.6611 - accuracy: 0.6130 - precision: 0.6429 - recall: 0.5110 - val_loss: 0.6911 - val_accuracy: 0.5220 - val_precision: 0.5273 - val_recall: 0.3671\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "➡️ Stage 2 | Epochs: 95 | Loss: nonconvex_focal_loss\n",
      "Epoch 16/110\n",
      "633/633 [==============================] - 6s 8ms/step - loss: 0.0059 - accuracy: 0.5829 - precision: 0.5936 - recall: 0.5300 - val_loss: 0.0043 - val_accuracy: 0.5660 - val_precision: 0.5758 - val_recall: 0.4810\n",
      "Epoch 17/110\n",
      "633/633 [==============================] - 5s 7ms/step - loss: 0.0046 - accuracy: 0.6367 - precision: 0.6381 - recall: 0.6341 - val_loss: 0.0041 - val_accuracy: 0.5786 - val_precision: 0.5732 - val_recall: 0.5949\n",
      "Epoch 18/110\n",
      "633/633 [==============================] - 5s 7ms/step - loss: 0.0046 - accuracy: 0.6145 - precision: 0.6083 - recall: 0.6467 - val_loss: 0.0048 - val_accuracy: 0.5786 - val_precision: 0.5652 - val_recall: 0.6582\n",
      "Epoch 19/110\n",
      "633/633 [==============================] - 5s 7ms/step - loss: 0.0048 - accuracy: 0.5877 - precision: 0.5838 - recall: 0.6151 - val_loss: 0.0042 - val_accuracy: 0.5409 - val_precision: 0.5357 - val_recall: 0.5696\n",
      "Epoch 20/110\n",
      "633/633 [==============================] - 5s 7ms/step - loss: 0.0041 - accuracy: 0.6288 - precision: 0.6171 - recall: 0.6814 - val_loss: 0.0043 - val_accuracy: 0.5597 - val_precision: 0.5517 - val_recall: 0.6076\n",
      "Epoch 21/110\n",
      "633/633 [==============================] - 5s 7ms/step - loss: 0.0040 - accuracy: 0.6240 - precision: 0.6179 - recall: 0.6530 - val_loss: 0.0039 - val_accuracy: 0.5849 - val_precision: 0.5823 - val_recall: 0.5823\n",
      "Epoch 22/110\n",
      "633/633 [==============================] - 5s 7ms/step - loss: 0.0040 - accuracy: 0.6209 - precision: 0.6156 - recall: 0.6467 - val_loss: 0.0041 - val_accuracy: 0.5849 - val_precision: 0.5802 - val_recall: 0.5949\n",
      "Epoch 23/110\n",
      "633/633 [==============================] - 5s 7ms/step - loss: 0.0038 - accuracy: 0.6272 - precision: 0.6160 - recall: 0.6782 - val_loss: 0.0039 - val_accuracy: 0.5975 - val_precision: 0.5824 - val_recall: 0.6709\n",
      "Epoch 24/110\n",
      "633/633 [==============================] - 5s 7ms/step - loss: 0.0036 - accuracy: 0.6272 - precision: 0.6154 - recall: 0.6814 - val_loss: 0.0037 - val_accuracy: 0.5912 - val_precision: 0.5833 - val_recall: 0.6203\n",
      "Epoch 25/110\n",
      "633/633 [==============================] - 5s 7ms/step - loss: 0.0035 - accuracy: 0.6240 - precision: 0.6152 - recall: 0.6656 - val_loss: 0.0037 - val_accuracy: 0.5723 - val_precision: 0.5647 - val_recall: 0.6076\n",
      "Epoch 26/110\n",
      "633/633 [==============================] - 5s 7ms/step - loss: 0.0033 - accuracy: 0.6398 - precision: 0.6328 - recall: 0.6688 - val_loss: 0.0034 - val_accuracy: 0.5975 - val_precision: 0.5926 - val_recall: 0.6076\n",
      "Epoch 27/110\n",
      "633/633 [==============================] - 5s 7ms/step - loss: 0.0032 - accuracy: 0.6303 - precision: 0.6285 - recall: 0.6404 - val_loss: 0.0033 - val_accuracy: 0.5849 - val_precision: 0.5867 - val_recall: 0.5570\n",
      "Epoch 28/110\n",
      "633/633 [==============================] - 5s 7ms/step - loss: 0.0031 - accuracy: 0.6272 - precision: 0.6286 - recall: 0.6246 - val_loss: 0.0032 - val_accuracy: 0.5849 - val_precision: 0.5802 - val_recall: 0.5949\n",
      "Epoch 29/110\n",
      "633/633 [==============================] - 5s 7ms/step - loss: 0.0030 - accuracy: 0.6398 - precision: 0.6336 - recall: 0.6656 - val_loss: 0.0033 - val_accuracy: 0.5849 - val_precision: 0.5783 - val_recall: 0.6076\n",
      "Epoch 30/110\n",
      "633/633 [==============================] - 5s 7ms/step - loss: 0.0032 - accuracy: 0.6461 - precision: 0.6431 - recall: 0.6593 - val_loss: 0.0036 - val_accuracy: 0.5786 - val_precision: 0.5698 - val_recall: 0.6203\n",
      "Epoch 31/110\n",
      "633/633 [==============================] - 5s 7ms/step - loss: 0.0031 - accuracy: 0.6382 - precision: 0.6310 - recall: 0.6688 - val_loss: 0.0035 - val_accuracy: 0.6038 - val_precision: 0.6026 - val_recall: 0.5949\n",
      "Epoch 32/110\n",
      "633/633 [==============================] - 5s 7ms/step - loss: 0.0031 - accuracy: 0.6430 - precision: 0.6409 - recall: 0.6530 - val_loss: 0.0037 - val_accuracy: 0.5849 - val_precision: 0.5823 - val_recall: 0.5823\n",
      "Epoch 33/110\n",
      "633/633 [==============================] - 5s 7ms/step - loss: 0.0031 - accuracy: 0.6398 - precision: 0.6344 - recall: 0.6625 - val_loss: 0.0037 - val_accuracy: 0.5786 - val_precision: 0.5714 - val_recall: 0.6076\n",
      "Epoch 34/110\n",
      "633/633 [==============================] - 5s 7ms/step - loss: 0.0030 - accuracy: 0.6524 - precision: 0.6431 - recall: 0.6877 - val_loss: 0.0035 - val_accuracy: 0.5660 - val_precision: 0.5641 - val_recall: 0.5570\n",
      "Epoch 35/110\n",
      "633/633 [==============================] - 5s 7ms/step - loss: 0.0030 - accuracy: 0.6556 - precision: 0.6495 - recall: 0.6782 - val_loss: 0.0035 - val_accuracy: 0.5975 - val_precision: 0.5904 - val_recall: 0.6203\n",
      "Epoch 36/110\n",
      "633/633 [==============================] - 5s 7ms/step - loss: 0.0029 - accuracy: 0.6572 - precision: 0.6497 - recall: 0.6845 - val_loss: 0.0034 - val_accuracy: 0.5912 - val_precision: 0.5921 - val_recall: 0.5696\n",
      "Epoch 37/110\n",
      "633/633 [==============================] - 5s 7ms/step - loss: 0.0029 - accuracy: 0.6603 - precision: 0.6564 - recall: 0.6751 - val_loss: 0.0033 - val_accuracy: 0.6226 - val_precision: 0.6118 - val_recall: 0.6582\n",
      "Epoch 38/110\n",
      "633/633 [==============================] - 5s 7ms/step - loss: 0.0028 - accuracy: 0.6682 - precision: 0.6597 - recall: 0.6972 - val_loss: 0.0033 - val_accuracy: 0.6415 - val_precision: 0.6310 - val_recall: 0.6709\n",
      "Epoch 39/110\n",
      "633/633 [==============================] - 5s 7ms/step - loss: 0.0028 - accuracy: 0.6809 - precision: 0.6748 - recall: 0.7003 - val_loss: 0.0033 - val_accuracy: 0.6226 - val_precision: 0.6118 - val_recall: 0.6582\n",
      "Epoch 40/110\n",
      "633/633 [==============================] - 5s 7ms/step - loss: 0.0027 - accuracy: 0.6730 - precision: 0.6698 - recall: 0.6845 - val_loss: 0.0032 - val_accuracy: 0.6226 - val_precision: 0.6145 - val_recall: 0.6456\n",
      "Epoch 41/110\n",
      "633/633 [==============================] - 5s 7ms/step - loss: 0.0028 - accuracy: 0.6761 - precision: 0.6728 - recall: 0.6877 - val_loss: 0.0033 - val_accuracy: 0.6164 - val_precision: 0.6125 - val_recall: 0.6203\n",
      "Epoch 42/110\n",
      "633/633 [==============================] - 5s 7ms/step - loss: 0.0028 - accuracy: 0.6714 - precision: 0.6637 - recall: 0.6972 - val_loss: 0.0034 - val_accuracy: 0.6164 - val_precision: 0.6125 - val_recall: 0.6203\n",
      "Epoch 43/110\n",
      "633/633 [==============================] - 5s 7ms/step - loss: 0.0028 - accuracy: 0.6746 - precision: 0.6687 - recall: 0.6940 - val_loss: 0.0034 - val_accuracy: 0.5975 - val_precision: 0.5974 - val_recall: 0.5823\n",
      "Epoch 44/110\n",
      "633/633 [==============================] - 5s 7ms/step - loss: 0.0028 - accuracy: 0.6698 - precision: 0.6656 - recall: 0.6845 - val_loss: 0.0034 - val_accuracy: 0.6038 - val_precision: 0.6026 - val_recall: 0.5949\n",
      "Epoch 45/110\n",
      "633/633 [==============================] - 5s 7ms/step - loss: 0.0027 - accuracy: 0.6635 - precision: 0.6625 - recall: 0.6688 - val_loss: 0.0033 - val_accuracy: 0.6101 - val_precision: 0.6049 - val_recall: 0.6203\n",
      "Epoch 46/110\n",
      "633/633 [==============================] - 5s 7ms/step - loss: 0.0027 - accuracy: 0.6682 - precision: 0.6646 - recall: 0.6814 - val_loss: 0.0033 - val_accuracy: 0.5975 - val_precision: 0.5949 - val_recall: 0.5949\n",
      "Epoch 47/110\n",
      "633/633 [==============================] - 5s 8ms/step - loss: 0.0027 - accuracy: 0.6619 - precision: 0.6625 - recall: 0.6625 - val_loss: 0.0033 - val_accuracy: 0.5975 - val_precision: 0.5949 - val_recall: 0.5949\n",
      "Epoch 48/110\n",
      "633/633 [==============================] - 5s 7ms/step - loss: 0.0027 - accuracy: 0.6619 - precision: 0.6645 - recall: 0.6562 - val_loss: 0.0033 - val_accuracy: 0.5975 - val_precision: 0.5949 - val_recall: 0.5949\n",
      "Epoch 49/110\n",
      "633/633 [==============================] - 5s 7ms/step - loss: 0.0027 - accuracy: 0.6698 - precision: 0.6698 - recall: 0.6719 - val_loss: 0.0033 - val_accuracy: 0.5975 - val_precision: 0.5949 - val_recall: 0.5949\n",
      "Epoch 50/110\n",
      "633/633 [==============================] - 5s 7ms/step - loss: 0.0027 - accuracy: 0.6651 - precision: 0.6656 - recall: 0.6656 - val_loss: 0.0033 - val_accuracy: 0.5975 - val_precision: 0.5949 - val_recall: 0.5949\n",
      "Epoch 51/110\n",
      "633/633 [==============================] - 5s 7ms/step - loss: 0.0027 - accuracy: 0.6635 - precision: 0.6656 - recall: 0.6593 - val_loss: 0.0032 - val_accuracy: 0.6038 - val_precision: 0.6026 - val_recall: 0.5949\n",
      "Epoch 52/110\n",
      "633/633 [==============================] - 5s 7ms/step - loss: 0.0026 - accuracy: 0.6572 - precision: 0.6582 - recall: 0.6562 - val_loss: 0.0032 - val_accuracy: 0.6038 - val_precision: 0.6026 - val_recall: 0.5949\n",
      "Epoch 53/110\n",
      "633/633 [==============================] - 5s 7ms/step - loss: 0.0026 - accuracy: 0.6635 - precision: 0.6667 - recall: 0.6562 - val_loss: 0.0033 - val_accuracy: 0.6038 - val_precision: 0.6026 - val_recall: 0.5949\n",
      "Epoch 54/110\n",
      "633/633 [==============================] - 5s 7ms/step - loss: 0.0027 - accuracy: 0.6524 - precision: 0.6530 - recall: 0.6530 - val_loss: 0.0033 - val_accuracy: 0.6164 - val_precision: 0.6125 - val_recall: 0.6203\n",
      "Epoch 55/110\n",
      "633/633 [==============================] - 5s 7ms/step - loss: 0.0027 - accuracy: 0.6572 - precision: 0.6553 - recall: 0.6656 - val_loss: 0.0033 - val_accuracy: 0.6038 - val_precision: 0.6000 - val_recall: 0.6076\n",
      "Epoch 56/110\n",
      "633/633 [==============================] - 5s 7ms/step - loss: 0.0027 - accuracy: 0.6509 - precision: 0.6519 - recall: 0.6498 - val_loss: 0.0033 - val_accuracy: 0.6101 - val_precision: 0.6049 - val_recall: 0.6203\n",
      "Epoch 57/110\n",
      "633/633 [==============================] - 5s 7ms/step - loss: 0.0026 - accuracy: 0.6588 - precision: 0.6563 - recall: 0.6688 - val_loss: 0.0031 - val_accuracy: 0.5975 - val_precision: 0.5949 - val_recall: 0.5949\n",
      "Epoch 58/110\n",
      "633/633 [==============================] - 5s 8ms/step - loss: 0.0026 - accuracy: 0.6603 - precision: 0.6584 - recall: 0.6688 - val_loss: 0.0032 - val_accuracy: 0.5975 - val_precision: 0.5949 - val_recall: 0.5949\n",
      "Epoch 59/110\n",
      "633/633 [==============================] - 5s 7ms/step - loss: 0.0026 - accuracy: 0.6635 - precision: 0.6667 - recall: 0.6562 - val_loss: 0.0032 - val_accuracy: 0.5975 - val_precision: 0.5949 - val_recall: 0.5949\n",
      "Epoch 60/110\n",
      "633/633 [==============================] - 5s 7ms/step - loss: 0.0026 - accuracy: 0.6588 - precision: 0.6603 - recall: 0.6562 - val_loss: 0.0032 - val_accuracy: 0.5975 - val_precision: 0.5949 - val_recall: 0.5949\n",
      "Epoch 61/110\n",
      "633/633 [==============================] - 5s 7ms/step - loss: 0.0026 - accuracy: 0.6667 - precision: 0.6688 - recall: 0.6625 - val_loss: 0.0031 - val_accuracy: 0.5912 - val_precision: 0.5897 - val_recall: 0.5823\n",
      "Epoch 62/110\n",
      "633/633 [==============================] - 5s 7ms/step - loss: 0.0026 - accuracy: 0.6635 - precision: 0.6677 - recall: 0.6530 - val_loss: 0.0031 - val_accuracy: 0.6164 - val_precision: 0.6154 - val_recall: 0.6076\n",
      "Epoch 63/110\n",
      "633/633 [==============================] - 5s 7ms/step - loss: 0.0026 - accuracy: 0.6635 - precision: 0.6667 - recall: 0.6562 - val_loss: 0.0031 - val_accuracy: 0.6164 - val_precision: 0.6154 - val_recall: 0.6076\n",
      "Epoch 64/110\n",
      "633/633 [==============================] - 5s 7ms/step - loss: 0.0026 - accuracy: 0.6714 - precision: 0.6775 - recall: 0.6562 - val_loss: 0.0031 - val_accuracy: 0.6289 - val_precision: 0.6250 - val_recall: 0.6329\n",
      "Epoch 65/110\n",
      "633/633 [==============================] - 5s 7ms/step - loss: 0.0027 - accuracy: 0.6682 - precision: 0.6720 - recall: 0.6593 - val_loss: 0.0032 - val_accuracy: 0.6289 - val_precision: 0.6250 - val_recall: 0.6329\n",
      "Epoch 66/110\n",
      "633/633 [==============================] - 5s 7ms/step - loss: 0.0026 - accuracy: 0.6714 - precision: 0.6719 - recall: 0.6719 - val_loss: 0.0032 - val_accuracy: 0.6226 - val_precision: 0.6173 - val_recall: 0.6329\n",
      "Epoch 67/110\n",
      "633/633 [==============================] - 5s 8ms/step - loss: 0.0026 - accuracy: 0.6761 - precision: 0.6795 - recall: 0.6688 - val_loss: 0.0032 - val_accuracy: 0.6226 - val_precision: 0.6173 - val_recall: 0.6329\n",
      "Epoch 68/110\n",
      "633/633 [==============================] - 5s 7ms/step - loss: 0.0026 - accuracy: 0.6730 - precision: 0.6763 - recall: 0.6656 - val_loss: 0.0031 - val_accuracy: 0.6226 - val_precision: 0.6173 - val_recall: 0.6329\n",
      "Epoch 69/110\n",
      "633/633 [==============================] - 5s 7ms/step - loss: 0.0026 - accuracy: 0.6619 - precision: 0.6678 - recall: 0.6467 - val_loss: 0.0031 - val_accuracy: 0.6164 - val_precision: 0.6125 - val_recall: 0.6203\n",
      "Epoch 70/110\n",
      "633/633 [==============================] - 5s 7ms/step - loss: 0.0026 - accuracy: 0.6619 - precision: 0.6667 - recall: 0.6498 - val_loss: 0.0031 - val_accuracy: 0.6164 - val_precision: 0.6154 - val_recall: 0.6076\n",
      "Epoch 71/110\n",
      "633/633 [==============================] - 5s 7ms/step - loss: 0.0026 - accuracy: 0.6588 - precision: 0.6624 - recall: 0.6498 - val_loss: 0.0031 - val_accuracy: 0.6164 - val_precision: 0.6154 - val_recall: 0.6076\n",
      "Epoch 72/110\n",
      "633/633 [==============================] - 5s 7ms/step - loss: 0.0026 - accuracy: 0.6619 - precision: 0.6667 - recall: 0.6498 - val_loss: 0.0031 - val_accuracy: 0.6164 - val_precision: 0.6154 - val_recall: 0.6076\n",
      "Epoch 73/110\n",
      "633/633 [==============================] - 5s 7ms/step - loss: 0.0026 - accuracy: 0.6619 - precision: 0.6678 - recall: 0.6467 - val_loss: 0.0031 - val_accuracy: 0.6226 - val_precision: 0.6234 - val_recall: 0.6076\n",
      "Epoch 74/110\n",
      "633/633 [==============================] - 5s 7ms/step - loss: 0.0026 - accuracy: 0.6635 - precision: 0.6677 - recall: 0.6530 - val_loss: 0.0031 - val_accuracy: 0.6164 - val_precision: 0.6154 - val_recall: 0.6076\n",
      "Epoch 75/110\n",
      "633/633 [==============================] - 5s 7ms/step - loss: 0.0026 - accuracy: 0.6635 - precision: 0.6677 - recall: 0.6530 - val_loss: 0.0031 - val_accuracy: 0.6101 - val_precision: 0.6076 - val_recall: 0.6076\n",
      "Epoch 76/110\n",
      "633/633 [==============================] - 5s 7ms/step - loss: 0.0026 - accuracy: 0.6619 - precision: 0.6667 - recall: 0.6498 - val_loss: 0.0030 - val_accuracy: 0.6101 - val_precision: 0.6076 - val_recall: 0.6076\n",
      "Epoch 77/110\n",
      "633/633 [==============================] - 5s 7ms/step - loss: 0.0026 - accuracy: 0.6619 - precision: 0.6667 - recall: 0.6498 - val_loss: 0.0030 - val_accuracy: 0.6101 - val_precision: 0.6076 - val_recall: 0.6076\n",
      "Epoch 78/110\n",
      "633/633 [==============================] - 5s 7ms/step - loss: 0.0026 - accuracy: 0.6588 - precision: 0.6624 - recall: 0.6498 - val_loss: 0.0030 - val_accuracy: 0.6038 - val_precision: 0.6000 - val_recall: 0.6076\n",
      "Epoch 79/110\n",
      "633/633 [==============================] - 5s 7ms/step - loss: 0.0026 - accuracy: 0.6588 - precision: 0.6613 - recall: 0.6530 - val_loss: 0.0030 - val_accuracy: 0.6038 - val_precision: 0.6000 - val_recall: 0.6076\n",
      "Epoch 80/110\n",
      "633/633 [==============================] - 5s 7ms/step - loss: 0.0026 - accuracy: 0.6588 - precision: 0.6613 - recall: 0.6530 - val_loss: 0.0030 - val_accuracy: 0.6038 - val_precision: 0.6000 - val_recall: 0.6076\n",
      "Epoch 81/110\n",
      "633/633 [==============================] - 5s 7ms/step - loss: 0.0026 - accuracy: 0.6572 - precision: 0.6592 - recall: 0.6530 - val_loss: 0.0030 - val_accuracy: 0.6038 - val_precision: 0.6000 - val_recall: 0.6076\n",
      "Epoch 82/110\n",
      "633/633 [==============================] - 5s 7ms/step - loss: 0.0026 - accuracy: 0.6572 - precision: 0.6592 - recall: 0.6530 - val_loss: 0.0030 - val_accuracy: 0.6101 - val_precision: 0.6076 - val_recall: 0.6076\n",
      "Epoch 83/110\n",
      "633/633 [==============================] - 5s 7ms/step - loss: 0.0026 - accuracy: 0.6572 - precision: 0.6592 - recall: 0.6530 - val_loss: 0.0030 - val_accuracy: 0.6101 - val_precision: 0.6076 - val_recall: 0.6076\n",
      "Epoch 84/110\n",
      "633/633 [==============================] - 5s 7ms/step - loss: 0.0026 - accuracy: 0.6572 - precision: 0.6592 - recall: 0.6530 - val_loss: 0.0030 - val_accuracy: 0.6101 - val_precision: 0.6076 - val_recall: 0.6076\n",
      "Epoch 85/110\n",
      "633/633 [==============================] - 5s 7ms/step - loss: 0.0026 - accuracy: 0.6588 - precision: 0.6613 - recall: 0.6530 - val_loss: 0.0030 - val_accuracy: 0.6101 - val_precision: 0.6076 - val_recall: 0.6076\n",
      "Epoch 86/110\n",
      "633/633 [==============================] - 5s 7ms/step - loss: 0.0027 - accuracy: 0.6588 - precision: 0.6613 - recall: 0.6530 - val_loss: 0.0030 - val_accuracy: 0.6101 - val_precision: 0.6076 - val_recall: 0.6076\n",
      "Epoch 87/110\n",
      "633/633 [==============================] - 5s 7ms/step - loss: 0.0027 - accuracy: 0.6588 - precision: 0.6613 - recall: 0.6530 - val_loss: 0.0030 - val_accuracy: 0.6101 - val_precision: 0.6076 - val_recall: 0.6076\n",
      "Epoch 88/110\n",
      "633/633 [==============================] - 5s 7ms/step - loss: 0.0027 - accuracy: 0.6588 - precision: 0.6613 - recall: 0.6530 - val_loss: 0.0030 - val_accuracy: 0.6101 - val_precision: 0.6076 - val_recall: 0.6076\n",
      "Epoch 89/110\n",
      "633/633 [==============================] - 5s 7ms/step - loss: 0.0027 - accuracy: 0.6588 - precision: 0.6603 - recall: 0.6562 - val_loss: 0.0030 - val_accuracy: 0.6101 - val_precision: 0.6076 - val_recall: 0.6076\n",
      "Epoch 90/110\n",
      "633/633 [==============================] - 5s 7ms/step - loss: 0.0027 - accuracy: 0.6588 - precision: 0.6603 - recall: 0.6562 - val_loss: 0.0030 - val_accuracy: 0.6101 - val_precision: 0.6076 - val_recall: 0.6076\n",
      "Epoch 91/110\n",
      "633/633 [==============================] - 5s 7ms/step - loss: 0.0027 - accuracy: 0.6588 - precision: 0.6603 - recall: 0.6562 - val_loss: 0.0030 - val_accuracy: 0.6101 - val_precision: 0.6076 - val_recall: 0.6076\n",
      "Epoch 92/110\n",
      "633/633 [==============================] - 5s 7ms/step - loss: 0.0027 - accuracy: 0.6588 - precision: 0.6603 - recall: 0.6562 - val_loss: 0.0030 - val_accuracy: 0.6101 - val_precision: 0.6076 - val_recall: 0.6076\n",
      "Epoch 93/110\n",
      "633/633 [==============================] - 5s 7ms/step - loss: 0.0027 - accuracy: 0.6588 - precision: 0.6603 - recall: 0.6562 - val_loss: 0.0030 - val_accuracy: 0.6101 - val_precision: 0.6076 - val_recall: 0.6076\n",
      "Epoch 94/110\n",
      "633/633 [==============================] - 5s 7ms/step - loss: 0.0027 - accuracy: 0.6588 - precision: 0.6603 - recall: 0.6562 - val_loss: 0.0030 - val_accuracy: 0.6101 - val_precision: 0.6076 - val_recall: 0.6076\n",
      "Epoch 95/110\n",
      "633/633 [==============================] - 5s 7ms/step - loss: 0.0027 - accuracy: 0.6588 - precision: 0.6603 - recall: 0.6562 - val_loss: 0.0030 - val_accuracy: 0.6101 - val_precision: 0.6076 - val_recall: 0.6076\n",
      "Epoch 96/110\n",
      "633/633 [==============================] - 5s 7ms/step - loss: 0.0027 - accuracy: 0.6588 - precision: 0.6603 - recall: 0.6562 - val_loss: 0.0030 - val_accuracy: 0.6101 - val_precision: 0.6076 - val_recall: 0.6076\n",
      "Epoch 97/110\n",
      "633/633 [==============================] - 5s 7ms/step - loss: 0.0027 - accuracy: 0.6588 - precision: 0.6603 - recall: 0.6562 - val_loss: 0.0030 - val_accuracy: 0.6101 - val_precision: 0.6076 - val_recall: 0.6076\n",
      "Epoch 98/110\n",
      "633/633 [==============================] - 5s 7ms/step - loss: 0.0027 - accuracy: 0.6588 - precision: 0.6603 - recall: 0.6562 - val_loss: 0.0030 - val_accuracy: 0.6101 - val_precision: 0.6076 - val_recall: 0.6076\n",
      "Epoch 99/110\n",
      "633/633 [==============================] - 5s 7ms/step - loss: 0.0027 - accuracy: 0.6572 - precision: 0.6572 - recall: 0.6593 - val_loss: 0.0030 - val_accuracy: 0.6101 - val_precision: 0.6076 - val_recall: 0.6076\n",
      "Epoch 100/110\n",
      "633/633 [==============================] - 5s 7ms/step - loss: 0.0027 - accuracy: 0.6572 - precision: 0.6572 - recall: 0.6593 - val_loss: 0.0030 - val_accuracy: 0.6101 - val_precision: 0.6076 - val_recall: 0.6076\n",
      "Epoch 101/110\n",
      "633/633 [==============================] - 5s 7ms/step - loss: 0.0027 - accuracy: 0.6572 - precision: 0.6572 - recall: 0.6593 - val_loss: 0.0030 - val_accuracy: 0.6101 - val_precision: 0.6076 - val_recall: 0.6076\n",
      "Epoch 102/110\n",
      "633/633 [==============================] - 5s 7ms/step - loss: 0.0027 - accuracy: 0.6556 - precision: 0.6552 - recall: 0.6593 - val_loss: 0.0030 - val_accuracy: 0.6101 - val_precision: 0.6076 - val_recall: 0.6076\n",
      "Epoch 103/110\n",
      "633/633 [==============================] - 5s 8ms/step - loss: 0.0027 - accuracy: 0.6556 - precision: 0.6552 - recall: 0.6593 - val_loss: 0.0030 - val_accuracy: 0.6101 - val_precision: 0.6076 - val_recall: 0.6076\n",
      "Epoch 104/110\n",
      "633/633 [==============================] - 5s 8ms/step - loss: 0.0027 - accuracy: 0.6556 - precision: 0.6552 - recall: 0.6593 - val_loss: 0.0030 - val_accuracy: 0.6101 - val_precision: 0.6076 - val_recall: 0.6076\n",
      "Epoch 105/110\n",
      "633/633 [==============================] - 5s 8ms/step - loss: 0.0027 - accuracy: 0.6556 - precision: 0.6552 - recall: 0.6593 - val_loss: 0.0030 - val_accuracy: 0.6101 - val_precision: 0.6076 - val_recall: 0.6076\n",
      "Epoch 106/110\n",
      "633/633 [==============================] - 5s 8ms/step - loss: 0.0027 - accuracy: 0.6556 - precision: 0.6552 - recall: 0.6593 - val_loss: 0.0030 - val_accuracy: 0.6101 - val_precision: 0.6076 - val_recall: 0.6076\n",
      "Epoch 107/110\n",
      "633/633 [==============================] - 5s 8ms/step - loss: 0.0027 - accuracy: 0.6556 - precision: 0.6552 - recall: 0.6593 - val_loss: 0.0030 - val_accuracy: 0.6101 - val_precision: 0.6076 - val_recall: 0.6076\n",
      "Epoch 108/110\n",
      "633/633 [==============================] - 5s 8ms/step - loss: 0.0026 - accuracy: 0.6556 - precision: 0.6552 - recall: 0.6593 - val_loss: 0.0030 - val_accuracy: 0.6101 - val_precision: 0.6076 - val_recall: 0.6076\n",
      "Epoch 109/110\n",
      "633/633 [==============================] - 5s 8ms/step - loss: 0.0026 - accuracy: 0.6556 - precision: 0.6552 - recall: 0.6593 - val_loss: 0.0030 - val_accuracy: 0.6101 - val_precision: 0.6076 - val_recall: 0.6076\n",
      "Epoch 110/110\n",
      "633/633 [==============================] - 5s 8ms/step - loss: 0.0026 - accuracy: 0.6556 - precision: 0.6552 - recall: 0.6593 - val_loss: 0.0030 - val_accuracy: 0.6101 - val_precision: 0.6076 - val_recall: 0.6076\n",
      "5/5 [==============================] - 1s 27ms/step\n",
      "\n",
      "🎯 Finished schedule 'Multistage' with test loss: 0.0030, accuracy: 0.6101, precision: 0.6076, recall: 0.6076, f1-score: 0.6076\n",
      "🔍 Computing SHAP values...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/boabangfrancis/anaconda3/lib/python3.11/site-packages/keras/src/backend.py:452: UserWarning: `tf.keras.backend.set_learning_phase` is deprecated and will be removed after 2020-10-11. To update it, simply pass a True/False value to the `training` argument of the `__call__` method of your layer or model.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ SHAP values computed.\n",
      "\n",
      "🚀 Training schedule: Nonconvex Only\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "➡️ Stage 1 | Epochs: 100 | Loss: nonconvex_focal_loss\n",
      "Epoch 1/100\n",
      "633/633 [==============================] - 7s 9ms/step - loss: 0.0076 - accuracy: 0.5166 - precision: 0.5137 - recall: 0.6498 - val_loss: 0.0046 - val_accuracy: 0.5660 - val_precision: 0.5595 - val_recall: 0.5949\n",
      "Epoch 2/100\n",
      "633/633 [==============================] - 5s 8ms/step - loss: 0.0051 - accuracy: 0.5166 - precision: 0.5196 - recall: 0.4606 - val_loss: 0.0040 - val_accuracy: 0.5912 - val_precision: 0.6029 - val_recall: 0.5190\n",
      "Epoch 3/100\n",
      "633/633 [==============================] - 5s 7ms/step - loss: 0.0047 - accuracy: 0.4834 - precision: 0.4823 - recall: 0.4290 - val_loss: 0.0045 - val_accuracy: 0.5157 - val_precision: 0.5172 - val_recall: 0.3797\n",
      "Epoch 4/100\n",
      "633/633 [==============================] - 5s 7ms/step - loss: 0.0043 - accuracy: 0.5103 - precision: 0.5125 - recall: 0.4511 - val_loss: 0.0042 - val_accuracy: 0.5283 - val_precision: 0.5278 - val_recall: 0.4810\n",
      "Epoch 5/100\n",
      "633/633 [==============================] - 5s 7ms/step - loss: 0.0040 - accuracy: 0.5340 - precision: 0.5369 - recall: 0.5047 - val_loss: 0.0040 - val_accuracy: 0.5660 - val_precision: 0.5735 - val_recall: 0.4937\n",
      "Epoch 6/100\n",
      "633/633 [==============================] - 5s 7ms/step - loss: 0.0038 - accuracy: 0.5592 - precision: 0.5638 - recall: 0.5300 - val_loss: 0.0033 - val_accuracy: 0.5912 - val_precision: 0.5946 - val_recall: 0.5570\n",
      "Epoch 7/100\n",
      "633/633 [==============================] - 5s 7ms/step - loss: 0.0035 - accuracy: 0.5845 - precision: 0.5957 - recall: 0.5300 - val_loss: 0.0032 - val_accuracy: 0.6289 - val_precision: 0.6389 - val_recall: 0.5823\n",
      "Epoch 8/100\n",
      "633/633 [==============================] - 5s 7ms/step - loss: 0.0036 - accuracy: 0.6161 - precision: 0.6194 - recall: 0.6057 - val_loss: 0.0031 - val_accuracy: 0.5975 - val_precision: 0.5882 - val_recall: 0.6329\n",
      "Epoch 9/100\n",
      "633/633 [==============================] - 5s 7ms/step - loss: 0.0034 - accuracy: 0.6082 - precision: 0.6088 - recall: 0.6088 - val_loss: 0.0032 - val_accuracy: 0.5786 - val_precision: 0.5769 - val_recall: 0.5696\n",
      "Epoch 10/100\n",
      "633/633 [==============================] - 5s 7ms/step - loss: 0.0035 - accuracy: 0.5908 - precision: 0.5942 - recall: 0.5773 - val_loss: 0.0035 - val_accuracy: 0.6038 - val_precision: 0.6081 - val_recall: 0.5696\n",
      "Epoch 11/100\n",
      "633/633 [==============================] - 5s 8ms/step - loss: 0.0035 - accuracy: 0.6114 - precision: 0.6212 - recall: 0.5741 - val_loss: 0.0033 - val_accuracy: 0.5597 - val_precision: 0.5529 - val_recall: 0.5949\n",
      "Epoch 12/100\n",
      "633/633 [==============================] - 5s 8ms/step - loss: 0.0035 - accuracy: 0.6066 - precision: 0.6062 - recall: 0.6120 - val_loss: 0.0030 - val_accuracy: 0.5975 - val_precision: 0.6027 - val_recall: 0.5570\n",
      "Epoch 13/100\n",
      "633/633 [==============================] - 5s 8ms/step - loss: 0.0034 - accuracy: 0.6130 - precision: 0.6184 - recall: 0.5931 - val_loss: 0.0030 - val_accuracy: 0.6164 - val_precision: 0.6184 - val_recall: 0.5949\n",
      "Epoch 14/100\n",
      "633/633 [==============================] - 5s 7ms/step - loss: 0.0033 - accuracy: 0.5908 - precision: 0.5948 - recall: 0.5741 - val_loss: 0.0031 - val_accuracy: 0.6164 - val_precision: 0.6286 - val_recall: 0.5570\n",
      "Epoch 15/100\n",
      "633/633 [==============================] - 5s 8ms/step - loss: 0.0033 - accuracy: 0.5798 - precision: 0.5876 - recall: 0.5394 - val_loss: 0.0031 - val_accuracy: 0.6101 - val_precision: 0.6269 - val_recall: 0.5316\n",
      "Epoch 16/100\n",
      "633/633 [==============================] - 5s 7ms/step - loss: 0.0032 - accuracy: 0.6019 - precision: 0.6117 - recall: 0.5615 - val_loss: 0.0031 - val_accuracy: 0.6415 - val_precision: 0.6667 - val_recall: 0.5570\n",
      "Epoch 17/100\n",
      "633/633 [==============================] - 5s 8ms/step - loss: 0.0031 - accuracy: 0.6082 - precision: 0.6186 - recall: 0.5678 - val_loss: 0.0031 - val_accuracy: 0.6164 - val_precision: 0.6250 - val_recall: 0.5696\n",
      "Epoch 18/100\n",
      "633/633 [==============================] - 5s 7ms/step - loss: 0.0030 - accuracy: 0.6098 - precision: 0.6182 - recall: 0.5773 - val_loss: 0.0031 - val_accuracy: 0.6415 - val_precision: 0.6571 - val_recall: 0.5823\n",
      "Epoch 19/100\n",
      "633/633 [==============================] - 5s 7ms/step - loss: 0.0030 - accuracy: 0.6066 - precision: 0.6181 - recall: 0.5615 - val_loss: 0.0031 - val_accuracy: 0.6164 - val_precision: 0.6250 - val_recall: 0.5696\n",
      "Epoch 20/100\n",
      "633/633 [==============================] - 5s 7ms/step - loss: 0.0030 - accuracy: 0.6161 - precision: 0.6259 - recall: 0.5804 - val_loss: 0.0030 - val_accuracy: 0.6226 - val_precision: 0.6377 - val_recall: 0.5570\n",
      "Epoch 21/100\n",
      "633/633 [==============================] - 5s 7ms/step - loss: 0.0030 - accuracy: 0.6145 - precision: 0.6237 - recall: 0.5804 - val_loss: 0.0034 - val_accuracy: 0.6164 - val_precision: 0.6286 - val_recall: 0.5570\n",
      "Epoch 22/100\n",
      "633/633 [==============================] - 5s 7ms/step - loss: 0.0030 - accuracy: 0.6114 - precision: 0.6195 - recall: 0.5804 - val_loss: 0.0033 - val_accuracy: 0.6164 - val_precision: 0.6324 - val_recall: 0.5443\n",
      "Epoch 23/100\n",
      "633/633 [==============================] - 5s 7ms/step - loss: 0.0030 - accuracy: 0.6161 - precision: 0.6285 - recall: 0.5710 - val_loss: 0.0033 - val_accuracy: 0.6164 - val_precision: 0.6324 - val_recall: 0.5443\n",
      "Epoch 24/100\n",
      "633/633 [==============================] - 5s 7ms/step - loss: 0.0030 - accuracy: 0.6130 - precision: 0.6250 - recall: 0.5678 - val_loss: 0.0034 - val_accuracy: 0.6164 - val_precision: 0.6324 - val_recall: 0.5443\n",
      "Epoch 25/100\n",
      "633/633 [==============================] - 5s 7ms/step - loss: 0.0030 - accuracy: 0.6019 - precision: 0.6157 - recall: 0.5457 - val_loss: 0.0034 - val_accuracy: 0.5975 - val_precision: 0.6119 - val_recall: 0.5190\n",
      "Epoch 26/100\n",
      "633/633 [==============================] - 5s 7ms/step - loss: 0.0031 - accuracy: 0.5987 - precision: 0.6137 - recall: 0.5363 - val_loss: 0.0037 - val_accuracy: 0.5660 - val_precision: 0.5758 - val_recall: 0.4810\n",
      "Epoch 27/100\n",
      "633/633 [==============================] - 5s 7ms/step - loss: 0.0032 - accuracy: 0.6003 - precision: 0.6143 - recall: 0.5426 - val_loss: 0.0036 - val_accuracy: 0.5535 - val_precision: 0.5606 - val_recall: 0.4684\n",
      "Epoch 28/100\n",
      "633/633 [==============================] - 5s 7ms/step - loss: 0.0032 - accuracy: 0.6035 - precision: 0.6196 - recall: 0.5394 - val_loss: 0.0037 - val_accuracy: 0.5535 - val_precision: 0.5606 - val_recall: 0.4684\n",
      "Epoch 29/100\n",
      "633/633 [==============================] - 5s 7ms/step - loss: 0.0032 - accuracy: 0.6130 - precision: 0.6250 - recall: 0.5678 - val_loss: 0.0037 - val_accuracy: 0.5660 - val_precision: 0.5758 - val_recall: 0.4810\n",
      "Epoch 30/100\n",
      "633/633 [==============================] - 5s 7ms/step - loss: 0.0031 - accuracy: 0.6130 - precision: 0.6259 - recall: 0.5647 - val_loss: 0.0036 - val_accuracy: 0.5660 - val_precision: 0.5758 - val_recall: 0.4810\n",
      "Epoch 31/100\n",
      "633/633 [==============================] - 5s 7ms/step - loss: 0.0031 - accuracy: 0.6082 - precision: 0.6194 - recall: 0.5647 - val_loss: 0.0035 - val_accuracy: 0.5660 - val_precision: 0.5735 - val_recall: 0.4937\n",
      "Epoch 32/100\n",
      "633/633 [==============================] - 5s 7ms/step - loss: 0.0031 - accuracy: 0.6066 - precision: 0.6181 - recall: 0.5615 - val_loss: 0.0035 - val_accuracy: 0.5723 - val_precision: 0.5775 - val_recall: 0.5190\n",
      "Epoch 33/100\n",
      "633/633 [==============================] - 5s 7ms/step - loss: 0.0031 - accuracy: 0.6082 - precision: 0.6186 - recall: 0.5678 - val_loss: 0.0035 - val_accuracy: 0.5660 - val_precision: 0.5694 - val_recall: 0.5190\n",
      "Epoch 34/100\n",
      "633/633 [==============================] - 5s 7ms/step - loss: 0.0031 - accuracy: 0.6130 - precision: 0.6241 - recall: 0.5710 - val_loss: 0.0034 - val_accuracy: 0.5660 - val_precision: 0.5694 - val_recall: 0.5190\n",
      "Epoch 35/100\n",
      "633/633 [==============================] - 5s 7ms/step - loss: 0.0030 - accuracy: 0.6066 - precision: 0.6156 - recall: 0.5710 - val_loss: 0.0034 - val_accuracy: 0.5849 - val_precision: 0.5915 - val_recall: 0.5316\n",
      "Epoch 36/100\n",
      "633/633 [==============================] - 5s 7ms/step - loss: 0.0031 - accuracy: 0.6130 - precision: 0.6241 - recall: 0.5710 - val_loss: 0.0034 - val_accuracy: 0.5786 - val_precision: 0.5833 - val_recall: 0.5316\n",
      "Epoch 37/100\n",
      "633/633 [==============================] - 5s 7ms/step - loss: 0.0030 - accuracy: 0.6209 - precision: 0.6296 - recall: 0.5899 - val_loss: 0.0034 - val_accuracy: 0.5912 - val_precision: 0.5972 - val_recall: 0.5443\n",
      "Epoch 38/100\n",
      "633/633 [==============================] - 5s 7ms/step - loss: 0.0030 - accuracy: 0.6193 - precision: 0.6293 - recall: 0.5836 - val_loss: 0.0034 - val_accuracy: 0.5975 - val_precision: 0.6056 - val_recall: 0.5443\n",
      "Epoch 39/100\n",
      "633/633 [==============================] - 5s 7ms/step - loss: 0.0030 - accuracy: 0.6272 - precision: 0.6355 - recall: 0.5994 - val_loss: 0.0031 - val_accuracy: 0.5975 - val_precision: 0.6087 - val_recall: 0.5316\n",
      "Epoch 40/100\n",
      "633/633 [==============================] - 5s 7ms/step - loss: 0.0030 - accuracy: 0.6240 - precision: 0.6348 - recall: 0.5868 - val_loss: 0.0031 - val_accuracy: 0.5975 - val_precision: 0.6087 - val_recall: 0.5316\n",
      "Epoch 41/100\n",
      "633/633 [==============================] - 5s 8ms/step - loss: 0.0029 - accuracy: 0.6224 - precision: 0.6327 - recall: 0.5868 - val_loss: 0.0031 - val_accuracy: 0.5975 - val_precision: 0.6056 - val_recall: 0.5443\n",
      "Epoch 42/100\n",
      "633/633 [==============================] - 5s 7ms/step - loss: 0.0029 - accuracy: 0.6256 - precision: 0.6342 - recall: 0.5962 - val_loss: 0.0031 - val_accuracy: 0.6038 - val_precision: 0.6111 - val_recall: 0.5570\n",
      "Epoch 43/100\n",
      "633/633 [==============================] - 5s 7ms/step - loss: 0.0029 - accuracy: 0.6209 - precision: 0.6279 - recall: 0.5962 - val_loss: 0.0031 - val_accuracy: 0.6164 - val_precision: 0.6250 - val_recall: 0.5696\n",
      "Epoch 44/100\n",
      "633/633 [==============================] - 5s 7ms/step - loss: 0.0029 - accuracy: 0.6224 - precision: 0.6275 - recall: 0.6057 - val_loss: 0.0031 - val_accuracy: 0.6101 - val_precision: 0.6164 - val_recall: 0.5696\n",
      "Epoch 45/100\n",
      "633/633 [==============================] - 5s 7ms/step - loss: 0.0029 - accuracy: 0.6193 - precision: 0.6250 - recall: 0.5994 - val_loss: 0.0031 - val_accuracy: 0.6226 - val_precision: 0.6301 - val_recall: 0.5823\n",
      "Epoch 46/100\n",
      "633/633 [==============================] - 5s 7ms/step - loss: 0.0029 - accuracy: 0.6272 - precision: 0.6337 - recall: 0.6057 - val_loss: 0.0030 - val_accuracy: 0.6226 - val_precision: 0.6301 - val_recall: 0.5823\n",
      "Epoch 47/100\n",
      "633/633 [==============================] - 5s 7ms/step - loss: 0.0029 - accuracy: 0.6224 - precision: 0.6275 - recall: 0.6057 - val_loss: 0.0030 - val_accuracy: 0.6164 - val_precision: 0.6216 - val_recall: 0.5823\n",
      "Epoch 48/100\n",
      "633/633 [==============================] - 5s 8ms/step - loss: 0.0029 - accuracy: 0.6240 - precision: 0.6287 - recall: 0.6088 - val_loss: 0.0032 - val_accuracy: 0.6101 - val_precision: 0.6133 - val_recall: 0.5823\n",
      "Epoch 49/100\n",
      "633/633 [==============================] - 5s 7ms/step - loss: 0.0029 - accuracy: 0.6209 - precision: 0.6207 - recall: 0.6246 - val_loss: 0.0031 - val_accuracy: 0.5975 - val_precision: 0.5949 - val_recall: 0.5949\n",
      "Epoch 50/100\n",
      "633/633 [==============================] - 5s 7ms/step - loss: 0.0029 - accuracy: 0.6193 - precision: 0.6195 - recall: 0.6215 - val_loss: 0.0031 - val_accuracy: 0.5912 - val_precision: 0.5897 - val_recall: 0.5823\n",
      "Epoch 51/100\n",
      "633/633 [==============================] - 5s 7ms/step - loss: 0.0029 - accuracy: 0.6272 - precision: 0.6262 - recall: 0.6341 - val_loss: 0.0031 - val_accuracy: 0.5849 - val_precision: 0.5823 - val_recall: 0.5823\n",
      "Epoch 52/100\n",
      "633/633 [==============================] - 5s 7ms/step - loss: 0.0029 - accuracy: 0.6303 - precision: 0.6285 - recall: 0.6404 - val_loss: 0.0031 - val_accuracy: 0.5849 - val_precision: 0.5823 - val_recall: 0.5823\n",
      "Epoch 53/100\n",
      "633/633 [==============================] - 5s 7ms/step - loss: 0.0029 - accuracy: 0.6335 - precision: 0.6324 - recall: 0.6404 - val_loss: 0.0031 - val_accuracy: 0.5849 - val_precision: 0.5844 - val_recall: 0.5696\n",
      "Epoch 54/100\n",
      "633/633 [==============================] - 5s 7ms/step - loss: 0.0029 - accuracy: 0.6335 - precision: 0.6332 - recall: 0.6372 - val_loss: 0.0031 - val_accuracy: 0.5849 - val_precision: 0.5867 - val_recall: 0.5570\n",
      "Epoch 55/100\n",
      "633/633 [==============================] - 5s 7ms/step - loss: 0.0029 - accuracy: 0.6335 - precision: 0.6349 - recall: 0.6309 - val_loss: 0.0031 - val_accuracy: 0.5912 - val_precision: 0.5897 - val_recall: 0.5823\n",
      "Epoch 56/100\n",
      "633/633 [==============================] - 5s 7ms/step - loss: 0.0028 - accuracy: 0.6382 - precision: 0.6384 - recall: 0.6404 - val_loss: 0.0031 - val_accuracy: 0.5849 - val_precision: 0.5844 - val_recall: 0.5696\n",
      "Epoch 57/100\n",
      "633/633 [==============================] - 5s 7ms/step - loss: 0.0028 - accuracy: 0.6367 - precision: 0.6381 - recall: 0.6341 - val_loss: 0.0031 - val_accuracy: 0.5975 - val_precision: 0.6000 - val_recall: 0.5696\n",
      "Epoch 58/100\n",
      "633/633 [==============================] - 5s 7ms/step - loss: 0.0028 - accuracy: 0.6445 - precision: 0.6447 - recall: 0.6467 - val_loss: 0.0031 - val_accuracy: 0.5975 - val_precision: 0.6000 - val_recall: 0.5696\n",
      "Epoch 59/100\n",
      "633/633 [==============================] - 5s 7ms/step - loss: 0.0028 - accuracy: 0.6493 - precision: 0.6489 - recall: 0.6530 - val_loss: 0.0031 - val_accuracy: 0.5975 - val_precision: 0.6000 - val_recall: 0.5696\n",
      "Epoch 60/100\n",
      "633/633 [==============================] - 5s 7ms/step - loss: 0.0028 - accuracy: 0.6493 - precision: 0.6489 - recall: 0.6530 - val_loss: 0.0031 - val_accuracy: 0.5975 - val_precision: 0.6000 - val_recall: 0.5696\n",
      "Epoch 61/100\n",
      "633/633 [==============================] - 5s 7ms/step - loss: 0.0028 - accuracy: 0.6493 - precision: 0.6489 - recall: 0.6530 - val_loss: 0.0031 - val_accuracy: 0.5975 - val_precision: 0.6000 - val_recall: 0.5696\n",
      "Epoch 62/100\n",
      "633/633 [==============================] - 5s 7ms/step - loss: 0.0028 - accuracy: 0.6493 - precision: 0.6489 - recall: 0.6530 - val_loss: 0.0031 - val_accuracy: 0.5975 - val_precision: 0.6000 - val_recall: 0.5696\n",
      "Epoch 63/100\n",
      "633/633 [==============================] - 5s 7ms/step - loss: 0.0028 - accuracy: 0.6477 - precision: 0.6487 - recall: 0.6467 - val_loss: 0.0031 - val_accuracy: 0.5975 - val_precision: 0.6000 - val_recall: 0.5696\n",
      "Epoch 64/100\n",
      "633/633 [==============================] - 5s 7ms/step - loss: 0.0028 - accuracy: 0.6477 - precision: 0.6487 - recall: 0.6467 - val_loss: 0.0031 - val_accuracy: 0.5975 - val_precision: 0.6000 - val_recall: 0.5696\n",
      "Epoch 65/100\n",
      "633/633 [==============================] - 5s 7ms/step - loss: 0.0028 - accuracy: 0.6477 - precision: 0.6487 - recall: 0.6467 - val_loss: 0.0030 - val_accuracy: 0.6038 - val_precision: 0.6081 - val_recall: 0.5696\n",
      "Epoch 66/100\n",
      "633/633 [==============================] - 5s 7ms/step - loss: 0.0028 - accuracy: 0.6461 - precision: 0.6476 - recall: 0.6435 - val_loss: 0.0030 - val_accuracy: 0.6038 - val_precision: 0.6081 - val_recall: 0.5696\n",
      "Epoch 67/100\n",
      "633/633 [==============================] - 5s 7ms/step - loss: 0.0028 - accuracy: 0.6477 - precision: 0.6497 - recall: 0.6435 - val_loss: 0.0030 - val_accuracy: 0.6038 - val_precision: 0.6081 - val_recall: 0.5696\n",
      "Epoch 68/100\n",
      "633/633 [==============================] - 5s 7ms/step - loss: 0.0028 - accuracy: 0.6477 - precision: 0.6497 - recall: 0.6435 - val_loss: 0.0030 - val_accuracy: 0.6038 - val_precision: 0.6081 - val_recall: 0.5696\n",
      "Epoch 69/100\n",
      "633/633 [==============================] - 5s 7ms/step - loss: 0.0028 - accuracy: 0.6477 - precision: 0.6497 - recall: 0.6435 - val_loss: 0.0030 - val_accuracy: 0.6038 - val_precision: 0.6081 - val_recall: 0.5696\n",
      "Epoch 70/100\n",
      "633/633 [==============================] - 5s 7ms/step - loss: 0.0028 - accuracy: 0.6477 - precision: 0.6497 - recall: 0.6435 - val_loss: 0.0030 - val_accuracy: 0.6038 - val_precision: 0.6081 - val_recall: 0.5696\n",
      "Epoch 71/100\n",
      "633/633 [==============================] - 5s 7ms/step - loss: 0.0028 - accuracy: 0.6477 - precision: 0.6497 - recall: 0.6435 - val_loss: 0.0030 - val_accuracy: 0.6038 - val_precision: 0.6081 - val_recall: 0.5696\n",
      "Epoch 72/100\n",
      "633/633 [==============================] - 5s 7ms/step - loss: 0.0028 - accuracy: 0.6477 - precision: 0.6497 - recall: 0.6435 - val_loss: 0.0030 - val_accuracy: 0.6038 - val_precision: 0.6081 - val_recall: 0.5696\n",
      "Epoch 73/100\n",
      "633/633 [==============================] - 5s 7ms/step - loss: 0.0028 - accuracy: 0.6477 - precision: 0.6497 - recall: 0.6435 - val_loss: 0.0030 - val_accuracy: 0.6038 - val_precision: 0.6081 - val_recall: 0.5696\n",
      "Epoch 74/100\n",
      "633/633 [==============================] - 5s 7ms/step - loss: 0.0028 - accuracy: 0.6477 - precision: 0.6497 - recall: 0.6435 - val_loss: 0.0030 - val_accuracy: 0.6038 - val_precision: 0.6081 - val_recall: 0.5696\n",
      "Epoch 75/100\n",
      "633/633 [==============================] - 5s 7ms/step - loss: 0.0028 - accuracy: 0.6477 - precision: 0.6497 - recall: 0.6435 - val_loss: 0.0030 - val_accuracy: 0.6038 - val_precision: 0.6081 - val_recall: 0.5696\n",
      "Epoch 76/100\n",
      "633/633 [==============================] - 5s 7ms/step - loss: 0.0028 - accuracy: 0.6477 - precision: 0.6497 - recall: 0.6435 - val_loss: 0.0030 - val_accuracy: 0.6038 - val_precision: 0.6081 - val_recall: 0.5696\n",
      "Epoch 77/100\n",
      "633/633 [==============================] - 5s 7ms/step - loss: 0.0028 - accuracy: 0.6477 - precision: 0.6497 - recall: 0.6435 - val_loss: 0.0030 - val_accuracy: 0.6038 - val_precision: 0.6081 - val_recall: 0.5696\n",
      "Epoch 78/100\n",
      "633/633 [==============================] - 5s 7ms/step - loss: 0.0028 - accuracy: 0.6461 - precision: 0.6486 - recall: 0.6404 - val_loss: 0.0030 - val_accuracy: 0.6038 - val_precision: 0.6081 - val_recall: 0.5696\n",
      "Epoch 79/100\n",
      "633/633 [==============================] - 5s 7ms/step - loss: 0.0028 - accuracy: 0.6445 - precision: 0.6465 - recall: 0.6404 - val_loss: 0.0030 - val_accuracy: 0.6038 - val_precision: 0.6081 - val_recall: 0.5696\n",
      "Epoch 80/100\n",
      "633/633 [==============================] - 5s 7ms/step - loss: 0.0028 - accuracy: 0.6445 - precision: 0.6465 - recall: 0.6404 - val_loss: 0.0030 - val_accuracy: 0.6038 - val_precision: 0.6081 - val_recall: 0.5696\n",
      "Epoch 81/100\n",
      "633/633 [==============================] - 5s 7ms/step - loss: 0.0028 - accuracy: 0.6445 - precision: 0.6465 - recall: 0.6404 - val_loss: 0.0030 - val_accuracy: 0.6038 - val_precision: 0.6081 - val_recall: 0.5696\n",
      "Epoch 82/100\n",
      "633/633 [==============================] - 5s 7ms/step - loss: 0.0028 - accuracy: 0.6445 - precision: 0.6465 - recall: 0.6404 - val_loss: 0.0030 - val_accuracy: 0.6038 - val_precision: 0.6081 - val_recall: 0.5696\n",
      "Epoch 83/100\n",
      "633/633 [==============================] - 5s 7ms/step - loss: 0.0028 - accuracy: 0.6445 - precision: 0.6465 - recall: 0.6404 - val_loss: 0.0030 - val_accuracy: 0.6038 - val_precision: 0.6081 - val_recall: 0.5696\n",
      "Epoch 84/100\n",
      "633/633 [==============================] - 5s 7ms/step - loss: 0.0028 - accuracy: 0.6445 - precision: 0.6465 - recall: 0.6404 - val_loss: 0.0030 - val_accuracy: 0.6038 - val_precision: 0.6081 - val_recall: 0.5696\n",
      "Epoch 85/100\n",
      "633/633 [==============================] - 5s 7ms/step - loss: 0.0028 - accuracy: 0.6461 - precision: 0.6486 - recall: 0.6404 - val_loss: 0.0030 - val_accuracy: 0.6038 - val_precision: 0.6081 - val_recall: 0.5696\n",
      "Epoch 86/100\n",
      "633/633 [==============================] - 5s 7ms/step - loss: 0.0028 - accuracy: 0.6461 - precision: 0.6486 - recall: 0.6404 - val_loss: 0.0030 - val_accuracy: 0.6038 - val_precision: 0.6081 - val_recall: 0.5696\n",
      "Epoch 87/100\n",
      "633/633 [==============================] - 5s 7ms/step - loss: 0.0028 - accuracy: 0.6461 - precision: 0.6486 - recall: 0.6404 - val_loss: 0.0030 - val_accuracy: 0.6038 - val_precision: 0.6081 - val_recall: 0.5696\n",
      "Epoch 88/100\n",
      "633/633 [==============================] - 5s 7ms/step - loss: 0.0028 - accuracy: 0.6461 - precision: 0.6486 - recall: 0.6404 - val_loss: 0.0030 - val_accuracy: 0.6038 - val_precision: 0.6081 - val_recall: 0.5696\n",
      "Epoch 89/100\n",
      "633/633 [==============================] - 5s 7ms/step - loss: 0.0028 - accuracy: 0.6461 - precision: 0.6486 - recall: 0.6404 - val_loss: 0.0030 - val_accuracy: 0.6038 - val_precision: 0.6081 - val_recall: 0.5696\n",
      "Epoch 90/100\n",
      "633/633 [==============================] - 5s 7ms/step - loss: 0.0028 - accuracy: 0.6461 - precision: 0.6486 - recall: 0.6404 - val_loss: 0.0030 - val_accuracy: 0.6038 - val_precision: 0.6081 - val_recall: 0.5696\n",
      "Epoch 91/100\n",
      "633/633 [==============================] - 5s 7ms/step - loss: 0.0028 - accuracy: 0.6461 - precision: 0.6486 - recall: 0.6404 - val_loss: 0.0030 - val_accuracy: 0.6038 - val_precision: 0.6081 - val_recall: 0.5696\n",
      "Epoch 92/100\n",
      "633/633 [==============================] - 5s 7ms/step - loss: 0.0028 - accuracy: 0.6461 - precision: 0.6486 - recall: 0.6404 - val_loss: 0.0030 - val_accuracy: 0.6038 - val_precision: 0.6081 - val_recall: 0.5696\n",
      "Epoch 93/100\n",
      "633/633 [==============================] - 5s 8ms/step - loss: 0.0028 - accuracy: 0.6461 - precision: 0.6486 - recall: 0.6404 - val_loss: 0.0030 - val_accuracy: 0.6038 - val_precision: 0.6081 - val_recall: 0.5696\n",
      "Epoch 94/100\n",
      "633/633 [==============================] - 5s 7ms/step - loss: 0.0028 - accuracy: 0.6461 - precision: 0.6486 - recall: 0.6404 - val_loss: 0.0030 - val_accuracy: 0.6038 - val_precision: 0.6081 - val_recall: 0.5696\n",
      "Epoch 95/100\n",
      "633/633 [==============================] - 5s 7ms/step - loss: 0.0028 - accuracy: 0.6461 - precision: 0.6486 - recall: 0.6404 - val_loss: 0.0030 - val_accuracy: 0.6038 - val_precision: 0.6081 - val_recall: 0.5696\n",
      "Epoch 96/100\n",
      "633/633 [==============================] - 5s 7ms/step - loss: 0.0028 - accuracy: 0.6461 - precision: 0.6486 - recall: 0.6404 - val_loss: 0.0030 - val_accuracy: 0.6038 - val_precision: 0.6081 - val_recall: 0.5696\n",
      "Epoch 97/100\n",
      "633/633 [==============================] - 5s 7ms/step - loss: 0.0028 - accuracy: 0.6461 - precision: 0.6486 - recall: 0.6404 - val_loss: 0.0030 - val_accuracy: 0.6038 - val_precision: 0.6081 - val_recall: 0.5696\n",
      "Epoch 98/100\n",
      "633/633 [==============================] - 5s 7ms/step - loss: 0.0028 - accuracy: 0.6445 - precision: 0.6474 - recall: 0.6372 - val_loss: 0.0030 - val_accuracy: 0.6038 - val_precision: 0.6081 - val_recall: 0.5696\n",
      "Epoch 99/100\n",
      "633/633 [==============================] - 5s 7ms/step - loss: 0.0028 - accuracy: 0.6461 - precision: 0.6486 - recall: 0.6404 - val_loss: 0.0030 - val_accuracy: 0.6038 - val_precision: 0.6081 - val_recall: 0.5696\n",
      "Epoch 100/100\n",
      "633/633 [==============================] - 5s 7ms/step - loss: 0.0028 - accuracy: 0.6461 - precision: 0.6486 - recall: 0.6404 - val_loss: 0.0030 - val_accuracy: 0.6038 - val_precision: 0.6081 - val_recall: 0.5696\n",
      "5/5 [==============================] - 1s 9ms/step\n",
      "\n",
      "🎯 Finished schedule 'Nonconvex Only' with test loss: 0.0030, accuracy: 0.6038, precision: 0.6081, recall: 0.5696, f1-score: 0.5882\n",
      "🔍 Computing SHAP values...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/boabangfrancis/anaconda3/lib/python3.11/site-packages/keras/src/backend.py:452: UserWarning: `tf.keras.backend.set_learning_phase` is deprecated and will be removed after 2020-10-11. To update it, simply pass a True/False value to the `training` argument of the `__call__` method of your layer or model.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ SHAP values computed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/ql/5nmrkn3145n9_pzr7h7w7_gh0000gn/T/ipykernel_30863/2641176262.py:55: UserWarning: Glyph 128269 (\\N{LEFT-POINTING MAGNIFYING GLASS}) missing from current font.\n",
      "  plt.tight_layout()\n",
      "/Users/boabangfrancis/anaconda3/lib/python3.11/site-packages/IPython/core/pylabtools.py:152: UserWarning: Glyph 128269 (\\N{LEFT-POINTING MAGNIFYING GLASS}) missing from current font.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABKUAAAJOCAYAAABm7rQwAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8pXeV/AAAACXBIWXMAAA9hAAAPYQGoP6dpAACWeElEQVR4nOzdeZyN9f//8eeZ3WBoMNYZjH3fJmtZG0LWapR1DH1oKiQVKWbsFEmFFnu2ZElRTLJLH0TpY1JCk8wk+5Yxy/X7w2/O1zGLM8xch3Me99vt3HLe1/t9Pd/vyzlT8+paLIZhGAIAAAAAAABM5OboCQAAAAAAAMD1UJQCAAAAAACA6ShKAQAAAAAAwHQUpQAAAAAAAGA6ilIAAAAAAAAwHUUpAAAAAAAAmI6iFAAAAAAAAExHUQoAAAAAAACmoygFAAAAAAAA01GUAoB7XGxsrAoWLJjl67fffstwbPPmzWWxWG77ioqKyvV1zJgxQw0bNlThwoXl7e2toKAgPfXUU/rf//6Xrm9CQoKef/55BQcHK0+ePCpdurT69eunuLi42+Zs2bJFFotFn332WW4swxTr16835e9Ekk6ePKmoqCgdOHDArv5pxzej1xNPPHFPzPFetXDhQhUpUkSXLl2ytl25ckWTJ09WrVq15Ofnp/z586tcuXIKCwvT1q1brf1u97l+/vnnZbFYMs2uW7euLBaL3nrrrQy3z58/3+bv0sPDQ6VKlVLfvn31119/Wftt2rRJ+fLls2mz19ChQ2WxWPTYY49le+y9Lioqyq6ftc2bN7/rrLv5md28efMcmcOdSEpK0gcffKAHH3xQ/v7+8vX1VenSpdWpUyetXr06W/s6fvx4lp/nnJT23Th+/Hi2x6Z9b7ds2ZLj8wIAZ+Hh6AkAALKWkpKi6tWra8eOHRluf+ihh5SSkpLhtpkzZ+rixYvW9+vWrdO4ceM0b948Va5c2dpeqlSpnJ10Bs6cOaO2bduqVq1aeuCBB3T06FFNmjRJDRo00L59+1SpUiVJUmJiopo2bapz584pOjpaVatW1eHDhzV69Ght2LBBsbGxyp8/f67P15HWr1+v999/35TC1MmTJxUdHa0yZcqodu3ado+bMGGCWrRoYdNWqFChHJ7dDXc6x3vJ1atX9dprr+nVV1+1fn5TUlLUunVrHTx4UC+//LLq168vSfrtt9/0xRdfaPv27WrWrNldZx84cED79++XJM2ZM0fDhg3LtG/az4Z///1X27Zt08SJE7V161YdPHhQefPmVatWrVS/fn299tprWrBggd1zSEpK0ieffCJJ+vrrr/XXX3+pZMmSd7ewe0j//v316KOPWt/Hx8era9eueuGFF9S9e3dru5+f311nfffdd3f8M3vmzJl3nX+nevXqpVWrVmnIkCGKjo6Wt7e3jh49qq+//lobNmxQly5dHDY3AIDjUJQCACdWtWpVm/e//PKLJKl69eoKCQkxdS7R0dE275s1a6aGDRuqatWqWrx4scaMGSNJ2r59u3777Td9/PHH6tevn6Qb/3ffz89P3bt31zfffOO0v7xcvXpVvr6+jp6GXSpUqKCGDRs6ehp3JSUlRcnJyfL29s71rAULFujMmTPq37+/tW3btm3atWuX5s6dq759+1rb27Rpo+eff16pqak5kv3xxx9Lktq3b69169Zp165daty4cYZ9b/7Z0KJFC6WkpGjs2LFas2aNevToIUl67rnn1K1bN40bN06BgYF2zeHzzz/XP//8Y53DggUL9Nprr+XA6qR///1XPj4+WZ4plttKlSplUyhKO6smKCgoy+9JUlKS9cw0e93N9+7WfyeY5dixY1q+fLlGjRpl8++CVq1a6ZlnnsmxzzoA4P7D5XsA4OJSU1M1ZcoUVa5cWd7e3goICFDv3r114sQJm37NmzdX9erVtX37djVs2FB58uRRyZIl9cYbb2R6ptbtFClSRJJsfiHz9PSUJBUoUMCmb8GCBSVJPj4+2c5Ju7Tmp59+0pNPPqkCBQrI399fQ4cOVXJysg4fPqxHH31U+fPnV5kyZTRlyhSb8WmXYHzyyScaOnSoihUrpjx58qhZs2bWM1ButnbtWjVq1Ei+vr7Knz+/QkND9d1332U4px9++EFPPPGEHnjgAZUrV07h4eF6//33Jcnmsp+0X3Lff/99NW3aVAEBAcqbN69q1KihKVOmKCkpyWb/aX9fe/bs0cMPPyxfX18FBwdr0qRJ1l8At2zZogcffFCS1Ldv3xy9nPObb75Rq1at5OfnJ19fXzVp0kSbNm2y6XPkyBH17dtXFSpUkK+vr0qWLKkOHTro4MGD1j63m2NmlyOFh4erTJky1vdpl/tMmTJF48aNU9myZeXt7a3NmzdLkvbu3auOHTvK399fPj4+qlOnjj799FObfV69elXDhg1T2bJl5ePjI39/f4WEhGjp0qW3PR6zZs1Shw4drJ9j6cbZg5JUvHjxDMe4ud39f6Zdu3ZNS5YsUb169fT2229LkubOnWv3+LQCyB9//GFt69Chg/Lly6ePPvrI7v3MmTNHXl5emjdvngIDAzVv3jwZhpGu3y+//KKnn35aRYsWtV7m27t3byUmJkr6v0upNm7cqIiICBUpUkS+vr5KTEy0+2fZ/v379dhjjykgIEDe3t4qUaKE2rdvb9NvxYoVatCggQoUKGD97kRERNi93oyk/RxZtGiRXnrpJZUsWVLe3t46cuSI/vnnH0VGRqpq1arKly+fAgIC1LJlS23fvj3dfm79jqYdk82bN+vZZ59V4cKFVahQIXXt2lUnT560GXvr9+Xmy+CmTZumsmXLKl++fGrUqJF2796dLvujjz5SxYoV5e3trapVq2rJkiXpvmsZye5n/fz583rppZcUHBxs/bts166d9X+s3Myeedvz/Zak3bt3q0mTJvLx8VGJEiU0YsSIdD9bpcwvoSxTpozCw8MzXGN253M3P28A4H5CUQoAXNyzzz6rV199VaGhoVq7dq3Gjh2rr7/+Wo0bN9bp06dt+iYkJOipp55Sjx499Pnnn+uJJ57QuHHjNHjwYLvzUlJSlJiYqF9++UX9+/dXQECAzVkiTZo0Ub169RQVFaU9e/bo8uXL+uGHH/Taa6+pbt26euSRR+54rWFhYapVq5ZWrlypZ555Rm+//bZefPFFde7cWe3bt9fq1avVsmVLvfrqq1q1alW68a+99pqOHj2qjz/+WB9//LFOnjyp5s2b6+jRo9Y+S5YsUadOneTn56elS5dqzpw5OnfunJo3b57hJZhdu3ZV+fLltWLFCs2ePVtvvPGG9d5M3333nfWV9svc77//ru7du2vRokX68ssv1a9fP7355psaMGBAun0nJCSoR48e6tmzp9auXau2bdtqxIgR1suo6tatq3nz5kmSXn/9dWvWzWfzZCY1NVXJyck2rzSffPKJWrduLT8/Py1YsECffvqp/P391aZNG5vC1MmTJ1WoUCFNmjRJX3/9td5//315eHioQYMGOnz48F3PMSMzZszQt99+q7feektfffWVKleurM2bN6tJkyY6f/68Zs+erc8//1y1a9dWt27dNH/+fOvYoUOHatasWRo0aJC+/vprLVq0SE8++aT1F+7MnDhxQgcPHkx3uWNISIg8PT01ePBgLV68WPHx8bedf0bHPTk5OcMCjyStWrVK586dU0REhCpUqKCHHnpIy5cv1+XLl29/sHSjcCj9XwFZkry8vNS4cWOtW7fOrn2cOHFCGzduVKdOnVSkSBH16dNHR44c0bZt22z6/fjjj3rwwQe1e/dujRkzRl999ZUmTpyoxMREXb9+3aZvRESEPD09tWjRIn322Wfy9PS062fZlStXFBoaqr///lvvv/++YmJiNH36dAUFBVnv9fXdd9+pW7duCg4O1rJly7Ru3TqNGjXK5jN+N0aMGKG4uDjNnj1bX3zxhQICAnT27FlJ0ujRo7Vu3TrNmzdPwcHBat68ud33I+rfv788PT21ZMkSTZkyRVu2bFHPnj3tGnvzsVi8eLGuXLmidu3a6cKFC9Y+H374of7zn/+oZs2aWrVqlV5//XVFR0fbNb8qVaqoYMGCio6O1ocffpjl/ZkuXbqkhx56SB988IH69u2rL774QrNnz1bFihXTfUfsmbe93+9Dhw6pVatWOn/+vObPn6/Zs2dr//79GjdunF3H0F65/fMGAO47BgDgnnbw4EGjSZMmmW5v0qSJERsba9e+5s2bZ0gy9uzZYxiGYcTGxhqSjMjISJt+33//vSHJeO2116xtzZo1MyQZn3/+uU3fZ555xnBzczP++OMPu+bg7e1tSDIkGRUrVjQOHTqUrs/FixeNDh06WPtJMpo3b26cOXPmtvvfvHmzIclYsWKFtW306NGGJGPq1Kk2fWvXrm1IMlatWmVtS0pKMooUKWJ07do13T7r1q1rpKamWtuPHz9ueHp6Gv379zcMwzBSUlKMEiVKGDVq1DBSUlKs/S5dumQEBAQYjRs3TjenUaNGpVvDc889Z9jzr+iUlBQjKSnJWLhwoeHu7m6cPXvWui3t7+v777+3GVO1alWjTZs21vd79uwxJBnz5s27bZ5h/N+xyOj122+/GVeuXDH8/f2NDh06pJtrrVq1jPr162e67+TkZOP69etGhQoVjBdffNGuOTZr1sxo1qxZuvY+ffoYpUuXtr4/duyYIckoV66ccf36dZu+lStXNurUqWMkJSXZtD/22GNG8eLFrX+X1atXNzp37pzp/DOzfPlyQ5Kxe/fudNvmzJlj5MuXz3oMixcvbvTu3dvYtm2bTb+sjvvNr1u1bNnS8PHxMc6dO2cYxv/9DJgzZ45Nv7T23bt3G0lJScalS5eML7/80ihSpIiRP39+IyEhwab/yJEjDTc3N+Py5cu3Xf+YMWMMScbXX39tGIZhHD161LBYLEavXr3SzbVgwYLGqVOnMt1X2jx79+5t027vz7K9e/cakow1a9ZkmvHWW28Zkozz58/fdm2ZSfu8vfnmm9a2tL/Dpk2b3nZ8cnKykZSUZLRq1cro0qWLzTZJxujRo63v047JrWufMmWKIcmIj4+3tt36fUmbZ40aNYzk5GRr+3//+19DkrF06VLDMG58f4sVK2Y0aNDAJuOPP/4wPD09bb5rmVm3bp1RuHBh62e1UKFCxpNPPmmsXbvWpl/a5yUmJibTfdk7b8Ow//vdrVs3I0+ePDaf9eTkZKNy5cqGJOPYsWPW9lv/DtKULl3a6NOnj/V92t/55s2bsz2fO/15AwD3G86UAgAXlnbp0q2XG9SvX19VqlRJd7lV/vz51bFjR5u27t27KzU1Nd1ZD5nZtWuXvvvuO33yySfKnz+/WrRoYfMEvqSkJHXr1k0HDhzQRx99pG3btmnBggX666+/FBoaavN/wLPr1qd+ValSRRaLRW3btrW2eXh4qHz58jaXK6Xp3r27zX1rSpcurcaNG1uP4+HDh3Xy5En16tXL5nKUfPny6fHHH9fu3bt19epVm30+/vjj2VrD/v371bFjRxUqVEju7u7y9PRU7969lZKSol9//dWmb7Fixaw3z05Ts2bNDNeWXZMnT9aePXtsXoGBgdq1a5fOnj2rPn362JzJk5qaqkcffVR79uzRlStXJEnJycmaMGGCqlatKi8vL3l4eMjLy0u//fabYmNj73qOGenYsaP1ElHpxplAv/zyi/V+STfPuV27doqPj7eetVW/fn199dVXGj58uLZs2aJ///3Xrsy0S6gCAgLSbYuIiNCJEye0ZMkSDRo0SIGBgfrkk0/UrFkzvfnmm+n6Z3Tc9+zZo7CwsHR9jx07ps2bN6tr167WywaffPJJ5c+fP9NL+Bo2bChPT0/lz59fjz32mIoVK6avvvpKRYsWtekXEBCg1NRUJSQkZLl2wzCsl+yFhoZKksqWLavmzZtr5cqV1gcxXL16VVu3blVYWJjNWVmZufV7Y+/PsvLly+uBBx7Qq6++qtmzZ+vQoUPp9p12uWhYWJg+/fTTO3rSYHbmnmb27NmqW7eufHx85OHhIU9PT23atMnu78KtP5tr1qwpSXZ939u3by93d/dMxx4+fFgJCQnpPmdBQUFq0qSJXfNr166d4uLitHr1ag0bNkzVqlXTmjVr1LFjRz3//PPWfl999ZUqVqxo11mxt5t3dr7fmzdvVqtWrWw+6+7u7urWrZtd67OHGT9vAOB+Q1EKAFxYVvf5KFGiRLrLBG79xVS6Ufi4eV+3U7duXTVs2FA9evTQ5s2bZRiGzQ2P58yZo6+++kqrVq1S//799fDDD6t37976+uuv9cMPP2j69On2Li8df39/m/deXl7y9fVNd58qLy8vXbt2Ld34tLXe2pa29tsdz9TUVJ07d86mPbN7rGQkLi5ODz/8sP766y+988472r59u/bs2WO9B9Wtv7Rk9DQ8b2/vHPnlJjg4WCEhITYvb29v/f3335KkJ554Qp6enjavyZMnyzAM66VKQ4cO1RtvvKHOnTvriy++0Pfff689e/aoVq1aufYL2K3HO22+w4YNSzffyMhISbJe+jVjxgy9+uqrWrNmjVq0aCF/f3917txZv/32W5aZaWvJ7H5oBQoU0NNPP6133nlH33//vX766ScVLVpUI0eO1Pnz5236ZnTcQ0JCMizkzJ07V4Zh6IknntD58+d1/vx5JSUlqWPHjtq5c2eG9+dZuHCh9uzZo/379+vkyZP66aefMiw6pK3ldn9P3377rY4dO6Ynn3xSFy9etM4jLCxMV69etd4f59y5c0pJSbH7qXK3/j3a+7OsQIEC2rp1q2rXrq3XXntN1apVU4kSJTR69GjrvYOaNm2qNWvWKDk5Wb1791apUqVUvXr1HLuXT0ZznDZtmp599lk1aNBAK1eu1O7du7Vnzx49+uijdn8Xbv2+p93A357xtxubdvwy+ndARm2ZyZMnjzp37qw333xTW7du1ZEjR1S1alW9//771v858c8//9j9ObjdvLPz/T5z5kymP+Nzihk/bwDgfsPT9wDAhaX9B318fHy6XwJOnjypwoUL27Sl/Qf1zdLOlMioAHI7+fPnV+XKlW3O8Dlw4IDc3d1Vt25dm77BwcEqVKiQfv7552zn5JSMzgpJSEiwrv3m43mrkydPys3NTQ888IBNe3aeGLZmzRpduXJFq1atUunSpa3tBw4csHsfuS3tM/Puu+9m+pSwtF9iP/nkE/Xu3VsTJkyw2X769GmbG4JnxcfHJ8Oz5269H1qaW4932nxHjBihrl27ZjimUqVKkqS8efMqOjpa0dHR+vvvv61nMXTo0CHDAs+tGWfPnrWrCFmtWjU99dRTmj59un799dd0Z7vZIzU11Xp/mszWNXfu3HQ39a9SpYpdT+ZMKyze+jPiVnPmzJF0o+gybdq0DLcPGDBA/v7+cnd3T3dT8szc+veYnZ9lNWrU0LJly2QYhn766SfNnz9fY8aMUZ48eTR8+HBJUqdOndSpUyclJiZq9+7dmjhxorp3764yZcqoUaNGds3R3rlLN74LzZs316xZs2za0+5z5WhpxzerfwfciaCgIP3nP//RkCFD9L///U/VqlVTkSJF7P4c3E52vt+FChXK9Gf8rby9va0337/Z7f7njBk/bwDgfsOZUgDgwlq2bClJ1htfp9mzZ49iY2PVqlUrm/ZLly5p7dq1Nm1LliyRm5ubmjZtmu3806dP6+DBgypfvry1rUSJEkpJSdGePXts+v766686c+aM3f8HPTcsXbrU5obSf/zxh3bt2mV9mlWlSpVUsmRJLVmyxKbflStXtHLlSusT+W4nszMc0n6ZTdsu3bg8KjtPQbM36041adJEBQsW1KFDhzI8oyckJEReXl6Sbqzn5rVI0rp169JdLpXVHMuUKaNff/3V5hfEM2fOaNeuXXbNt1KlSqpQoYJ+/PHHTOebP3/+dOOKFi2q8PBwPf300zp8+HC6yzJvVrlyZUk3blJ/szNnzqS7gXeatF86S5QoYdc6brVhwwadOHFCzz33nDZv3pzuVa1aNS1cuPCOb9599OhRFSpUKMuzZM6dO6fVq1erSZMmGc6hR48e2rNnj37++Wfr0yxXrFiRaUExK9n9WSbd+PzVqlVLb7/9tgoWLKgffvghXR9vb281a9ZMkydPlqQMn7aZEzL6Lvz000/pntrpKJUqVVKxYsXSPSEuLi7Oru/apUuXMr25ftrliWmf9bZt2+rXX3/Vt99+e5ezzt73u0WLFtq0aZNN4S0lJUXLly9Pt98yZcrop59+smn79ttvb/sAATN+3gDA/YYzpQDAhVWqVEn/+c9/9O6778rNzU1t27bV8ePH9cYbbygwMFAvvviiTf9ChQrp2WefVVxcnCpWrKj169fro48+0rPPPqugoKBMcy5cuKDQ0FB1795dFSpUUJ48efTrr7/qnXfeUWJiokaPHm3t27dvX7399tt6/PHH9frrr6tSpUo6evSoJkyYoLx582rgwIG5djxu59SpU+rSpYueeeYZXbhwQaNHj5aPj49GjBgh6cZjzadMmaIePXroscce04ABA5SYmKg333xT58+f16RJk+zKqVGjhqQb9w9q27at3N3dVbNmTYWGhsrLy0tPP/20XnnlFV27dk2zZs1Kd0lgdpQrV0558uTR4sWLVaVKFeXLl08lSpS442JIvnz59O6776pPnz46e/asnnjiCQUEBOiff/7Rjz/+qH/++cd6Nshjjz2m+fPnq3LlyqpZs6b27dunN998M13hMas59urVSx988IF69uypZ555RmfOnNGUKVPk5+dn95w/+OADtW3bVm3atFF4eLhKliyps2fPKjY2Vj/88INWrFghSWrQoIEee+wx1axZUw888IBiY2O1aNGi2xYbGzRooDx58mj37t029/3ZvHmzBg8erB49eqhx48YqVKiQTp06paVLl+rrr7+2Xjp2J+bMmSMPDw+99tprGf5dDhgwQIMGDdK6devUqVOnbO9/9+7datasWZZn+i1evFjXrl3ToEGDrIXbmxUqVEiLFy/WnDlz9Pbbb2vatGl66KGH1KBBAw0fPlzly5fX33//rbVr1+qDDz7I8Jf1NPb+LPvyyy81c+ZMde7cWcHBwTIMQ6tWrdL58+et97waNWqUTpw4oVatWqlUqVI6f/683nnnHXl6eqpZs2bZPlb2eOyxxzR27FiNHj1azZo10+HDhzVmzBiVLVs2x576dzfc3NwUHR2tAQMG6IknnlBERITOnz+v6OhoFS9e3OYeehk5fPiw2rRpo6eeekrNmjVT8eLFde7cOa1bt04ffvihmjdvrsaNG0uShgwZouXLl6tTp04aPny46tevr3///Vdbt27VY489lu4plrdj7/f79ddf19q1a9WyZUuNGjVKvr6+ev/99633wLtZr1699MYbb2jUqFFq1qyZDh06pPfee08FChTIsfnc6c8bALjvOOwW6wAAu+Tm0/cM48ZTlSZPnmxUrFjR8PT0NAoXLmz07NnT+PPPP23GNmvWzKhWrZqxZcsWIyQkxPD29jaKFy9uvPbaa+meInSra9euGf379zeqVKli5MuXz/Dw8DBKlSpl9OzZ0/jf//6Xrv9vv/1m9OrVyyhTpozh7e1tBAUFGd26dcuw762yevreP//8Y9O3T58+Rt68edPtI22tt+5z0aJFxqBBg4wiRYoY3t7exsMPP2zs3bs33fg1a9YYDRo0MHx8fIy8efMarVq1Mnbu3GnTJ7M5GYZhJCYmGv379zeKFCliWCwWmyc/ffHFF0atWrUMHx8fo2TJksbLL79sfPXVV+me8HTrGm5e861Pylq6dKlRuXJlw9PTM9OnSt16LG4+vhnZunWr0b59e8Pf39/w9PQ0SpYsabRv395m3Llz54x+/foZAQEBhq+vr/HQQw8Z27dvz/CJelnNccGCBUaVKlUMHx8fo2rVqsby5cszffrezU9Du9mPP/5ohIWFGQEBAYanp6dRrFgxo2XLlsbs2bOtfYYPH26EhIQYDzzwgOHt7W0EBwcbL774onH69Oksj4VhGEavXr2MqlWr2rT9+eefxuuvv240adLEKFasmOHh4WHkz5/faNCggfHuu+/aPFXsdsf95ic2/vPPP4aXl1eWT+46d+6ckSdPHutTEjP62ZCZI0eOGJKMlStXZtmvdu3aRkBAgJGYmJhpn4YNGxqFCxe29jl06JDx5JNPGoUKFTK8vLyMoKAgIzw83Lh27dpt52nPz7JffvnFePrpp41y5coZefLkMQoUKGDUr1/fmD9/vrXPl19+abRt29YoWbKk4eXlZQQEBBjt2rUztm/ffttjkyarp+9l9HeYmJhoDBs2zChZsqTh4+Nj1K1b11izZk2G39dbP/+ZHZOMnvyW2dP3MvpeZPSz4MMPPzTKly9veHl5GRUrVjTmzp1rdOrUyahTp06Wx+PcuXPGuHHjjJYtW1qPa968eY3atWsb48aNM65evZqu/+DBg42goCDD09PTCAgIMNq3b2/88ssvdzRve77fhmEYO3fuNBo2bGh4e3sbxYoVM15++WXjww8/TPf0vcTEROOVV14xAgMDjTx58hjNmjUzDhw4YNfT9+ydz938vAGA+4nFMG66vgAAcM/5+eefNXDgQO3YsSPD7Q899JA+/vhj6yVCuaV58+Y6ffq0Q+/p5ChbtmxRixYttGLFCj3xxBOOng7uQ3v37tWDDz6o3bt3q0GDBo6ezl154403tHDhQv3+++/y8OCke1d2/vx5VaxYUZ07d9aHH37o6OkAAO5D3FMKAAAgl4WEhCgsLExjx4519FTuyvnz5/X+++9rwoQJFKRcTEJCgl544QWtWrVKW7du1cKFC9WiRQtdunRJgwcPdvT0AAD3Kf5rAgDuce7u7vrxxx8zfRpZSkrKbe/nAcDxpk6dqjlz5ujSpUtZ3h/pXnbs2DGNGDFC3bt3d/RUYDJvb28dP35ckZGROnv2rHx9fdWwYUPNnj1b1apVc/T0AAD3KS7fAwAAAAAAgOn4X+sAAAAAAAAwHUUpAAAAAAAAmI6iFAAAAAAAAEznkjc6T01N1cmTJ5U/f35ZLBZHTwcAAAAAAMBpGIahS5cuqUSJElk+lMnhRalt27bpzTff1L59+xQfH6/Vq1erc+fOWY5JTEzUmDFj9MknnyghIUGlSpXSyJEjFRERYVfmyZMnFRgYmAOzBwAAAAAAQEb+/PNPlSpVKtPtDi9KXblyRbVq1VLfvn31+OOP2zUmLCxMf//9t+bMmaPy5cvr1KlTSk5Otjsz7THMf/75p/z8/LI136SkJG3cuFGtW7eWp6dntsbeKVfJdFQua3XOXFfJdFQua3XOXFfJdFQua3XOXFfJdFQua3W+TEflslbnzHWVTEfl3k3mxYsXFRgYaK2/ZMbhRam2bduqbdu2dvf/+uuvtXXrVh09elT+/v6SpDJlymQrM+2SPT8/vzsqSvn6+srPz8/UD4IrZDoql7U6Z66rZDoql7U6Z66rZDoql7U6Z66rZDoql7U6X6ajclmrc+a6SqajcnMi83a3TLrvbnS+du1ahYSEaMqUKSpZsqQqVqyoYcOG6d9//3X01AAAAAAAAGAnh58plV1Hjx7Vjh075OPjo9WrV+v06dOKjIzU2bNnNXfu3AzHJCYmKjEx0fr+4sWLkm5U/ZKSkrKVn9Y/u+PuhqtkOiqXtTpnrqtkOiqXtTpnrqtkOiqXtTpnrqtkOiqXtTpfpqNyWatz5rpKpqNy7ybT3jEWwzCMbO89l1gsltve6Lx169bavn27EhISVKBAAUnSqlWr9MQTT+jKlSvKkydPujFRUVGKjo5O175kyRL5+vrm2PwBAAAAAABc3dWrV9W9e3dduHAhy9sm3XdnShUvXlwlS5a0FqQkqUqVKjIMQydOnFCFChXSjRkxYoSGDh1qfZ92w63WrVvf0T2lYmJiFBoaaup1nK6Q6ahc1uqcua6S6ahc1uqcua6S6ahc1uqcua6S6ahc1up8mY7Kdca1pqamKikpSbeeZ5KcnKxdu3apcePG8vAw71d+R+S6SqajcjPLtFgs8vDwkLu7e6Zj065Qu537rijVpEkTrVixQpcvX1a+fPkkSb/++qvc3Nwyfcygt7e3vL2907V7enre8Q+Guxl7p1wl01G5rNU5c10l01G5rNU5c10l01G5rNU5c10l01G5rNX5Mh2V6yxrvX79uo4fP67U1NR02wzDULFixRQfH3/bm0znJEfkukqmo3Jvl1mwYEEVK1Ysw232ft4dXpS6fPmyjhw5Yn1/7NgxHThwQP7+/goKCtKIESP0119/aeHChZKk7t27a+zYserbt6+io6N1+vRpvfzyy4qIiMjw0j0AAAAAAJyFYRiKj4+Xu7u7AgMD5eZm+/yy1NRU60kct27LTY7IdZVMR+VmlmkYhq5evapTp05JunFF251yeFFq7969atGihfV92mV2ffr00fz58xUfH6+4uDjr9nz58ikmJkYvvPCCQkJCVKhQIYWFhWncuHGmzx0AAAAAADMlJyfr6tWrKlGiRIb3SE5NTdX169fl4+NjetHE7FxXyXRUblaZaScFnTp1SgEBAVleypcVhxelmjdvnu4a2JvNnz8/XVvlypUVExOTi7MCAAAAAODek5KSIkny8vJy8Ezg6tKKoklJSXdclDKvrAcAAAAAAHKEmfczAjKSE59BilIAAAAAAAAwHUUpAAAAAACQ45o3b64hQ4bY3f/48eOyWCw6cOBArs0ppwQHB2v69Ok5vt/w8HB17tw5W2MsFovWrFmT43MxA0UpAAAAAABcmMViyfIVHh5+R/tdtWqVxo4da3f/wMBAxcfHq3r16neUlx0rV65UgwYNVKBAAeXPn1/VqlXTSy+9lOu5sOXwG50DAAAAAADHiY+Pt/55+fLlGjVqlA4fPmxtS3vSWpqkpCR5enredr/+/v7Zmoe7u7uKFSuWrTF34ptvvtFTTz2lCRMmqGPHjrJYLDp06JA2bdqU69mwxZlSAAAAAAC4sGLFillfBQoUkMVisb6/du2aChYsqE8//VTNmzeXj4+PPvnkE505c0ZPP/20SpUqJV9fX9WqVUufffaZzX5vvXyvTJkymjBhgiIiIpQ/f34FBQXpww8/tG6/9fK9LVu2yGKxaNOmTQoJCZGvr68aN25sUzCTpPHjxysgIED58+dX//79NXz4cNWuXTvT9X755Zd66KGH9PLLL6tSpUqqWLGiOnfurHfffdem39q1axUSEiIfHx8VLlxYXbt2tdl+9erVTNciSX/99Ze6deumBx54QIUKFVKnTp10/Phx6/aUlBQNHTpUBQsWVKFChfTKK6/IMAybfZQpUybdZYK1a9dWVFRUpuu7Xe69hKIUAAAAAADI0quvvqpBgwYpNjZWbdq00bVr11SvXj19+eWX+vnnn/XMM89o4MCB+v7777Pcz9SpUxUSEqL9+/crMjJSzz77rH755Zcsx4wcOVJTp07V3r175eHhoYiICOu2Tz/9VBMmTNDkyZO1b98+BQUFadasWVnur1ixYvrf//6nn3/+OdM+69atU9euXdW+fXvt37/fWhizdy1Xr15VixYtlC9fPm3btk07duxQvnz59Oijj+r69evW8XPnztWcOXO0Y8cOnT17VqtXr85y7rdjT+69hMv3AAAAAABAloYMGZLuTKFhw4ZZ//z888/ryy+/1GeffaZGjRplup927dopMjJS0o1C19tvv60tW7aocuXKmY4ZP368mjVrJkkaPny42rdvr2vXrsnLy0sfffSRIiIi1LdvX0nSqFGjtHHjRl2+fDnT/b3wwgvavn27atSoodKlS6thw4Zq3bq1evToIW9vb2vmU089pejoaOu4WrVqKTU11a61LFu2TG5ubvr4449lsVgkSfPmzVPBggW1ZcsWtW7dWtOnT9eIESP0+OOPS5Jmz56tDRs2ZDpve9iTey/hTCkAAAAAAJClW88SSklJ0fjx41WzZk0VKlRIfn5+2rx5s+Li4rLcT82aNa1/TrtM8NSpU3aPKV68uCRZx/z222968MEHbfrXr18/y/3lzZtX69at05EjR/T6668rX758eumll1S/fn1dvXpVknTgwAG1atXqjteyb98+HTlyRPnz51e+fPmUL18++fv769q1a/r999914cIFxcfH2xTwPDw80h3n7Lpd7r2GM6UAAAAAAECW8ubNa/N+6tSpevvttzV9+nTVqFFDefLk0QsvvHDbS8RuvUG6xWKxOfvodmPSzv65eUxaW5pb78uUmXLlyqlcuXLq37+/Ro4cqYoVK2r58uXq27dvupu7325eafNIm1dqaqrq1aunxYsXpxtXpEgRu+YnSW5ubunWk5SUlGn/nMo1C2dKAQAAAACAbNm+fbs6deqknj17qlatWgoODtbRo0dNn0eFChW0Z88em7a9e/dmez9lypSRr6+vrly5IunGWVB38zS+unXr6rffflNAQIDKly9v8ypQoIAKFCig4sWLa/fu3dYxycnJ2rdvn81+ihQpYvN0xIsXL+rYsWN3nHuvoSgFAAAAAACypXz58oqJidGuXbsUGxurgQMH6u+//zZ9Hs8884zmzp2rBQsW6LffftO4ceP0008/pTt76mZRUVF65ZVXtGXLFh07dkz79+9XRESEkpKSFBoaKkkaPXq0li5dqtGjRys2NlYHDx7UlClT7J5Xjx49VLhwYXXq1Enbt2/XsWPHtHXrVg0ePFgnTpyQJA0ePFiTJk3S6tWr9csvvygyMlLnz5+32U/Lli21aNEibd++XYcOHVJ4eLjc3d3vKvdeQlEKAAAAAABkyxtvvKG6deuqTZs2at68uYoVK6b27dubPo+wsDANHz5cw4YNU926dXXs2DGFh4fLx8cn0zHNmjXT0aNH1bt3b1WuXFlt27ZVQkKCNm7cqEqVKkmSmjdvrhUrVmjt2rWqXbu2WrZsedsnC97M19dX27ZtU1BQkLp27aoqVaooIiJC//77r/z8/CRJL730knr37q3w8HA1atRI+fPnV5cuXWz2M2LECDVt2lQdO3ZUWFiYOnXqpHLlyt1V7r2Ee0rd66IKSG4+Uq0PpYmlpFHmV54BAAAAAK4hPDxc4eHh1vdlypTJ8B5N/v7+WrNmjfV9amqqLl68aFP42LJli82Y48ePp9vPgQMHMs1q3rx5uuzatWtb29Lu3/T6669r1KhR1j6hoaEqX758pmts0aKFWrRoken2NF27dk33xMG0zKNHj8rNzfY8n5vXIknFihXTggULMt2/h4eHpk+frunTp2fax8/PT8uXL7c5vmlPGkxz6zG6Xe69hKIUAAAAAAC4L129elVz5szRo48+Knd3dy1dulTffPONYmJiHD012IGiFAAAAAAAuC9ZLBZ99dVXGj9+vBITE1WpUiWtXLlSjzzyiKOnBjtQlLJX1P+/Sz2X0gEAAAAAcE/IkyePNm7cmO5SOtwf+FsDAAAAAACA6ShKAQAAAAAAwHQUpQAAAAAAAGA6ilIAAAAAAAAwHUUpAAAAAAAAmI6iFAAAAAAAAExHUQoAAAAAAOAeYrFYtGbNGkdPI9d5OHoCAAAAAADg7pQZvs7UvOOT2md7TEJCgsaPH69169bpr7/+UkBAgGrXrq0hQ4aoVatWuTBLc509e1ZjxozRmjVrdPLkSRUqVEiPPvqooqOjFRQU5Ojp3ZMoSgEAAAAAgFx1/PhxNWnSRAULFtSUKVNUs2ZNJSUlacOGDXruuef0yy+/OHqKd+Xs2bNq2LChvLy8NHPmTFWvXl3Hjx/X66+/rgcffFDfffedgoODHT3New6X7wEAAAAAgFwVGRkpi8Wi//73v3riiSdUsWJFVatWTUOHDtXu3but/eLi4tSpUyfly5dPfn5+CgsL099//23dHhUVpdq1a2vRokUKDg5WUFCQnn76aV26dEmS9MEHH6hkyZJKTU21ye/YsaP69Oljff/FF1+oXr168vHxUXBwsKKjo5WcnCxJGjNmjEqUKKEzZ87YjG/atGm6/aYZOXKkTp48qW+++Ubt2rVTUFCQmjZtqg0bNsjT01PPPfectW/z5s01aNAgvfLKK/L391exYsUUFRWV6bFr2bKlXnjhBZu2M2fOyNvbW99++22m4+4HFKUAAAAAAECuOXv2rL7++ms999xzyps3b7rtBQsWlCQZhqHOnTvr7Nmz2rp1q2JiYvT777+rW7duNv1///13rVmzRmvXrtWyZcu0bds2TZo0SZL05JNP6vTp09q8ebO1/7lz57Rhwwb16NFDkrRhwwb17NlTgwYN0qFDh/TBBx9o/vz5Gj9+vKQbBaYyZcqof//+kqTZs2dr27ZtWrRokdzc0pdRUlNTtWzZMvXo0UPFihWz2ZYnTx5FRkZqw4YNOnv2rLV9wYIFyps3r77//ntNmTJFY8aMUUxMTIbHr3///lq6dKkSExOtbYsXL1aJEiXUokWLjA/6fYKiFAAAAAAAyDVHjhyRYRiqXLlylv2++eYb/fTTT1qyZInq1aunBg0aaNGiRdq6dav27Nlj7Zeamqr58+erevXqaty4sXr27KlNmzZJkvz9/fXoo49qyZIl1v4rVqyQv7+/9b5V48eP1/Dhw9WnTx8FBwcrNDRUY8eO1QcffCBJcnd31yeffKJNmzZp+PDheumll/T++++rdOnSGc77n3/+0fnz51WlSpUMt1epUkWGYejIkSPWtpo1a2r06NGqUKGCevfurZCQEOsabvX444/LYrFo/fr11rZ58+YpPDxcFosly2N6r6MoBQAAAAAAco1hGJJ02wJKbGysAgMDFRgYaG2rWrWqChYsqNjYWGtbmTJllD9/fuv74sWL69SpU9b3PXr00MqVK61nFi1evFhPPfWU3N3dJUn79u3TmDFjlC9fPuvrmWeeUXx8vK5evSpJCg4O1ltvvaXJkyerQ4cO1rOscmr9NWvWtOlz6xpu5u3trR49emjx4sWSpAMHDujHH39UeHj4Hc/pXsGNzgEAAAAAQK6pUKGCLBaLYmNj1blz50z7GYaRYeHq1nZPT0+b7RaLxeZeTx06dFBqaqrWrVunBx98UNu3b9e0adOs21NTUxUdHa2uXbumy/Lx8bH+edu2bXJ3d9fx48eVnJwsD4+MSyhFihRRwYIFdejQoQy3//LLL7JYLCpXrpzda7hVv379VLduXZ04cUJz585Vq1atMj1z637CmVIAAAAAACDX+Pv7q02bNnr//fd15cqVdNvPnz8v6cZZUXFxcfrzzz+t2w4dOqQLFy5kemlcRvLkyaOuXbtq8eLFWrp0qSpWrKh69epZt9etW1eHDx9W+fLl073S7hm1fPlyrVq1Slu2bNGff/6psWPHZprn5uamsLAwLVmyRAkJCTbb/v33X82cOVNt2rSRv7+/3Wu4VY0aNVSnTh19/PHHWrJkiSIiIu54X/cSilIAAAAAACBXzZw5UykpKapfv75Wrlyp3377TbGxsZoxY4YaNWokSXrkkUdUs2ZN9ejRQz/88IP++9//qnfv3mrWrJlCQkKyldejRw+tW7dOc+fOVc+ePW22jRo1SgsXLlRUVJT+97//KTY2VsuXL9frr78uSTpx4oSeffZZTZ48WQ899JDmz5+viRMn2jwl8Fbjx49XsWLFFBoaqq+++kp//vmntm3bpjZt2igpKUnvv/9+No9Yer169dLkyZOVkpKiLl263PX+7gUUpQAAAAAAQK4qW7asfvjhB7Vo0UIvvfSSqlevrtDQUG3atEmzZs2SdOMStjVr1uiBBx5Q06ZN9cgjjyg4OFjLly/Pdl7Lli3l7++vw4cPq3v37jbb2rRpoy+//FIxMTF68MEH1bBhQ02bNk2lS5eWYRgKDw9X/fr19fzzz0uSQkND9fzzz6tnz566fPlyhnmFCxfW7t271aJFCw0YMEDBwcEKCwtTcHCw9uzZo+Dg4Gyv4VaPP/64PDw81L17d5vLDO9n3FMKAAAAAID73PFJ7SXduF/SxYsX5efnZ70U7V5RvHhxvffee3rvvfcy7RMUFKTPP/880+1RUVGKioqyaRs8eLBefPFFmzZ3d3edPHky0/20adNGbdq0yXDbN998k65t2rRpmjZtmvX4ZqRw4cKaMWOGZsyYkWmuJG3ZsiVd25o1a2zep90c/Wbnz5/XtWvX1K9fvyz3fz+hKAUAAAAAAHCPSkpK0l9//aWoqCg1bNhQdevWdfSUcgxFKQAAAAAAgHvUzp071aJFC5UvX16fffaZo6eToyhKAQAAAAAA3KOaN2+ulJQU62WZzuTeusAUAAAAAAAALoGiFAAAAAAAAExHUQoAAAAAAACmoygFAAAAAAAA01GUAgAAAAAAgOkoSgEAAAAAAMB0FKUAAAAAAMB9x93dXevWrcuyT3h4uDp37mzOhJBtHo6eAAAAAAAAuEtRBSTdOPOkoCl5F7I9JDw8XAsWLNCAAQM0e/Zsm22RkZGaNWuW+vTpo/nz52d738ePH1fZsmW1f/9+1a5d29r+zjvvyDAMu+d3/vx5rVmzJtv5uDOcKQUAAAAAAEwRGBioZcuW6d9//7W2Xbt2TUuXLlVQUFCO5xUoUEAFCxbM8f0iZ1CUAgAAAAAApqhbt66CgoK0atUqa9uqVasUGBioOnXqWNvKlCmj6dOn24ytXbu2oqKiMtxv2bJlJUl16tSRxWJR8+bNJaW/fO+zzz5TjRo1lCdPHhUqVEiPPPKIrly5oqioKC1YsECff/65LBaLLBaLtmzZIkl69dVXVbFiRfn6+qp8+fIaP368kpKSbPLHjRungIAA5c+fX/3799fw4cNtztiSpHnz5qlKlSry8fFR5cqVNXPmTPsPnJOiKAUAAAAAAEzTt29fzZs3z/p+7ty5ioiIuKt9/ve//5UkffPNN4qPj7cpeqWJj4/X008/rYiICMXGxmrLli3q2rWrDMPQsGHDFBYWpkcffVTx8fGKj49X48aNJUn58+fX/PnzdejQIb399ttauHChTcFs8eLFGj9+vCZPnqx9+/YpKChIs2bNssn+6KOPNHLkSI0fP16xsbGaMGGC3njjDS1YsOCu1n2/455SAAAAAADANL169dKIESN0/PhxWSwW7dy5U8uWLbOemXQnihQpIkkqVKiQihUrlmGf+Ph4JScnq2vXripdurQkqUaNGtbtefLkUWJiYrrxr7/+uvXPQUFBeu6557RixQq9+uqrkqR3331X/fr1U9++fSVJo0aN0saNG3X58mXruLFjx2rq1Knq2rWrpBtndh06dEgffPCB+vTpc8frvt9RlAIAAAAAAKYpXLiw2rdvrwULFsgwDLVv316FCxfO9dxatWqpVatWqlGjhtq0aaPWrVvriSee0AMPPJDluM8++0zTp0/XkSNHdPnyZSUnJ8vPz8+6/fDhw4qMjLQZU79+fX377beSpH/++Ud//vmn+vXrp2eeecbaJzk5WQUKFMjBFd5/KEoBAAAAAABTRURE6Pnnn5ckvf/+++m2u7m5pXtq3q33ccoud3d3xcTEaNeuXdq4caPeffddjRw5Ut9//731nlS32r17t5566ilFR0erTZs2yp8/vxYuXJhuzhaLxeb9zXNPTU2VdOMSvgYNGqSbkytz+D2ltm3bpg4dOqhEiRKyWCzZevTizp075eHhke7mYQAAAAAA4N716KOP6vr167p+/bratGmTbnuRIkUUHx9vfX/x4kUdO3Ys0/15eXlJklJSUrLMtVgsatKkiaKjo7V//355eXlp9erV1n3cOn7nzp0qXbq0Ro4cqZCQEFWoUEF//vmnTZ9KlSpZ72mVZu/evdY/Fy1aVCVLltTRo0dVvnx5m1dmxTBX4fAzpa5cuaJatWqpb9++evzxx+0ed+HCBfXu3VutWrXS33//nYszBAAAAAAAOcnd3V2xsbHWP9+qZcuWmj9/vjp06KAHHnhAb7zxRpZnFQUEBChPnjz6+uuvVapUKfn4+KS7NO7777/Xpk2b1Lp1awUEBOj777/XP//8oypVqki68cS/DRs26PDhwypUqJAKFCig8uXLKy4uTsuWLdODDz6oL7/8Ul9++aXNfl944QU988wzCgkJUePGjbV8+XL99NNPCg4OtvaJiorSoEGD5Ofnp7Zt2yoxMVF79+7VuXPnNHTo0Ds+jvc7hxel2rZtq7Zt22Z73IABA9S9e3e5u7tn6+wqAAAAAADgeDffl+lWI0aM0NGjR/XYY4+pQIECGjt2bJZnSnl4eGjGjBkaM2aMRo0apYcffjjdjdP9/Py0bds2TZ8+XRcvXlTp0qU1depUa03imWee0ZYtWxQSEqLLly9r8+bN6tSpk1588UU9//zzSkxMVLt27fTyyy9r8uTJ1v326NFDR48e1bBhw3Tt2jWFhYUpPDzc5uyp/v37y9fXV2+++aZeeeUV5c2bVzVq1NCQIUPu7OA5CYcXpe7EvHnz9Pvvv+uTTz7RuHHjbts/MTFRiYmJ1vcXL16UdON6VLuvSXXzuTHm5n/e5fWs9uaanqn/u1b3bq/ZvR9yWatz5rpKpqNyWatz5rpKpqNyWatz5rpKpqNyWavzZToq15nWmpSUJMMwlJqaar1XkSRp1DlJN+5ldOnSJeXPnz/dfY5y1M3Z+r97KKXNLSNz5879/0Mz3r5q1Srr9nz58mnp0qU223v16mUzPjk5WZcuXbJmRkREKCIi4qYpptpkVqpUSevXr89gKTf2V6hQIX399dfptk2aNEmTJk2yru/SpUt65ZVXbNYxcuRIjRw50vq+devWKleunE2fp556Sk899VSm+Vmx5/jmtNtlpqamyjAMJSUlpTuLzd7PvMW49c5hDmSxWLR69Wp17tw50z6//fabHnroIW3fvl0VK1ZUVFSU1qxZowMHDmQ6JioqStHR0enalyxZIl9f3xyYOQAAAAAAuc/Dw0PFihVTYGCg9T5KcKyrV69q3rx5atmypdzd3bVy5UpNmTJFq1evVvPmzR09vVxz/fp1/fnnn0pISFBycrLNtqtXr6p79+66cOFClmfE3VdnSqWkpKh79+6Kjo5WxYoV7R43YsQIm2s0L168qMDAQLVu3TrLg2NjYilJN85WiqkxQ6EHB8nz1SPZmv8dmVjK/EzdqGrGxMQoNDRUnp6epmQ6Kpe1Omeuq2Q6Kpe1Omeuq2Q6Kpe1Omeuq2Q6Kpe1Ol+mo3Kdaa3Xrl3Tn3/+qXz58snHxyfddtPOlLoHcu+VTE9PT23evFlTp05VYmKiKlWqpBUrVqhjx465mpvbbpd57do15cmTR02bNk33WUy7Qu127qui1KVLl7R3717t37/f+ujItNPFPDw8tHHjRrVs2TLdOG9vb3l7e6dr9/T0tP8HQ+o127Gp18z5YXZTrmmZN8nWMbrPc1mrc+a6Sqajclmrc+a6Sqajclmrc+a6Sqajclmr82U6KtcZ1pqSkiKLxSI3Nze5ubml2552mVVaH7M4IvdeycybN6+++eYb03Nz2+0y3dzcZLFYMvx82/t5v6+KUn5+fjp48KBN28yZM/Xtt9/qs88+c/lHKQIAAAAAANwvHF6Uunz5so4c+b9L0o4dO6YDBw7I399fQUFBGjFihP766y8tXLhQbm5uql69us34gIAA+fj4pGsHAAAAAADAvcvhRam9e/eqRYsW1vdp937q06eP5s+fr/j4eMXFxTlqegAAAAAAAMgFDi9KNW/eXFk9AHD+/PlZjo+KilJUVFTOTgoAAAAAAAC5yry7ngEAAAAAAAD/H0UpAAAAAAAAmI6iFAAAAAAAgIuJiopS7dq1HToHh99TCgAAAAAA3J0aC2qYmnewz8Fs9Q8PD9eCBQs0ceJEDR8+3Nq+Zs0adenSJct7TcPWl19+qbfeekv79u1TSkqKqlWrpueee07h4eGOnlq2caYUAAAAAADIdT4+Ppo8ebLOnTvn6Knct9577z116tRJjRs31vfff6+ffvpJTz31lAYOHKhhw4Y5enrZRlEKAAAAAADkukceeUTFihXTxIkTs+y3cuVKVatWTd7e3ipTpoymTp1qs71MmTKaMGGC+vXrp8DAQJUpU0YffvihTZ8TJ07oqaeekr+/v/LmzauQkBB9//331u2zZs1SuXLl5OXlpUqVKmnRokU24y0Wiz7++GN16dJFvr6+qlChgtauXStJSk1NVVBQkGbPnm0z5ocffpDFYtHRo0clSRcuXNB//vMfBQQEyM/PTy1bttSPP/4oSfrnn39UrFgxTZgwwTr++++/l5eXlzZu3JjhcTlx4oSGDRumIUOGaMKECapatarKly+vl156SW+++aamTp1qXeOWLVtksVi0adMmhYSEyNfXV40bN9bhw4cz3Pe2bdvk6emphIQEm/bXX39dzZs3z3BMTqAoBQAAAAAAcp27u7smTJigd999VydOnMiwz759+xQWFqannnpKBw8eVFRUlN544w3Nnz/fpt/UqVMVEhKirVu36tlnn9Wzzz6rX375RZJ0+fJlNWvWTCdPntTatWv1448/6pVXXlFqaqokafXq1Ro8eLBeeukl/fzzzxowYID69u2rzZs322RER0crLCxMP/30k9q1a6cePXro7NmzcnNzU7du3bR48WKb/kuWLFGjRo0UHBwswzDUvn17JSQkaP369dq3b5/q1q2rVq1a6ezZsypSpIjmzp2rqKgo7d27V5cvX1bPnj0VGRmp1q1bZ3hs1q5dq6SkpAzPiBowYIDy5cunpUuX2rSPHDlSU6dO1d69e+Xh4aGIiIgM9920aVMFBwfbFOeSk5P16aefqk+fPhmOyQkUpQAAAAAAgCm6dOmi2rVra/To0RlunzZtmlq1aqU33nhDFStWVHh4uJ5//nm9+eabNv3atWunZ599VsHBwXrllVdUuHBhbdmyRdKN4tA///yjNWvW6KGHHlL58uUVFhamRo0aSZLeeusthYeHKzIyUhUrVtTQoUPVtWtXvfXWWzYZ4eHhevrpp1W+fHlNmDBBV65c0X//+19JUvfu3bVz50798ccfkm6cPbVs2TL17NlTkrR582YdPHhQK1asUEhIiCpUqKC33npLBQsW1GeffWZdwzPPPKMePXpo4MCB8vHx0aRJkzI9dkeOHFGBAgVUvHjxdNu8vLwUHBysX3/91aZ9/PjxatasmapWrarhw4dr165dunbtWob779evn+bNm2d9v27dOv37778KCwvLdE53i6IUAAAAAAAwzeTJk7VgwQIdOnQo3bbY2Fg1adLEpq1Jkyb67bfflJKSYm2rWbOm9c8Wi0XFihXTqVOnJEkHDhxQnTp15O/vn2F+ZhmxsbE2bTdn5M2bV/nz57dm1KlTR5UrV7aembR161adOnXKWsDZt2+fLl++rEKFCilfvnzW17Fjx/T7779b9/vWW29Zz0havHixfHx8Mjlqt2cYhiwWS6ZrSCtmpa3hVuHh4Tpy5Ih2794tSZo3b546d+6svHnz3vGcboeiFAAAAAAAME3Tpk3Vpk0bvfbaa+m2ZVRYyejJfJ6enjbvLRaL9fK8PHny3HYOGWXc2pZVhiT16NFDS5YskXTj7Kw2bdqocOHCkm6cOVW8eHEdOHDA5nX48GG9/PLL1n0cPXpUJ0+eVGpqqvWsq8yUL19eFy5c0MmTJ9Ntu379uo4ePaoKFSpkuoa09d28hpsFBASoQ4cOmjdvnk6dOqWvvvpKPXr0yHJOd4uiFAAAAAAAMNWkSZP0xRdfaNeuXTbtVatW1Y4dO2zadu3apYoVK8rd3d2ufdesWVMHDhzQ2bNnM9xepUqVDDOqVKmSjRXcuITv4MGD2rdvnz777DObAk7dunWVkJAgDw8PlS9f3uaVVri6fv26evTooW7dumncuHHq16+f/v7770zzOnToIA8Pj3Q3fpek2bNn68qVK3r66aeztYZb9e/fX8uWLdMHH3ygcuXKqWHDhne1v9vxyNW9AwAAAAAA3KJGjRrq0aOH3n33XZv2l156SQ8++KDGjh2rbt266bvvvtN7772nmTNn2r3vp59+WhMmTFDnzp01ceJEFS9eXPv371eJEiXUqFEjvfzyywoLC7PeePyLL77QqlWr9M0332RrDWXLllXjxo3Vr18/JScnq1OnTtZtjzzyiBo1aqTOnTtr8uTJqlSpkk6ePKn169erc+fOCgkJ0ciRI3XhwgXNmDFD+fLl01dffaV+/frpyy+/zDAvMDBQkydP1ssvvywfHx/16tVLnp6e+vzzz/Xaa6/ppZdeUoMGDbK1hlu1adNGBQoU0Lhx4xQdHX1X+7IHZ0oBAAAAAADTjR07Nt2leXXr1tWnn36qZcuWqXr16ho1apTGjBmj8PBwu/fr5eWljRs3KiAgQO3atVONGjU0adIk65lWnTt31jvvvKM333xT1apV0wcffKB58+apefPm2V5Djx499OOPP6pr1642lw1aLBatX79eTZs2VUREhCpWrKinnnpKx48fV9GiRbVlyxZNnz5dixYtkp+fn9zc3LRo0SLt2LFDs2bNyjRvyJAhWr16tbZv366QkBBVr15dS5Ys0axZs9LdqP1OuLm5KTw8XCkpKerVq9dd7+92OFMKAAAAAID73ME+ByXduF/QxYsXrYWOe8X8+fPTtZUuXTrDJ8E9/vjjevzxxzPd1/HjxyXZ3hvpwIED6fad9pS7jDz77LN69tlnM92e0X2szp8/bz2+aSIjIxUZGZnhPvLnz68ZM2ZoxowZ6bYFBgYqKSnJpi0oKEjnz5/PdE5pOnbsqI4dO2bZp3nz5unWULt2bZu2qKgoRUVFpRsbHx+vdu3aqXjx4jZrzQ0UpQAAAAAAAFzchQsXtGfPHi1evFiff/65KZkUpQAAAAAAAFxcp06d9N///lcDBgxQaGhopk/py0kUpQAAAAAAAFzcli1bTM+8dy4wBQAAAAAAgMugKAUAAAAAwH0moxtxA2bKic8gRSkAAAAAAO4T7u7ukqTr1687eCZwdVevXpUkeXp63vE+uKcUAAAAAAD3CQ8PD/n6+uqff/6Rp6en3NxszzVJTU3V9evXde3atXTbcpMjcl0l01G5mWUahqGrV6/q1KlTKliwoLVQeicoSgEAAAAAcJ+wWCwqXry4jh07pj/++CPddsMw9O+//ypPnjyyWCymzcsRua6S6ajc22UWLFhQxYoVu6sMilIAAAAAANxHvLy8VKFChQwv4UtKStK2bdvUtGnTu7qsKrsckesqmY7KzSrT09Pzrs6QSkNRCgAAAACA+4ybm5t8fHzStbu7uys5OVk+Pj6mFk0ckesqmY7KNSOTG50DAAAAAADAdBSlAAAAAAAAYDqKUgAAAAAAADAdRSkAAAAAAACYjqIUAAAAAAAATEdRCgAAAAAAAKajKAUAAAAAAADTUZQCAAAAAACA6ShKAQAAAAAAwHQUpQAAAAAAAGA6ilIAAAAAAAAwHUUpAAAAAAAAmI6iFAAAAAAAAExHUQoAAAAAAACmoygFAAAAAAAA01GUAgAAAAAAgOkoSgEAAAAAAMB0FKUAAAAAAABgOopSAAAAAAAAMB1FKQAAAAAAAJiOohQAAAAAAABMR1EKAAAAAAAApqMoBQAAAAAAANNRlAIAAAAAAIDpKEoBAAAAAADAdBSlAAAAAAAAYDqKUgAAAAAAADAdRSkAAAAAAACYzuFFqW3btqlDhw4qUaKELBaL1qxZk2X/VatWKTQ0VEWKFJGfn58aNWqkDRs2mDNZAAAAAAAA5AiHF6WuXLmiWrVq6b333rOr/7Zt2xQaGqr169dr3759atGihTp06KD9+/fn8kwBAAAAAACQUzwcPYG2bduqbdu2dvefPn26zfsJEybo888/1xdffKE6derk8OwAAAAAAACQGxxelLpbqampunTpkvz9/TPtk5iYqMTEROv7ixcvSpKSkpKUlJRkX5Cbz40xN//T3rF3w83H/EzJelzsPj73cS5rdc5cV8l0VC5rdc5cV8l0VC5rdc5cV8l0VC5rdb5MR+WyVufMdZVMR+XeTaa9YyyGYRjZ3nsusVgsWr16tTp37mz3mDfffFOTJk1SbGysAgICMuwTFRWl6OjodO1LliyRr6/vnU4XAAAAAAAAt7h69aq6d++uCxcuyM/PL9N+93VRaunSperfv78+//xzPfLII5n2y+hMqcDAQJ0+fTrLg2NjYilJN85WiqkxQ6EHB8nz1SP2jb0bE0uZn6kbVc2YmBiFhobK09PTlExH5bJW58x1lUxH5bJW58x1lUxH5bJW58x1lUxH5bJW58t0VC5rdc5cV8l0VO7dZF68eFGFCxe+bVHqvr18b/ny5erXr59WrFiRZUFKkry9veXt7Z2u3dPT0/4Dm3rNdmzqNXM+CDflmpZ5k2wdo/s8l7U6Z66rZDoql7U6Z66rZDoql7U6Z66rZDoql7U6X6ajclmrc+a6Sqajcu8k097+Dn/63p1YunSpwsPDtWTJErVv397R0wEAAAAAAEA2OfxMqcuXL+vIkf+7JO3YsWM6cOCA/P39FRQUpBEjRuivv/7SwoULJd0oSPXu3VvvvPOOGjZsqISEBElSnjx5VKBAAYesAQAAAAAAANnj8DOl9u7dqzp16qhOnTqSpKFDh6pOnToaNWqUJCk+Pl5xcXHW/h988IGSk5P13HPPqXjx4tbX4MGDHTJ/AAAAAAAAZJ/Dz5Rq3ry5srrX+vz5823eb9myJXcnBAAAAAAAgFzn8DOlAAAAAAAA4HooSgEAAAAAAMB0FKUAAAAAAABgOopSAAAAAAAAMB1FKQAAAAAAAJiOohQAAAAAAABMR1EKAAAAAAAApqMoBQAAAAAAANNRlAIAAAAAAIDpKEoBAAAAAADAdBSlAAAAAAAAYDqKUgAAAAAAADAdRSkAAAAAAACYjqIUAAAAAAAATEdRCgAAAAAAAKajKAUAAAAAAADTUZQCAAAAAACA6ShKAQAAAAAAwHQUpQAAAAAAAGA6ilIAAAAAAAAwHUUpAAAAAAAAmI6iFAAAAAAAAExHUQoAAAAAAACmoygFAAAAAAAA01GUAgAAAAAAgOkoSgEAAAAAAMB0FKUAAAAAAABgOopSAAAAAAAAMB1FKQAAAAAAAJiOohQAAAAAAABMR1EKAAAAAAAApqMoBQAAAAAAANNRlAIAAAAAAIDpKEoBAAAAAADAdBSlAAAAAAAAYDqKUgAAAAAAADAdRSkAAAAAAACYjqIUAAAAAAAATEdRCgAAAAAAAKajKAUAAAAAAADTUZQCAAAAAACA6ShKAQAAAAAAwHQUpQAAAAAAAGA6ilIAAAAAAAAwHUUpAAAAAAAAmI6iFAAAAAAAAExHUQoAAAAAAACmoygFAAAAAAAA01GUAgAAAAAAgOkoSgEAAAAAAMB0FKUAAAAAAABgOocXpbZt26YOHTqoRIkSslgsWrNmzW3HbN26VfXq1ZOPj4+Cg4M1e/bs3J8oAAAAAAAAcozDi1JXrlxRrVq19N5779nV/9ixY2rXrp0efvhh7d+/X6+99poGDRqklStX5vJMAQAAAAAAkFM87Om0bdu2O9p5mTJlFBQUlGWftm3bqm3btnbvc/bs2QoKCtL06dMlSVWqVNHevXv11ltv6fHHH7+jeQIAAAAAAMBcdhWl+vTpk+0dWywWDRkyRIMGDcr22Kx89913at26tU1bmzZtNGfOHCUlJcnT0zNH8wAAAAAAAJDz7CpKHTt2LLfnYbeEhAQVLVrUpq1o0aJKTk7W6dOnVbx48XRjEhMTlZiYaH1/8eJFSVJSUpKSkpLsC3bzuTHm5n/aO/ZuuPmYnylZj4vdx+c+zmWtzpnrKpmOymWtzpnrKpmOymWtzpnrKpmOymWtzpfpqFzW6py5rpLpqNy7ybR3jMUwDCPbe88lFotFq1evVufOnTPtU7FiRfXt21cjRoywtu3cuVMPPfSQ4uPjVaxYsXRjoqKiFB0dna59yZIl8vX1zZG5AwAAAAAAQLp69aq6d++uCxcuyM/PL9N+dp0pdavff/9d8+bN0++//6533nlHAQEB+vrrrxUYGKhq1ard8aTtUaxYMSUkJNi0nTp1Sh4eHipUqFCGY0aMGKGhQ4da31+8eFGBgYFq3bp1lgfHxsRSkm6crRRTY4ZCDw6S56tH7mwR2TGxlPmZulHVjImJUWhoqKmXRDoil7U6Z66rZDoql7U6Z66rZDoql7U6Z66rZDoql7U6X6ajclmrc+a6Sqajcu8mM+0KtdvJdlFq69atatu2rZo0aaJt27Zp/PjxCggI0E8//aSPP/5Yn332WXZ3mS2NGjXSF198YdO2ceNGhYSEZHqQvL295e3tna7d09PT/gObes12bOo1cz4IN+WalnmTbB2j+zyXtTpnrqtkOiqXtTpnrqtkOiqXtTpnrqtkOiqXtTpfpqNyWatz5rpKpqNy7yTT3v5u2Z3M8OHDNW7cOMXExMjLy8va3qJFC3333XfZ3Z0uX76sAwcO6MCBA5Ju3L/qwIEDiouLk3TjLKfevXtb+w8cOFB//PGHhg4dqtjYWM2dO1dz5szRsGHDsp0NAAAAAAAAx8h2UergwYPq0qVLuvYiRYrozJkz2Z7A3r17VadOHdWpU0eSNHToUNWpU0ejRo2SJMXHx1sLVJJUtmxZrV+/Xlu2bFHt2rU1duxYzZgxQ48//ni2swEAAAAAAOAY2b58r2DBgoqPj1fZsmVt2vfv36+SJUtmewLNmzdXVvdanz9/frq2Zs2a6Ycffsh2FgAAAAAAAO4N2T5Tqnv37nr11VeVkJAgi8Wi1NRU7dy5U8OGDbO5zA4AAAAAAADITLaLUuPHj1dQUJBKliypy5cvq2rVqmratKkaN26s119/PTfmCAAAAAAAACeT7cv3PD09tXjxYo0ZM0b79+9Xamqq6tSpowoVKuTG/AAAAAAAAOCEsl2USlOuXDmVK1cuJ+cCAAAAAAAAF5HtolRERESW2+fOnXvHkwEAAAAAAIBryHZR6ty5czbvk5KS9PPPP+v8+fNq2bJljk0MAAAAAAAAzivbRanVq1ena0tNTVVkZKSCg4NzZFIAAAAAAABwbtl++l6GO3Fz04svvqi33347J3YHAAAAAAAAJ5cjRSlJ+v3335WcnJxTuwMAAAAAAIATy/ble0OHDrV5bxiG4uPjtW7dOvXp0yfHJgYAAAAAAADnle2i1P79+23eu7m5qUiRIpo6deptn8wHAAAAAAAASHdQlNq8eXNuzAMAAAAAAAAuJMfuKQUAAAAAAADYy64zperUqSOLxWLXDn/44Ye7mhAAAAAAAACcn11Fqc6dO+fyNAAAAAAAAOBK7CpKjR49OrfnAQAAAAAAABfCPaUAAAAAAABgumw/fS8lJUVvv/22Pv30U8XFxen69es228+ePZtjkwMAAAAAAIBzyvaZUtHR0Zo2bZrCwsJ04cIFDR06VF27dpWbm5uioqJyYYoAAAAAAABwNtkuSi1evFgfffSRhg0bJg8PDz399NP6+OOPNWrUKO3evTs35ggAAAAAAAAnk+2iVEJCgmrUqCFJypcvny5cuCBJeuyxx7Ru3bqcnR0AAAAAAACcUraLUqVKlVJ8fLwkqXz58tq4caMkac+ePfL29s7Z2QEAAAAAAMApZbso1aVLF23atEmSNHjwYL3xxhuqUKGCevfurYiIiByfIAAAAAAAAJxPtp++N2nSJOufn3jiCQUGBmrnzp0qX768OnbsmKOTAwAAAAAAgHPKdlHq6tWr8vX1tb5v0KCBGjRokKOTAgAAAAAAgHPL9uV7AQEB6tmzpzZs2KDU1NTcmBMAAAAAAACcXLaLUgsXLlRiYqK6dOmiEiVKaPDgwdqzZ09uzA0AAAAAAABOKttFqa5du2rFihX6+++/NXHiRMXGxqpx48aqWLGixowZkxtzBAAAAAAAgJPJdlEqTf78+dW3b19t3LhRP/74o/Lmzavo6OicnBsAAAAAAACc1B0Xpa5du6ZPP/1UnTt3Vt26dXXmzBkNGzYsJ+cGAAAAAAAAJ5Xtp+9t3LhRixcv1po1a+Tu7q4nnnhCGzZsULNmzXJjfgAAAAAAAHBC2S5Kde7cWe3bt9eCBQvUvn17eXp65sa8AAAAAAAA4MSyXZRKSEiQn59fbswFAAAAAAAALiLb95SiIAUAAAAAAIC7dcc3OgcAAAAAAADuFEUpAAAAAAAAmI6iFAAAAAAAAEyXrRud//HHH9q4caOSkpLUrFkzVatWLbfmBQAAAAAAACdmd1Fq27Ztateuna5evXpjoIeHFixYoKeffjrXJgcAAAAAAADnZPfle2+88YZatGihEydO6MyZM4qIiNArr7ySm3MDAAAAAACAk7K7KHXw4EFNnDhRJUqU0AMPPKCpU6fq5MmTOnfuXG7ODwAAAAAAAE7I7qLU+fPnFRAQYH2fN29e+fr66vz587kxLwAAAAAAADixbN3o/NChQ0pISLC+NwxDsbGxunTpkrWtZs2aOTc7AAAAAAAAOKVsFaVatWolwzBs2h577DFZLBYZhiGLxaKUlJQcnSAAAAAAAACcj91FqWPHjuXmPAAAAAAAAOBC7C5KlS5d+rZ9Dhw4YFc/AAAAAAAAuDa7b3SemQsXLmjmzJmqW7eu6tWrlxNzAgAAAAAAgJO746LUt99+q549e6p48eJ699131a5dO+3duzcn5wYAAAAAAAAnla0bnZ84cULz58/X3LlzdeXKFYWFhSkpKUkrV65U1apVc2uOAAAAAAAAcDJ2nynVrl07Va1aVYcOHdK7776rkydP6t13383NuQEAAAAAAMBJ2X2m1MaNGzVo0CA9++yzqlChQm7OCQAAAAAAAE7O7jOltm/frkuXLikkJEQNGjTQe++9p3/++Sc35wYAAAAAAAAnZXdRqlGjRvroo48UHx+vAQMGaNmyZSpZsqRSU1MVExOjS5cu3fEkZs6cqbJly8rHx0f16tXT9u3bs+y/ePFi1apVS76+vipevLj69u2rM2fO3HE+AAAAAAAAzJXtp+/5+voqIiJCO3bs0MGDB/XSSy9p0qRJCggIUMeOHbM9geXLl2vIkCEaOXKk9u/fr4cfflht27ZVXFxchv137Nih3r17q1+/fvrf//6nFStWaM+ePerfv3+2swEAAAAAAOAY2S5K3axSpUqaMmWKTpw4oaVLl97RPqZNm6Z+/fqpf//+qlKliqZPn67AwEDNmjUrw/67d+9WmTJlNGjQIJUtW1YPPfSQBgwYoL17997NUgAAAAAAAGCiuypKpXF3d1fnzp21du3abI27fv269u3bp9atW9u0t27dWrt27cpwTOPGjXXixAmtX79ehmHo77//1meffab27dvf8fwBAAAAAABgLrufvte1a9fb9rFYLFq5cqXd4adPn1ZKSoqKFi1q0160aFElJCRkOKZx48ZavHixunXrpmvXrik5OVkdO3bUu+++m2lOYmKiEhMTre8vXrwoSUpKSlJSUpJ9k3XzuTHm5n/aO/ZuuPmYnylZj4vdx+c+zmWtzpnrKpmOymWtzpnrKpmOymWtzpnrKpmOymWtzpfpqFzW6py5rpLpqNy7ybR3jMUwDMOejn379rV5v2TJEnXo0EH58+e3aZ83b56dU5ROnjypkiVLateuXWrUqJG1ffz48Vq0aJF++eWXdGMOHTqkRx55RC+++KLatGmj+Ph4vfzyy3rwwQc1Z86cDHOioqIUHR2drn3JkiXy9fW1e74AAAAAAADI2tWrV9W9e3dduHBBfn5+mfazuyh1q/z58+vHH39UcHDwHU/y+vXr8vX11YoVK9SlSxdr++DBg3XgwAFt3bo13ZhevXrp2rVrWrFihbVtx44devjhh3Xy5EkVL1483ZiMzpQKDAzU6dOnszw4NiaWknTjbKWYGjMUenCQPF89Yu9S79zEUuZn6kZVMyYmRqGhofL09DQl01G5rNU5c10l01G5rNU5c10l01G5rNU5c10l01G5rNX5Mh2Vy1qdM9dVMh2VezeZFy9eVOHChW9blLL78r3c4OXlpXr16ikmJsamKBUTE6NOnTplOObq1avy8LCdtru7uyQps/qat7e3vL2907V7enraf2BTr9mOTb1mzgfhplzTMm+SrWN0n+eyVufMdZVMR+WyVufMdZVMR+WyVufMdZVMR+WyVufLdFQua3XOXFfJdFTunWTa2z9HbnR+N4YOHaqPP/5Yc+fOVWxsrF588UXFxcVp4MCBkqQRI0aod+/e1v4dOnTQqlWrNGvWLB09elQ7d+7UoEGDVL9+fZUoUcJRywAAAAAAAEA2OPRMKUnq1q2bzpw5ozFjxig+Pl7Vq1fX+vXrVbp0aUlSfHy84uLirP3Dw8N16dIlvffee3rppZdUsGBBtWzZUpMnT3bUEgAAAAAAAJBNdhel1q5da/M+NTVVmzZt0s8//2zT3rFjx2xPIjIyUpGRkRlumz9/frq2F154QS+88EK2cwAAAAAAAHBvsLso1blz53RtAwYMsHlvsViUkpJy15MCAAAAAACAc7O7KJWampqb8wAAAAAAAIALcfiNzgEAAAAAAOB67C5KHTlyRPv27bNp27Rpk1q0aKH69etrwoQJOT45AAAAAAAAOCe7i1Ivv/yy1qxZY31/7NgxdejQQV5eXmrUqJEmTpyo6dOn58IUAQAAAAAA4GzsvqfU3r179corr1jfL168WBUrVtSGDRskSTVr1tS7776rIUOG5PgkAQAAAAAA4FzsPlPq9OnTKlWqlPX95s2b1aFDB+v75s2b6/jx4zk6OQAAAAAAADgnu4tS/v7+io+Pl3TjSXx79+5VgwYNrNuvX78uwzByfoYAAAAAAABwOnYXpZo1a6axY8fqzz//1PTp05WamqoWLVpYtx86dEhlypTJjTkCAAAAAADAydh9T6nx48crNDRUZcqUkZubm2bMmKG8efNaty9atEgtW7bMlUkCAAAAAADAudhdlCpbtqxiY2N16NAhFSlSRCVKlLDZHh0dbXPPKQAAAAAAACAzdhelJMnT01O1atXKcFtm7QAAAAAAAMCt7CpKxcXF3dHOCxYsKD8/vzsaCwAAAAAAAOdlV1GqTJkyslgs2Xq6nsVi0ejRozVq1Kg7nhwAAAAAAACck11FqdTU1NyeBwAAAAAAAFyIm6MnAAAAAAAAANdDUQoAAAAAAACmoygFAAAAAAAA01GUAgAAAAAAgOkoSgEAAAAAAMB0FKUAAAAAAABgOo/sDkhJSdH8+fO1adMmnTp1SqmpqTbbv/322xybHAAAAAAAAJxTtotSgwcP1vz589W+fXtVr15dFoslN+YFAAAAAAAAJ5btotSyZcv06aefql27drkxHwAAAAAAALiAbN9TysvLS+XLl8+NuQAAAAAAAMBFZLso9dJLL+mdd96RYRi5MR8AAAAAAAC4gGxfvrdjxw5t3rxZX331lapVqyZPT0+b7atWrcqxyQEAAAAAAMA5ZbsoVbBgQXXp0iU35gIAAAAAAAAXke2i1Lx583JjHgAAAAAAAHAh2b6nFAAAAAAAAHC3sn2mlCR99tln+vTTTxUXF6fr16/bbPvhhx9yZGIAAAAAAABwXtk+U2rGjBnq27evAgICtH//ftWvX1+FChXS0aNH1bZt29yYIwAAAAAAAJxMtotSM2fO1Icffqj33ntPXl5eeuWVVxQTE6NBgwbpwoULuTFHAAAAAAAAOJlsF6Xi4uLUuHFjSVKePHl06dIlSVKvXr20dOnSnJ0dAAAAAAAAnFK2i1LFihXTmTNnJEmlS5fW7t27JUnHjh2TYRg5OzsAAAAAAAA4pWwXpVq2bKkvvvhCktSvXz+9+OKLCg0NVbdu3dSlS5ccnyAAAAAAAACcT7afvvfhhx8qNTVVkjRw4ED5+/trx44d6tChgwYOHJjjEwQAAAAAAIDzyXZRys3NTW5u/3eCVVhYmMLCwnJ0UgAAAAAAAHBu2b58T5K2b9+unj17qlGjRvrrr78kSYsWLdKOHTtydHIAAAAAAABwTtkuSq1cuVJt2rRRnjx5tH//fiUmJkqSLl26pAkTJuT4BAEAAAAAAOB8sl2UGjdunGbPnq2PPvpInp6e1vbGjRvrhx9+yNHJAQAAAAAAwDlluyh1+PBhNW3aNF27n5+fzp8/nxNzAgAAAAAAgJPLdlGqePHiOnLkSLr2HTt2KDg4OEcmBQAAAAAAAOeW7aLUgAEDNHjwYH3//feyWCw6efKkFi9erGHDhikyMjI35ggAAAAAAAAn45HdAa+88oouXLigFi1a6Nq1a2ratKm8vb01bNgwPf/887kxRwAAAAAAADiZbBelJGn8+PEaOXKkDh06pNTUVFWtWlX58uXL6bkBAAAAAADASd1RUUqSfH19FRISkpNzAQAAAAAAgIuwuygVERFhV7+5c+fe8WQAAAAAAADgGuwuSs2fP1+lS5dWnTp1ZBhGbs4JAAAAAAAATs7uotTAgQO1bNkyHT16VBEREerZs6f8/f1zc24AAAAAAABwUm72dpw5c6bi4+P16quv6osvvlBgYKDCwsK0YcMGzpwCAAAAAABAtthdlJIkb29vPf3004qJidGhQ4dUrVo1RUZGqnTp0rp8+XJuzREAAAAAAABOJltFqZtZLBZZLBYZhqHU1NS7msTMmTNVtmxZ+fj4qF69etq+fXuW/RMTEzVy5EiVLl1a3t7eKleuHDdYBwAAAAAAuI9kqyiVmJiopUuXKjQ0VJUqVdLBgwf13nvvKS4uTvny5bujCSxfvlxDhgzRyJEjtX//fj388MNq27at4uLiMh0TFhamTZs2ac6cOTp8+LCWLl2qypUr31E+AAAAAAAAzGf3jc4jIyO1bNkyBQUFqW/fvlq2bJkKFSp01xOYNm2a+vXrp/79+0uSpk+frg0bNmjWrFmaOHFiuv5ff/21tm7dqqNHj1pvtF6mTJm7ngcAAAAAAADMY3dRavbs2QoKClLZsmW1detWbd26NcN+q1atsjv8+vXr2rdvn4YPH27T3rp1a+3atSvDMWvXrlVISIimTJmiRYsWKW/evOrYsaPGjh2rPHnyZDgmMTFRiYmJ1vcXL16UJCUlJSkpKcm+ybr53Bhz8z/tHXs33HzMz5Ssx8Xu43Mf57JW58x1lUxH5bJW58x1lUxH5bJW58x1lUxH5bJW58t0VC5rdc5cV8l0VO7dZNo7xmLY+ei88PBwWSyW2/abN2+eXcGSdPLkSZUsWVI7d+5U48aNre0TJkzQggULdPjw4XRjHn30UW3ZskWPPPKIRo0apdOnTysyMlItW7bM9L5SUVFRio6OTte+ZMkS+fr62j1fAAAAAAAAZO3q1avq3r27Lly4ID8/v0z72X2m1Pz583NiXhm6tdhlGEamBbDU1FRZLBYtXrxYBQoUkHTjEsAnnnhC77//foZnS40YMUJDhw61vr948aICAwPVunXrLA+OjYmlJN04WymmxgyFHhwkz1eP2Df2bkwsZX6mblQ1Y2JiFBoaKk9PT1MyHZXLWp0z11UyHZXLWp0z11UyHZXLWp0z11UyHZXLWp0v01G5rNU5c10l01G5d5OZdoXa7dhdlMoNhQsXlru7uxISEmzaT506paJFi2Y4pnjx4ipZsqS1ICVJVapUkWEYOnHihCpUqJBujLe3t7y9vdO1e3p62n9gU6/Zjk29Zs4H4aZc0zJvkq1jdJ/nslbnzHWVTEflslbnzHWVTEflslbnzHWVTEflslbny3RULmt1zlxXyXRU7p1k2ts/W0/fy2leXl6qV6+eYmJibNpjYmJsLue7WZMmTXTy5EldvnzZ2vbrr7/Kzc1NpUqVytX5AgAAAAAAIGc4tCglSUOHDtXHH3+suXPnKjY2Vi+++KLi4uI0cOBASTcuvevdu7e1f/fu3VWoUCH17dtXhw4d0rZt2/Tyyy8rIiIi0xudAwAAAAAA4N7i0Mv3JKlbt246c+aMxowZo/j4eFWvXl3r169X6dKlJUnx8fGKi4uz9s+XL59iYmL0wgsvKCQkRIUKFVJYWJjGjRvnqCUAAAAAAAAgmxxelJKkyMhIRUZGZrgtoxusV65cOd0lfwAAAAAAALh/OPzyPQAAAAAAALgeilIAAAAAAAAwHUUpAAAAAAAAmI6iFAAAAAAAAExHUQoAAAAAAACmoygFAAAAAAAA01GUAgAAAAAAgOkoSgEAAAAAAMB0FKUAAAAAAABgOopSAAAAAAAAMB1FKQAAAAAAAJiOohQAAAAAAABMR1EKAAAAAAAApqMoBQAAAAAAANNRlAIAAAAAAIDpKEoBAAAAAADAdBSlAAAAAAAAYDqKUgAAAAAAADAdRSkAAAAAAACYjqIUAAAAAAAATEdRCgAAAAAAAKajKAUAAAAAAADTUZQCAAAAAACA6ShKAQAAAAAAwHQUpQAAAAAAAGA6ilIAAAAAAAAwHUUpAAAAAAAAmI6iFAAAAAAAAExHUQoAAAAAAACmoygFAAAAAAAA01GUAgAAAAAAgOkoSgEAAAAAAMB0FKUAAAAAAABgOopSAAAAAAAAMB1FKQAAAAAAAJiOohQAAAAAAABMR1EKAAAAAAAApqMoBQAAAAAAANNRlAIAAAAAAIDpKEoBAAAAAADAdBSlAAAAAAAAYDqKUgAAAAAAADAdRSkAAAAAAACYjqIUAAAAAAAATEdRCgAAAAAAAKajKAUAAAAAAADTUZQCAAAAAACA6ShKAQAAAAAAwHQUpQAAAAAAAGA6ilIAAAAAAAAwHUUpAAAAAAAAmI6iFAAAAAAAAEx3TxSlZs6cqbJly8rHx0f16tXT9u3b7Rq3c+dOeXh4qHbt2rk7QQAAAAAAAOQohxelli9friFDhmjkyJHav3+/Hn74YbVt21ZxcXFZjrtw4YJ69+6tVq1amTRTAAAAAAAA5BSHF6WmTZumfv36qX///qpSpYqmT5+uwMBAzZo1K8txAwYMUPfu3dWoUSOTZgoAAAAAAICc4uHI8OvXr2vfvn0aPny4TXvr1q21a9euTMfNmzdPv//+uz755BONGzfutjmJiYlKTEy0vr948aIkKSkpSUlJSfZN1s3nxpib/2nv2Lvh5mN+pmQ9LnYfn/s4l7U6Z66rZDoql7U6Z66rZDoql7U6Z66rZDoql7U6X6ajclmrc+a6Sqajcu8m094xFsMwjGzvPYecPHlSJUuW1M6dO9W4cWNr+4QJE7RgwQIdPnw43ZjffvtNDz30kLZv366KFSsqKipKa9as0YEDBzLNiYqKUnR0dLr2JUuWyNfXN0fWAgAAAAAAAOnq1avq3r27Lly4ID8/v0z7OfRMqTQWi8XmvWEY6dokKSUlRd27d1d0dLQqVqxo9/5HjBihoUOHWt9fvHhRgYGBat26dZYHx8bEUpJunK0UU2OGQg8OkuerR+yewx2bWMr8TN2oasbExCg0NFSenp6mZDoql7U6Z66rZDoql7U6Z66rZDoql7U6Z66rZDoql7U6X6ajclmrc+a6Sqajcu8mM+0KtdtxaFGqcOHCcnd3V0JCgk37qVOnVLRo0XT9L126pL1792r//v16/vnnJUmpqakyDEMeHh7auHGjWrZsmW6ct7e3vL2907V7enraf2BTr9mOTb1mzgfhplzTMm+SrWN0n+eyVufMdZVMR+WyVufMdZVMR+WyVufMdZVMR+WyVufLdFQua3XOXFfJdFTunWTa29+hNzr38vJSvXr1FBMTY9MeExNjczlfGj8/Px08eFAHDhywvgYOHKhKlSrpwIEDatCggVlTBwAAAAAAwF1w+OV7Q4cOVa9evRQSEqJGjRrpww8/VFxcnAYOHCjpxqV3f/31lxYuXCg3NzdVr17dZnxAQIB8fHzStQMAAAAAAODe5fCiVLdu3XTmzBmNGTNG8fHxql69utavX6/SpUtLkuLj4xUXF+fgWQIAAAAAACAnObwoJUmRkZGKjIzMcNv8+fOzHBsVFaWoqKicnxQAAAAAAAByjUPvKQUAAAAAAADXRFEKAAAAAAAApqMoBQAAAAAAANNRlAIAAAAAAIDpKEoBAAAAAADAdBSlAAAAAAAAYDqKUgAAAAAAADAdRSkAAAAAAACYjqIUAAAAAAAATEdRCgAAAAAAAKajKAUAAAAAAADTUZQCAAAAAACA6ShKAQAAAAAAwHQUpQAAAAAAAGA6ilIAAAAAAAAwHUUpAAAAAAAAmI6iFAAAAAAAAExHUQoAAAAAAACmoygFAAAAAAAA01GUAgAAAAAAgOkoSgEAAAAAAMB0FKUAAAAAAABgOopSAAAAAAAAMB1FKQAAAAAAAJiOohQAAAAAAABMR1EKAAAAAAAApqMoBQAAAAAAANNRlAIAAAAAAIDpKEoBAAAAAADAdBSlAAAAAAAAYDqKUgAAAAAAADAdRSkAAAAAAACYjqIUAAAAAAAATEdRCgAAAAAAAKajKAUAAAAAAADTUZQCAAAAAACA6ShKAQAAAAAAwHQejp7AvajM8HXp2o77OGAiAAAAAAAAToozpQAAAAAAAGA6ilIAAAAAAAAwHUUpAAAAAAAAmI6iFAAAAAAAAExHUQoAAAAAAACmoygFAAAAAAAA01GUAgAAAAAAgOkoSgEAAAAAAMB0FKUAAAAAAABgOopSAAAAAAAAMB1FKQAAAAAAAJiOohQAAAAAAABMR1EKAAAAAAAAprsnilIzZ85U2bJl5ePjo3r16mn79u2Z9l21apVCQ0NVpEgR+fn5qVGjRtqwYYOJswUAAAAAAMDdcnhRavny5RoyZIhGjhyp/fv36+GHH1bbtm0VFxeXYf9t27YpNDRU69ev1759+9SiRQt16NBB+/fvN3nmAAAAAAAAuFMOL0pNmzZN/fr1U//+/VWlShVNnz5dgYGBmjVrVob9p0+frldeeUUPPvigKlSooAkTJqhChQr64osvTJ45AAAAAAAA7pRDi1LXr1/Xvn371Lp1a5v21q1ba9euXXbtIzU1VZcuXZK/v39uTBEAAAAAAAC5wMOR4adPn1ZKSoqKFi1q0160aFElJCTYtY+pU6fqypUrCgsLy7RPYmKiEhMTre8vXrwoSUpKSlJSUlK6/t7uRrq2JDef9P/MYGyOc/MxP1OyHpeMjo+z5bJW58x1lUxH5bJW58x1lUxH5bJW58x1lUxH5bJW58t0VC5rdc5cV8l0VO7dZNo7xmIYRvoKjElOnjypkiVLateuXWrUqJG1ffz48Vq0aJF++eWXLMcvXbpU/fv31+eff65HHnkk035RUVGKjo5O175kyRL5+vre+QIAAAAAAABg4+rVq+revbsuXLggPz+/TPs59EypwoULy93dPd1ZUadOnUp39tStli9frn79+mnFihVZFqQkacSIERo6dKj1/cWLFxUYGKjWrVtneHCqR6V/mt/P3v0k3ThbKabGDIUeHCTPV49kmZsjJpYyP1M3qpoxMTEKDQ2Vp6enKZmOymWtzpnrKpmOymWtzpnrKpmOymWtzpnrKpmOymWtzpfpqFzW6py5rpLpqNy7yUy7Qu12HFqU8vLyUr169RQTE6MuXbpY22NiYtSpU6dMxy1dulQRERFaunSp2rdvf9scb29veXt7p2v39PTM8MAmpljS9029lu59Tn8Qygxfl67tuM//5eZG5u1kdoycMZe1Omeuq2Q6Kpe1Omeuq2Q6Kpe1Omeuq2Q6Kpe1Ol+mo3JZq3Pmukqmo3LvJNPe/g4tSknS0KFD1atXL4WEhKhRo0b68MMPFRcXp4EDB0q6cZbTX3/9pYULF0q6UZDq3bu33nnnHTVs2NB6llWePHlUoEABh60DAAAAAAAA9nN4Uapbt246c+aMxowZo/j4eFWvXl3r169X6dKlJUnx8fGKi4uz9v/ggw+UnJys5557Ts8995y1vU+fPpo/f77Z0wcAAAAAAMAdcHhRSpIiIyMVGRmZ4bZbC01btmzJ/QkBAAAAAAAgV7k5egIAAAAAAABwPRSlAAAAAAAAYDqKUgAAAAAAADAdRSkAAAAAAACYjqIUAAAAAAAATEdRCgAAAAAAAKajKAUAAAAAAADTUZQCAAAAAACA6ShKAQAAAAAAwHQUpQAAAAAAAGA6ilIAAAAAAAAwHUUpAAAAAAAAmI6iFAAAAAAAAExHUQoAAAAAAACmoygFAAAAAAAA01GUAgAAAAAAgOkoSgEAAAAAAMB0FKUAAAAAAABgOopSAAAAAAAAMB1FKQAAAAAAAJiOohQAAAAAAABMR1EKAAAAAAAApqMoBQAAAAAAANNRlAIAAAAAAIDpKEoBAAAAAADAdBSlAAAAAAAAYDqKUgAAAAAAADAdRSkAAAAAAACYjqIUAAAAAAAATEdRCgAAAAAAAKajKAUAAAAAAADTUZQCAAAAAACA6ShKAQAAAAAAwHQUpQAAAAAAAGA6ilIAAAAAAAAwHUUpAAAAAAAAmM7D0RO4n9VYUCNd28E+Bx0wEwAAAAAAgPsLZ0oBAAAAAADAdJwpdZ/h7CwAAAAAAOAMKEq5uDLD16VrOz6pvQNmAgAAAAAAXAmX7wEAAAAAAMB0FKUAAAAAAABgOopSAAAAAAAAMB1FKQAAAAAAAJiOohTuDVEFbrwmlrrxPu2fAAAAAADAKVGUAgAAAAAAgOkoSgEAAAAAAMB0FKUAAAAAAABgOopSAAAAAAAAMB1FKaR36w3Howo4dj4AAAAAAMDpUJQCAAAAAACA6TwcPQHALGWGr0vXdnxSewfMBAAAAAAA3BNnSs2cOVNly5aVj4+P6tWrp+3bt2fZf+vWrapXr558fHwUHBys2bNnmzRTAAAAAAAA5ASHnym1fPlyDRkyRDNnzlSTJk30wQcfqG3btjp06JCCgoLS9T927JjatWunZ555Rp988ol27typyMhIFSlSRI8//rgDVoA7cetZS8d9HDQRAAAAAADgEA4/U2ratGnq16+f+vfvrypVqmj69OkKDAzUrFmzMuw/e/ZsBQUFafr06apSpYr69++viIgIvfXWWybPHAAAAAAAAHfKoWdKXb9+Xfv27dPw4cNt2lu3bq1du3ZlOOa7775T69atbdratGmjOXPmKCkpSZ6enrk2X1dWY0GNdG0H+xx0wEzuLxnex8qn+40/uPlItT688YTDUX/nfm4u3z/LUffsumfuFRZVwPbvNPWaFHXB/HkAAAAAwH3CoUWp06dPKyUlRUWLFrVpL1q0qBISEjIck5CQkGH/5ORknT59WsWLF083JjExUYmJidb3Fy7c+EXx7NmzSkpKStffI/lKurYz170kSUluXrp69arOXPeSx7/pD9+ZM2cynLc9Mst1dKZnamqu52Z0fNvOrpNu3DdPfpNjmdKNNSQlJd3IPHMmx4ua9n6WPO/iWNqd+1oRJbn56Gq1KToTHSzP1GvSS7/keqYk29wXD+RYZla5uZmZoQy+N8rBv9cGEzela/t+RCtJsvkMt13TNl2/u/neZJabm9+bDE2tLCl3P0uZ5ebm9yYrph9jF8p0VC5rdc5cV8l0VC5rdb5MR+WyVufMdZVMR+XeTealS5ckSYZhZN3RcKC//vrLkGTs2rXLpn3cuHFGpUqVMhxToUIFY8KECTZtO3bsMCQZ8fHxGY4ZPXq0IYkXL168ePHixYsXL168ePHixYuXSa8///wzy7qQQ8+UKly4sNzd3dOdFXXq1Kl0Z0OlKVasWIb9PTw8VKhQoQzHjBgxQkOHDrW+T01N1dmzZ1WoUCFZLJZszfnixYsKDAzUn3/+KT8/v2yNvVOukumoXNbqnLmukumoXNbqnLmukumoXNbqnLmukumoXNbqfJmOymWtzpnrKpmOyr2bTMMwdOnSJZUoUSLLfg4tSnl5ealevXqKiYlRly5drO0xMTHq1KlThmMaNWqkL774wqZt48aNCgkJyfR0Mm9vb3l7e9u0FSxY8K7m7ufnZ+oH0JUyHZXLWp0z11UyHZXLWp0z11UyHZXLWp0z11UyHZXLWp0v01G5rNU5c10l01G5d5pZoECB2/Zx+NP3hg4dqo8//lhz585VbGysXnzxRcXFxWngwIGSbpzl1Lt3b2v/gQMH6o8//tDQoUMVGxuruXPnas6cORo2bJijlgAAAAAAAIBscuiZUpLUrVs3nTlzRmPGjFF8fLyqV6+u9evXq3Tp0pKk+Ph4xcXFWfuXLVtW69ev14svvqj3339fJUqU0IwZM/T44487agkAAAAAAADIJocXpSQpMjJSkZGRGW6bP39+urZmzZrphx9+yOVZZczb21ujR49OdzkgmfdvLmt1zlxXyXRULmt1zlxXyXRULmt1zlxXyXRULmt1vkxH5bJW58x1lUxH5ZqRaTGM2z2fDwAAAAAAAMhZDr+nFAAAAAAAAFwPRSkAAAAAAACYjqIUAAAAAADA/2vvzqOiuu/3gT8DAgOouCKMKGIMYlxIRI1b1biASxRrjzGpGxVtjRolJ4lLMZVo3QtJKlXjEnCrS1s0xiUGDbjEDQQVlSoKqEHUrCqgLPL+/ZEzcxgcmAHuvfT39XmdM+eEm+E+97Pf+ZzhSprjppSNHj16hLCwMHh7e8PZ2Rk9e/ZEUlKSqplLly5F165dUa9ePbi7u2PkyJG4evWqohlr1qxBp06dUL9+fdSvXx89evTAwYMHTf8/Li4OQUFBaNKkCXQ6Hc6fP696Zll/+tOfoNPp8Mknn9Q499ixYxg+fDgMBgN0Oh327Nlj9v9DQkKg0+nMXt27d69RprWyRkREwM/PD66urmjYsCEGDhyIM2fOqJZZXFyMOXPmoGPHjnB1dYXBYMCECRNw586dGmUC1uu3fN0aXytXrqxxdllV6V9KadWqlcWyTZ8+XdEca3UMAOnp6RgxYgTc3NxQr149dO/e3exfMK0Ka3WpVZsCQE5ODsaNG4fGjRvDxcUFL7/8Ms6dO6fY+as63yo5N5VVUlKC+fPnw8fHB87OzmjdujUWLlyI0tJSRXPKs6VvqWX16tXw8fGBXq9HQEAAjh8/rti5bZkPlBwzgG19SY31xlobqrHeWMtVa82x1q5q1C9gvY6Vvmey1pfUql9r5czLy8OMGTPg5eUFZ2dntGvXDmvWrKlRpi3jRkQQEREBg8EAZ2dn9OvXD5cvX65RblXmPi3vSe/du4eQkBAYDAa4uLhg8ODByMjIqFGmLXWsdNtaG6tqtKktli5dCp1Oh7CwMEXPaa1+lb5vsiWztvqS1vOhkdJrenkRERHPtJ+Hh4di5wdsK6saa7q1eUnN8cpNKRtNnjwZ8fHx2LJlC9LS0hAYGIiBAwciJydHtcyjR49i+vTpOH36NOLj41FSUoLAwEDk5+crluHl5YVly5YhOTkZycnJ6N+/P4KDg00dLD8/H7169cKyZcs0yzTas2cPzpw5A4PBoEhufn4+/P39ER0dXeF7Bg8ejNzcXNPrwIEDNcq0VlZfX19ER0cjLS0NJ06cQKtWrRAYGIjvv/9elcyCggKkpKTgww8/REpKCuLi4nDt2jWMGDGiRuUErNdv2XrNzc3F559/Dp1Oh9/97nc1zi7L1v6lpKSkJLOyxcfHAwBGjx6taI61Or5x4wZ69+4NPz8/JCYm4sKFC/jwww+h1+urlWetLrVq059//hm9evWCg4MDDh48iCtXriAyMhINGjRQLKMq863Sc1NZy5cvx9q1axEdHY309HSsWLECK1euxKpVqxTPKsuW+VENO3fuRFhYGMLDw5Gamorf/OY3GDJkiGI3kNb6sNJjBrC9Lym93lhrQzXWG2u5aq05tszzStcvYL2Olb5nstaX1Kpfa+V899138dVXX2Hr1q1IT0/Hu+++i3feeQdffPFFtTNtGTcrVqxAVFQUoqOjkZSUBA8PDwwaNAiPHj2qdq6tc5+W96QigpEjRyIzMxNffPEFUlNT4e3tjYEDB9boM4Atdax021obq2q0qTVJSUlYt24dOnXqpOh5balfpe+brGXWZl/Sej4E1FnTLWnfvr1ZO6alpSl6flvKqsaabm0+VHW8CllVUFAg9vb2sm/fPrPj/v7+Eh4ertl13L9/XwDI0aNHVc1p2LChbNiwwexYVlaWAJDU1FRNMr/77jtp3ry5XLp0Sby9veXjjz9WNA+A7N692+zYxIkTJTg4WNEcSyzVr9GDBw8EgBw+fFizzLNnzwoAuXnzpmJ5luq3vODgYOnfv79imZWprPxqmDVrlrzwwgtSWlqqWoalOh4zZoyMGzdOtUyRyutSrTadM2eO9O7dW/HzVqai+VbtuWnYsGEyadIks2OjRo1SvV3LsmX8KqVbt24ydepUs2N+fn4yd+5c1TLL9mEtxoylvqT2emNLG6qx3tiSq8aaI2Lerlqs55WVVa17JlvuA5WuX0vlbN++vSxcuNDsWOfOnWX+/PmKZIo8W9bS0lLx8PCQZcuWmd7z5MkTcXNzk7Vr1yqSWVGban1PevXqVQEgly5dMh0rKSmRRo0ayfr16xXLtdSftGhb41jVok3Le/Tokbz44osSHx8vffv2lVmzZqmSI2LbeFX6vql8Zm32JSMt50Mt1vQFCxaIv7+/qhnl2dKXlF7Ty89Lao9XflPKBiUlJXj69Okzu6zOzs44ceKEZtfx4MEDAECjRo1UOf/Tp0+xY8cO5Ofno0ePHqpk2JJZWlqK8ePH44MPPkD79u01uQ6jxMREuLu7w9fXF1OmTMH9+/cVO7e1+i0qKsK6devg5uYGf39/TTKBX/uVTqdT9Fsn1ty7dw/79+9HaGioqjm10aeLioqwdetWTJo0CTqdTpNM4Ndxs3//fvj6+iIoKAju7u549dVXFfszLGt1qWab7t27F126dMHo0aPh7u6OV155BevXr1c8pyxL860Wc1Pv3r1x5MgRXLt2DQBw4cIFnDhxAkOHDlUlrzYVFRXh3LlzCAwMNDseGBiIkydPKp5Xvg+rPWaMKlq71VxvrFFjvbGV0mtORXNTbdavWmy5D9RiTe/duzf27t2LnJwciAgSEhJw7do1BAUFKZZRvqxZWVm4e/eu2Xzh5OSEvn37qjJfGNXGPWlhYSEAmH3usLe3h6Ojo6KfOyz1JzXbtvxYrY02nT59OoYNG4aBAweqcv6yrI1XNe6bymfWZl9SW/lMrdZ0AMjIyIDBYICPjw/efPNNZGZmKp5RlrX61WJNV3281nhb6znRo0cP6du3r+Tk5EhJSYls2bJFdDqd+Pr6apJfWloqw4cPV+XbAhcvXhRXV1ext7cXNzc32b9//zPvUXqXu7LMJUuWyKBBg0zfMtHqm1I7duyQffv2SVpamuzdu1f8/f2lffv28uTJkxplWavfL7/8UlxdXUWn04nBYJCzZ8/WKM+WTKPHjx9LQECAjB07tsaZZVmq37KWL18uDRs2lMePHyuaa2Rr+dWwc+dOsbe3l5ycHFVzytdxbm6uABAXFxeJioqS1NRUWbp0qeh0OklMTKx2jq11qWabOjk5iZOTk8ybN09SUlJk7dq1otfrZdOmTYpniVQ832oxN5WWlsrcuXNFp9NJnTp1RKfTyZIlSxTNsMba+FVKTk6OAJBvv/3W7PjixYsVXVsr6sNqjZmyKupLaq03RhW1oRrrjS25RkquOZXNTWrXr4j235Sy5T5QjTXdUjkLCwtlwoQJAkDq1Kkjjo6OsnnzZsUyLZX122+/FQDPrK1TpkyRwMBARXItlbU27kmLiorE29tbRo8eLT/99JMUFhbK0qVLBYBiZa2oP6nRthWNVS3atKzt27dLhw4dTPcpan5TypbxqvR9k6XM2uxLRlrNh1qs6SIiBw4ckH//+99y8eJF0zfumjVrJj/88INiGWVVVr9qrunl5yW1xys3pWx0/fp16dOnjwAQe3t76dq1q4wdO1batWunSf60adPE29tbbt++rfi5CwsLJSMjQ5KSkmTu3LnSpEkTuXz5stl7lJ5QKspMTk6WZs2amXV4rTalyrtz5444ODjIf/7znxplWavfvLw8ycjIkFOnTsmkSZOkVatWcu/ePVUzRX5dqIKDg+WVV16RBw8e1CivPGv127ZtW5kxY4aimWXZUn61BAYGyuuvv656Tvk6Nn7Af+utt8zeN3z4cHnzzTernWNrXarZpg4ODtKjRw+zY++88450795dlTxL861Wc9P27dvFy8tLtm/fLhcvXpTNmzdLo0aNJDY2VtGcymi9KXXy5Emz43/961+lbdu2iuVU1IfVGjNl2bp2K7XeGFXUhmqsN7bkiii/5lRlnle6fkW035Sy1pfUWtMtlXPlypXi6+sre/fulQsXLsiqVaukbt26Eh8fr0impbIaPxDduXPH7L2TJ0+WoKAgRXLLl7U270mTk5PF39/f9LkjKChIhgwZIkOGDFEks6L+pEbbVjRWtWhTo1u3bom7u7ucP3/edEzNTSlb5n6l75sqyqytvmSk1XyoxZpuSV5enjRr1kwiIyNVOX9l9avmml7RppRa45WbUlWUl5dnaow33nhDhg4dqnrmjBkzxMvLSzIzM1XPEhEZMGCA/PGPfzQ7pvYzpYyZH3/8seh0OrG3tze9AIidnZ14e3srlmfrh642bdqY/e2sEizVb/lMpb8ZUT6zqKhIRo4cKZ06dVJlZ7+y+j127JgAMLsxUJu1OldKdna22NnZyZ49e1TPKl/HhYWFUqdOHVm0aJHZ+2bPni09e/ZULNdSXardpi1btpTQ0FCzY6tXrxaDwaB4VkXzrVZzk5eXl0RHR5sdW7RokaKbNNZotSlVWFgo9vb2EhcXZ3Z85syZ0qdPH9VyjX1Y7TFT1bVbyfWmKmuckutNRblqrzkitq2tSq7nWm5KWetLatZv+XIWFBSIg4PDM89ZDQ0NVeSDSUVlvXHjhgCQlJQUs+MjRoyQCRMm1DhX5Nmy/i/ck/7yyy9y//59Efn1GXzTpk2rcV5Fdax22xoZx6oWbWq0e/du06ZM2bY0tm9JSYliWbbM/UrfN9mSqWVfKkur+VCr+2BLBg4c+MzzMZVQnfsIpdb08vOS2uOVz5SqIldXV3h6euLnn3/GoUOHEBwcrFqWiGDGjBmIi4vDN998Ax8fH9Wyyuca/wZZK8bM8ePH4+LFizh//rzpZTAY8MEHH+DQoUOaXtOPP/6I27dvw9PTU9HzWqtfNeq/7DmLi4vxxhtvICMjA4cPH0bjxo0VzbJm48aNCAgI0PQ5Jlr16ZiYGLi7u2PYsGGqZ5Xn6OiIrl27PvNPxl67dg3e3t6K5ViqS7XbtFevXpqUq7L5Vqu5qaCgAHZ25kuzvb09SktLFcv4X+Ho6IiAgADTv1ZpFB8fj549e6qWa+zDao2Z6qzdaq031mgxN2q15lRWltqq35qypS9pvaYXFxejuLhY8XnKWll9fHzg4eFhNl8UFRXh6NGjqs0X/wv3pG5ubmjatCkyMjKQnJxco88d1upYrba1dB2FhYWatumAAQOQlpZm1pZdunTB2LFjcf78edjb29c4oypzv1L3TVXJ1LIvqcFaplb3weUVFhYiPT1d0fWluvWr5pqu+nit8bbWc+Krr76SgwcPSmZmpnz99dfi7+8v3bp1k6KiItUy3377bXFzc5PExETJzc01vQoKChTLmDdvnhw7dkyysrLk4sWL8uc//1ns7Ozk66+/FhGRH3/8UVJTU2X//v0CQHbs2CGpqamSm5urWmZ5Sn1V+tGjR5KamiqpqakCwPT3xjdv3pRHjx7Je++9JydPnpSsrCxJSEiQHj16SPPmzeXhw4fVzqysrHl5eTJv3jw5deqUZGdny7lz5yQ0NFScnJzM/pUMJTOLi4tlxIgR4uXlJefPnzfrV4WFhdXOFKm8fo0ePHggLi4usmbNmhplVaaq/UspT58+lZYtW8qcOXNUy7BWx3FxceLg4CDr1q2TjIwMWbVqldjb28vx48erlWdLXWrRpmfPnpU6derI4sWLJSMjQ7Zt2yYuLi6ydetWxTKqM9+q8WccEydOlObNm8u+ffskKytL4uLipEmTJjJ79mxFc8qzZfyqYceOHeLg4CAbN26UK1euSFhYmLi6ukp2drYi57fWh5UeMyLW+5Ja601lbajWemMtV601p7J2Vat+rZVVRPl7Jmt9Sa36tVbOvn37Svv27SUhIUEyMzMlJiZG9Hq9rF69utqZtszBy5YtEzc3N4mLi5O0tDR56623xNPTU7VxY4kW96QiIrt27ZKEhAS5ceOG7NmzR7y9vWXUqFE1yrSljpVuW2tzsBptaiul/3zP1vsIJe+bbMmsrb6k9Xwoos6aXt57770niYmJkpmZKadPn5bXX39d6tWrp9h9i4j1sqq1plubl9Qcr9yUstHOnTuldevW4ujoKB4eHjJ9+nT55ZdfVM0EYPEVExOjWMakSZPE29tbHB0dpWnTpjJgwACzD5wxMTEWr2HBggWqZZan1A1AQkKCxbJMnDhRCgoKJDAwUJo2bSoODg7SsmVLmThxoty6datGmZWV9fHjx/Lb3/5WDAaDODo6iqenp4wYMaLGD6mrLNP4FVpLr4SEhBrlVla/Rp999pk4OzurOnaq2r+UcujQIQEgV69eVS3DljreuHGjtGnTRvR6vfj7+9foTwltqUst2lTk14c5dujQQZycnMTPz0/WrVun6PmrM9+qsSn18OFDmTVrlrRs2VL0er20bt1awsPDa7xpbI0tfUst//jHP0z9rHPnzpX+k8dVZUsfVnLMiFjvS2qtN5W1oVrrjbVctdacytpVrfq1VlYR5e+ZrPUlterXWjlzc3MlJCREDAaD6PV6adu2rURGRpoeBq5GWUV+feDvggULxMPDQ5ycnKRPnz6SlpZW7UxbylqeFvekIiKffvqpeHl5mfrw/Pnza7wO2FLHSrettTlYjTa1ldKbUrbeRyh532RLZm31Ja3nQyOl1/TyxowZI56enuLg4CAGg0FGjRql+HNrrZVVrTXd2ryk5njViYiAiIiIiIiIiIhIQ3ymFBERERERERERaY6bUkREREREREREpDluShERERERERERkea4KUVERERERERERJrjphQREREREREREWmOm1JERERERERERKQ5bkoREREREREREZHmuClFRERERERERESa46YUERERET1Dp9Nhz549tX0ZRERE9H8YN6WIiIjouRESEgKdTvfM6/r164qcPzY2Fg0aNFDkXNUVEhKCkSNH1uo1EBEREdmiTm1fABEREZGWBg8ejJiYGLNjTZs2raWrqVhxcTEcHBxq+zKIiIiIVMNvShEREdFzxcnJCR4eHmYve3t7AMCXX36JgIAA6PV6tG7dGh999BFKSkpMvxsVFYWOHTvC1dUVLVq0wLRp05CXlwcASExMxB/+8Ac8ePDA9A2siIgIAJb/FK5BgwaIjY0FAGRnZ0On02HXrl3o168f9Ho9tm7dCgCIiYlBu3btoNfr4efnh9WrV1epvP369cPMmTMxe/ZsNGrUCB4eHqbrMsrIyECfPn2g1+vx0ksvIT4+/pnz5OTkYMyYMWjYsCEaN26M4OBgZGdnAwD++9//wsXFBf/85z9N74+Li4Ner0daWlqVrpeIiIieH9yUIiIiIgJw6NAhjBs3DjNnzsSVK1fw2WefITY2FosXLza9x87ODn//+99x6dIlbNq0Cd988w1mz54NAOjZsyc++eQT1K9fH7m5ucjNzcX7779fpWuYM2cOZs6cifT0dAQFBWH9+vUIDw/H4sWLkZ6ejiVLluDDDz/Epk2bqnTeTZs2wdXVFWfOnMGKFSuwcOFC08ZTaWkpRo0aBXt7e5w+fRpr167FnDlzzH6/oKAAr732GurWrYtjx47hxIkTqFu3LgYPHoyioiL4+fnhb3/7G6ZNm4abN2/izp07mDJlCpYtW4aOHTtW6VqJiIjo+cE/3yMiIqLnyr59+1C3bl3Tz0OGDMG//vUvLF68GHPnzsXEiRMBAK1bt8aiRYswe/ZsLFiwAAAQFhZm+j0fHx8sWrQIb7/9NlavXg1HR0e4ublBp9PBw8OjWtcWFhaGUaNGmX5etGgRIiMjTcd8fHxMG2bG67RFp06dTGV48cUXER0djSNHjmDQoEE4fPgw0tPTkZ2dDS8vLwDAkiVLMGTIENPv79ixA3Z2dtiwYQN0Oh2AX7/B1aBBAyQmJiIwMBDTpk3DgQMHMH78eDg6OiIgIACzZs2qVj0QERHR84GbUkRERPRcee2117BmzRrTz66urgCAc+fOISkpyeybUU+fPsWTJ09QUFAAFxcXJCQkYMmSJbhy5QoePnyIkpISPHnyBPn5+abz1ESXLl1M//3999/j9u3bCA0NxZQpU0zHS0pK4ObmVqXzdurUyexnT09P3L9/HwCQnp6Oli1bmjakAKBHjx5m7z937hyuX7+OevXqmR1/8uQJbty4Yfr5888/h6+vL+zs7HDp0iXTBhYRERGRJdyUIiIioueKq6sr2rRp88zx0tJSfPTRR2bfVDLS6/W4efMmhg4diqlTp2LRokVo1KgRTpw4gdDQUBQXF1eaqdPpICJmxyz9TtmNrdLSUgDA+vXr8eqrr5q9z/gMLFuVf2C6Tqcznb/8dRn/f1mlpaUICAjAtm3bnnlv2YfEX7hwAfn5+bCzs8Pdu3dhMBiqdJ1ERET0fOGmFBERERGAzp074+rVqxY3rAAgOTkZJSUliIyMhJ3dr4/l3LVrl9l7HB0d8fTp02d+t2nTpsjNzTX9nJGRgYKCgkqvp1mzZmjevDkyMzMxduzYqhbHZi+99BJu3bqFO3fumDaRTp06Zfaezp07Y+fOnXB3d0f9+vUtnuenn35CSEgIwsPDcffuXYwdOxYpKSlwdnZW7dqJiIjo/2980DkRERERgL/85S/YvHkzIiIicPnyZaSnp2Pnzp2YP38+AOCFF15ASUkJVq1ahczMTGzZsgVr1641O0erVq2Ql5eHI0eO4IcffjBtPPXv3x/R0dFISUlBcnIypk6d+sy3lyyJiIjA0qVL8emnn+LatWtIS0tDTEwMoqKiFCv3wIED0bZtW0yYMAEXLlzA8ePHER4ebvaesWPHokmTJggODsbx48eRlZWFo0ePYtasWfjuu+8AAFOnTkWLFi0wf/58REVFQUSq/KB3IiIier5wU4qIiIgIQFBQEPbt24f4+Hh07doV3bt3R1RUFLy9vQEAL7/8MqKiorB8+XJ06NAB27Ztw9KlS83O0bNnT0ydOhVjxoxB06ZNsWLFCgBAZGQkWrRogT59+uD3v/893n//fbi4uFi9psmTJ2PDhg2IjY1Fx44d0bdvX8TGxsLHx0exctvZ2WH37t0oLCxEt27dMHnyZLPnagGAi4sLjh07hpYtW2LUqFFo164dJk2ahMePH6N+/frYvHkzDhw4gC1btqBOnTpwcXHBtm3bsGHDBhw4cECxayUiIqL/W3Ri6UECREREREREREREKuI3pYiIiIiIiIiISHPclCIiIiIiIiIiIs1xU4qIiIiIiIiIiDTHTSkiIiIiIiIiItIcN6WIiIiIiIiIiEhz3JQiIiIiIiIiIiLNcVOKiIiIiIiIiIg0x00pIiIiIiIiIiLSHDeliIiIiIiIiIhIc9yUIiIiIiIiIiIizXFTioiIiIiIiIiINMdNKSIiIiIiIiIi0tz/A0lDfeS2Ht8tAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 1200x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import shap\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#results = {}\n",
    "\n",
    "results[\"Convex Only\"] = train_with_schedule(training_schedule_convex, \"Convex Only\")\n",
    "results[\"Multistage\"] = train_with_schedule(training_schedule_multistage, \"Multistage\")\n",
    "results[\"Nonconvex Only\"] = train_with_schedule(training_schedule_nonconvex, \"Nonconvex Only\")\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# Define number of top features to compare\n",
    "TOP_K = 38\n",
    "\n",
    "# Store aggregated SHAP values per schedule\n",
    "shap_feature_importance = {}\n",
    "\n",
    "for name, metrics in results.items():\n",
    "    shap_values = metrics.get(\"shap_values\", None)\n",
    "\n",
    "    if shap_values is not None:\n",
    "        try:\n",
    "            # Transpose to (samples, time_steps, features)\n",
    "            reshaped_shap = np.transpose(shap_values[0], (2, 0, 1))  # (1, 51, 38)\n",
    "            repeated_shap = np.repeat(reshaped_shap, 100, axis=0)    # (100, 51, 38)\n",
    "            mean_abs_shap = np.mean(np.abs(repeated_shap), axis=(0, 1))  # (features,)\n",
    "\n",
    "            shap_feature_importance[name] = mean_abs_shap\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Failed to process SHAP values for {name}: {e}\")\n",
    "    else:\n",
    "        print(f\"⚠️ No SHAP values for {name}\")\n",
    "\n",
    "# Combine into DataFrame for comparison\n",
    "df_importance = pd.DataFrame(shap_feature_importance)\n",
    "\n",
    "# Optional: rename columns to feature names if you have them, e.g., feature_names = ['f1', ..., 'f38']\n",
    "# df_importance.index = feature_names\n",
    "\n",
    "# Get top K features by mean importance across all models\n",
    "top_features = df_importance.mean(axis=1).nlargest(TOP_K).index\n",
    "df_top = df_importance.loc[top_features]\n",
    "\n",
    "# Plot\n",
    "df_top.plot(kind='bar', figsize=(12, 6))\n",
    "plt.title(f\"🔍 Top {TOP_K} Important Features (SHAP) Across Training Schedules\")\n",
    "plt.ylabel(\"Mean |SHAP value|\")\n",
    "plt.xlabel(\"Feature Index\")\n",
    "plt.xticks(rotation=0)\n",
    "plt.grid(True)\n",
    "plt.legend(title=\"Training Schedule\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "90ebc216",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(51, 38, 1)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shap_values[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f3f88f6b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 51, 38)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testX[:100].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8f8a7f8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
